
==============================
FILE: .\bundle.py
==============================
import os

# ====== C·∫§U H√åNH ======
CODE_OUTPUT_FILE = "project_code.txt"
TREE_OUTPUT_FILE = "project_tree.txt"

# Extension ƒë∆∞·ª£c ph√©p xu·∫•t n·ªôi dung
ALLOWED_EXTENSIONS = {
    ".py", ".jsx", ".js", ".css", ".html",
    ".json", ".yml", ".yaml", ".md", ".txt"
}

# Th∆∞ m·ª•c lo·∫°i tr·ª´ ho√†n to√†n
EXCLUDE_DIRS = {
    "venv",
    "node_modules",
    ".git",
    "__pycache__",
    ".pytest_cache",
    "dist",
    "build",
}

# File lo·∫°i tr·ª´
EXCLUDE_FILES = {
    ".DS_Store",
    ".gitignore",
}

# =====================


def should_exclude_file(filename: str) -> bool:
    if filename in EXCLUDE_FILES:
        return True
    if filename.startswith(".env"):
        return True
    return False


def export_project_code():
    with open(CODE_OUTPUT_FILE, "w", encoding="utf-8") as outfile:
        for root, dirs, files in os.walk("."):
            # Lo·∫°i th∆∞ m·ª•c r√°c ƒë√∫ng c√°ch
            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

            for file in files:
                if should_exclude_file(file):
                    continue

                if any(file.endswith(ext) for ext in ALLOWED_EXTENSIONS):
                    full_path = os.path.join(root, file)

                    outfile.write("\n" + "=" * 30 + "\n")
                    outfile.write(f"FILE: {full_path}\n")
                    outfile.write("=" * 30 + "\n")

                    try:
                        with open(full_path, "r", encoding="utf-8", errors="ignore") as infile:
                            outfile.write(infile.read())
                    except Exception as e:
                        outfile.write(f"\n[ERROR READING FILE]: {e}\n")

    print(f"‚úÖ ƒê√£ xu·∫•t to√†n b·ªô code + config v√†o: {CODE_OUTPUT_FILE}")


def export_project_tree():
    lines = []

    for root, dirs, files in os.walk("."):
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        level = root.replace(".", "").count(os.sep)
        indent = "‚îÇ   " * level

        folder_name = os.path.basename(root) if root != "." else os.path.abspath(".")
        lines.append(f"{indent}‚îú‚îÄ‚îÄ {folder_name}/")

        sub_indent = "‚îÇ   " * (level + 1)
        for file in files:
            if should_exclude_file(file):
                continue
            lines.append(f"{sub_indent}‚îú‚îÄ‚îÄ {file}")

    with open(TREE_OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"‚úÖ ƒê√£ xu·∫•t c√¢y th∆∞ m·ª•c (ƒë√£ l·ªçc) v√†o: {TREE_OUTPUT_FILE}")


def main():
    export_project_code()
    export_project_tree()
    print("\nüéâ Ho√†n t·∫•t: ƒë√£ t·∫°o 2 file (CODE + TREE)")


if __name__ == "__main__":
    main()

==============================
FILE: .\check_imports.py
==============================
import pkgutil
import importlib
import traceback

BASE_PACKAGE = "backend"

errors = []

for module in pkgutil.walk_packages([BASE_PACKAGE], BASE_PACKAGE + "."):
    name = module.name
    try:
        importlib.import_module(name)
    except Exception as e:
        errors.append((name, e))

print("\n====== IMPORT ERRORS ======")
for name, err in errors:
    print(f"\n‚ùå {name}")
    traceback.print_exception(type(err), err, err.__traceback__)

print(f"\nTotal errors: {len(errors)}")

==============================
FILE: .\docker-compose.yml
==============================
version: "3.9"

services:
  backend:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data

==============================
FILE: .\main.py
==============================
import uvicorn
import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from backend.api import upload, chat, courses
from backend.database.db_config import init_db

app = FastAPI(title="AI Teaching Assistant System - No Auth Mode")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ƒêƒÉng k√Ω Router (ƒê√£ b·ªè auth)
app.include_router(courses.router, prefix="/courses", tags=["Courses"])
app.include_router(upload.router, prefix="/upload", tags=["Upload"])
app.include_router(chat.router, prefix="/chat", tags=["AI Chat"])

@app.on_event("startup")
def startup_event():
    init_db()
    print("üöÄ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng (CH·∫æ ƒê·ªò KH√îNG LOGIN)!")

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
==============================
FILE: .\package-lock.json
==============================
{
  "name": "ai-tro-giang",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "dependencies": {
        "axios": "^1.13.4"
      }
    },
    "node_modules/asynckit": {
      "version": "0.4.0",
      "resolved": "https://registry.npmjs.org/asynckit/-/asynckit-0.4.0.tgz",
      "integrity": "sha512-Oei9OH4tRh0YqU3GxhX79dM/mwVgvbZJaSNaRk+bshkj0S5cfHcgYakreBjrHwatXKbz+IoIdYLxrKim2MjW0Q==",
      "license": "MIT"
    },
    "node_modules/axios": {
      "version": "1.13.4",
      "resolved": "https://registry.npmjs.org/axios/-/axios-1.13.4.tgz",
      "integrity": "sha512-1wVkUaAO6WyaYtCkcYCOx12ZgpGf9Zif+qXa4n+oYzK558YryKqiL6UWwd5DqiH3VRW0GYhTZQ/vlgJrCoNQlg==",
      "license": "MIT",
      "dependencies": {
        "follow-redirects": "^1.15.6",
        "form-data": "^4.0.4",
        "proxy-from-env": "^1.1.0"
      }
    },
    "node_modules/call-bind-apply-helpers": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz",
      "integrity": "sha512-Sp1ablJ0ivDkSzjcaJdxEunN5/XvksFJ2sMBFfq6x0ryhQV/2b/KwFe21cMpmHtPOSij8K99/wSfoEuTObmuMQ==",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0",
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/combined-stream": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/combined-stream/-/combined-stream-1.0.8.tgz",
      "integrity": "sha512-FQN4MRfuJeHf7cBbBMJFXhKSDq+2kAArBlmRBvcvFE5BB1HZKXtSFASDhdlz9zOYwxh8lDdnvmMOe/+5cdoEdg==",
      "license": "MIT",
      "dependencies": {
        "delayed-stream": "~1.0.0"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/delayed-stream": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/delayed-stream/-/delayed-stream-1.0.0.tgz",
      "integrity": "sha512-ZySD7Nf91aLB0RxL4KGrKHBXl7Eds1DAmEdcoVawXnLD7SDhpNgtuII2aAkg7a7QS41jxPSZ17p4VdGnMHk3MQ==",
      "license": "MIT",
      "engines": {
        "node": ">=0.4.0"
      }
    },
    "node_modules/dunder-proto": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/dunder-proto/-/dunder-proto-1.0.1.tgz",
      "integrity": "sha512-KIN/nDJBQRcXw0MLVhZE9iQHmG68qAVIBg9CqmUYjmQIhgij9U5MFvrqkUL5FbtyyzZuOeOt0zdeRe4UY7ct+A==",
      "license": "MIT",
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.1",
        "es-errors": "^1.3.0",
        "gopd": "^1.2.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-define-property": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/es-define-property/-/es-define-property-1.0.1.tgz",
      "integrity": "sha512-e3nRfgfUZ4rNGL232gUgX06QNyyez04KdjFrF+LTRoOXmrOgFKDg4BCdsjW8EnT69eqdYGmRpJwiPVYNrCaW3g==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-errors": {
      "version": "1.3.0",
      "resolved": "https://registry.npmjs.org/es-errors/-/es-errors-1.3.0.tgz",
      "integrity": "sha512-Zf5H2Kxt2xjTvbJvP2ZWLEICxA6j+hAmMzIlypy4xcBg1vKVnx89Wy0GbS+kf5cwCVFFzdCFh2XSCFNULS6csw==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-object-atoms": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/es-object-atoms/-/es-object-atoms-1.1.1.tgz",
      "integrity": "sha512-FGgH2h8zKNim9ljj7dankFPcICIK9Cp5bm+c2gQSYePhpaG5+esrLODihIorn+Pe6FGJzWhXQotPv73jTaldXA==",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-set-tostringtag": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/es-set-tostringtag/-/es-set-tostringtag-2.1.0.tgz",
      "integrity": "sha512-j6vWzfrGVfyXxge+O0x5sh6cvxAog0a/4Rdd2K36zCMV5eJ+/+tOAngRO8cODMNWbVRdVlmGZQL2YS3yR8bIUA==",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0",
        "get-intrinsic": "^1.2.6",
        "has-tostringtag": "^1.0.2",
        "hasown": "^2.0.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/follow-redirects": {
      "version": "1.15.11",
      "resolved": "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.15.11.tgz",
      "integrity": "sha512-deG2P0JfjrTxl50XGCDyfI97ZGVCxIpfKYmfyrQ54n5FO/0gfIES8C/Psl6kWVDolizcaaxZJnTS0QSMxvnsBQ==",
      "funding": [
        {
          "type": "individual",
          "url": "https://github.com/sponsors/RubenVerborgh"
        }
      ],
      "license": "MIT",
      "engines": {
        "node": ">=4.0"
      },
      "peerDependenciesMeta": {
        "debug": {
          "optional": true
        }
      }
    },
    "node_modules/form-data": {
      "version": "4.0.5",
      "resolved": "https://registry.npmjs.org/form-data/-/form-data-4.0.5.tgz",
      "integrity": "sha512-8RipRLol37bNs2bhoV67fiTEvdTrbMUYcFTiy3+wuuOnUog2QBHCZWXDRijWQfAkhBj2Uf5UnVaiWwA5vdd82w==",
      "license": "MIT",
      "dependencies": {
        "asynckit": "^0.4.0",
        "combined-stream": "^1.0.8",
        "es-set-tostringtag": "^2.1.0",
        "hasown": "^2.0.2",
        "mime-types": "^2.1.12"
      },
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/function-bind": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz",
      "integrity": "sha512-7XHNxH7qX9xG5mIwxkhumTox/MIRNcOgDrxWsMt2pAr23WHp6MrRlN7FBSFpCpr+oVO0F744iUgR82nJMfG2SA==",
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-intrinsic": {
      "version": "1.3.0",
      "resolved": "https://registry.npmjs.org/get-intrinsic/-/get-intrinsic-1.3.0.tgz",
      "integrity": "sha512-9fSjSaos/fRIVIp+xSJlE6lfwhES7LNtKaCBIamHsjr2na1BiABJPo0mOjjz8GJDURarmCPGqaiVg5mfjb98CQ==",
      "license": "MIT",
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.2",
        "es-define-property": "^1.0.1",
        "es-errors": "^1.3.0",
        "es-object-atoms": "^1.1.1",
        "function-bind": "^1.1.2",
        "get-proto": "^1.0.1",
        "gopd": "^1.2.0",
        "has-symbols": "^1.1.0",
        "hasown": "^2.0.2",
        "math-intrinsics": "^1.1.0"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-proto": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/get-proto/-/get-proto-1.0.1.tgz",
      "integrity": "sha512-sTSfBjoXBp89JvIKIefqw7U2CCebsc74kiY6awiGogKtoSGbgjYE/G/+l9sF3MWFPNc9IcoOC4ODfKHfxFmp0g==",
      "license": "MIT",
      "dependencies": {
        "dunder-proto": "^1.0.1",
        "es-object-atoms": "^1.0.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/gopd": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/gopd/-/gopd-1.2.0.tgz",
      "integrity": "sha512-ZUKRh6/kUFoAiTAtTYPZJ3hw9wNxx+BIBOijnlG9PnrJsCcSjs1wyyD6vJpaYtgnzDrKYRSqf3OO6Rfa93xsRg==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-symbols": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz",
      "integrity": "sha512-1cDNdwJ2Jaohmb3sg4OmKaMBwuC48sYni5HUw2DvsC8LjGTLK9h+eb1X6RyuOHe4hT0ULCW68iomhjUoKUqlPQ==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-tostringtag": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/has-tostringtag/-/has-tostringtag-1.0.2.tgz",
      "integrity": "sha512-NqADB8VjPFLM2V0VvHUewwwsw0ZWBaIdgo+ieHtK3hasLz4qeCRjYcqfB6AQrBggRKppKF8L52/VqdVsO47Dlw==",
      "license": "MIT",
      "dependencies": {
        "has-symbols": "^1.0.3"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/hasown": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz",
      "integrity": "sha512-0hJU9SCPvmMzIBdZFqNPXWa6dqh7WdH0cII9y+CyS8rG3nL48Bclra9HmKhVVUHyPWNH5Y7xDwAB7bfgSjkUMQ==",
      "license": "MIT",
      "dependencies": {
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/math-intrinsics": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/math-intrinsics/-/math-intrinsics-1.1.0.tgz",
      "integrity": "sha512-/IXtbwEk5HTPyEwyKX6hGkYXxM9nbj64B+ilVJnC/R6B0pH5G4V3b0pVbL7DBj4tkhBAppbQUlf6F6Xl9LHu1g==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/mime-db": {
      "version": "1.52.0",
      "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.52.0.tgz",
      "integrity": "sha512-sPU4uV7dYlvtWJxwwxHD0PuihVNiE7TyAbQ5SWxDCB9mUYvOgroQOwYQQOKPJ8CIbE+1ETVlOoK1UC2nU3gYvg==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/mime-types": {
      "version": "2.1.35",
      "resolved": "https://registry.npmjs.org/mime-types/-/mime-types-2.1.35.tgz",
      "integrity": "sha512-ZDY+bPm5zTTF+YpCrAU9nK0UgICYPT0QtT1NZWFv4s++TNkcgVaT0g6+4R2uI4MjQjzysHB1zxuWL50hzaeXiw==",
      "license": "MIT",
      "dependencies": {
        "mime-db": "1.52.0"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/proxy-from-env": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/proxy-from-env/-/proxy-from-env-1.1.0.tgz",
      "integrity": "sha512-D+zkORCbA9f1tdWRK0RaCR3GPv50cMxcrz4X8k5LTSUD1Dkw47mKJEZQNunItRTkWwgtaUSo1RVFRIG9ZXiFYg==",
      "license": "MIT"
    }
  }
}

==============================
FILE: .\package.json
==============================
{
  "dependencies": {
    "axios": "^1.13.4"
  }
}

==============================
FILE: .\project_code.txt
==============================

==============================
FILE: .\bundle.py
==============================
import os

# ====== C·∫§U H√åNH ======
CODE_OUTPUT_FILE = "project_code.txt"
TREE_OUTPUT_FILE = "project_tree.txt"

# Extension ƒë∆∞·ª£c ph√©p xu·∫•t n·ªôi dung
ALLOWED_EXTENSIONS = {
    ".py", ".jsx", ".js", ".css", ".html",
    ".json", ".yml", ".yaml", ".md", ".txt"
}

# Th∆∞ m·ª•c lo·∫°i tr·ª´ ho√†n to√†n
EXCLUDE_DIRS = {
    "venv",
    "node_modules",
    ".git",
    "__pycache__",
    ".pytest_cache",
    "dist",
    "build",
}

# File lo·∫°i tr·ª´
EXCLUDE_FILES = {
    ".DS_Store",
    ".gitignore",
}

# =====================


def should_exclude_file(filename: str) -> bool:
    if filename in EXCLUDE_FILES:
        return True
    if filename.startswith(".env"):
        return True
    return False


def export_project_code():
    with open(CODE_OUTPUT_FILE, "w", encoding="utf-8") as outfile:
        for root, dirs, files in os.walk("."):
            # Lo·∫°i th∆∞ m·ª•c r√°c ƒë√∫ng c√°ch
            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

            for file in files:
                if should_exclude_file(file):
                    continue

                if any(file.endswith(ext) for ext in ALLOWED_EXTENSIONS):
                    full_path = os.path.join(root, file)

                    outfile.write("\n" + "=" * 30 + "\n")
                    outfile.write(f"FILE: {full_path}\n")
                    outfile.write("=" * 30 + "\n")

                    try:
                        with open(full_path, "r", encoding="utf-8", errors="ignore") as infile:
                            outfile.write(infile.read())
                    except Exception as e:
                        outfile.write(f"\n[ERROR READING FILE]: {e}\n")

    print(f"‚úÖ ƒê√£ xu·∫•t to√†n b·ªô code + config v√†o: {CODE_OUTPUT_FILE}")


def export_project_tree():
    lines = []

    for root, dirs, files in os.walk("."):
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        level = root.replace(".", "").count(os.sep)
        indent = "‚îÇ   " * level

        folder_name = os.path.basename(root) if root != "." else os.path.abspath(".")
        lines.append(f"{indent}‚îú‚îÄ‚îÄ {folder_name}/")

        sub_indent = "‚îÇ   " * (level + 1)
        for file in files:
            if should_exclude_file(file):
                continue
            lines.append(f"{sub_indent}‚îú‚îÄ‚îÄ {file}")

    with open(TREE_OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"‚úÖ ƒê√£ xu·∫•t c√¢y th∆∞ m·ª•c (ƒë√£ l·ªçc) v√†o: {TREE_OUTPUT_FILE}")


def main():
    export_project_code()
    export_project_tree()
    print("\nüéâ Ho√†n t·∫•t: ƒë√£ t·∫°o 2 file (CODE + TREE)")


if __name__ == "__main__":
    main()

==============================
FILE: .\check_imports.py
==============================
import pkgutil
import importlib
import traceback

BASE_PACKAGE = "backend"

errors = []

for module in pkgutil.walk_packages([BASE_PACKAGE], BASE_PACKAGE + "."):
    name = module.name
    try:
        importlib.import_module(name)
    except Exception as e:
        errors.append((name, e))

print("\n====== IMPORT ERRORS ======")
for name, err in errors:
    print(f"\n‚ùå {name}")
    traceback.print_exception(type(err), err, err.__traceback__)

print(f"\nTotal errors: {len(errors)}")

==============================
FILE: .\docker-compose.yml
==============================
version: "3.9"

services:
  backend:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data

==============================
FILE: .\main.py
==============================
import uvicorn
import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from backend.api import upload, chat, courses
from backend.database.db_config import init_db

app = FastAPI(title="AI Teaching Assistant System - No Auth Mode")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ƒêƒÉng k√Ω Router (ƒê√£ b·ªè auth)
app.include_router(courses.router, prefix="/courses", tags=["Courses"])
app.include_router(upload.router, prefix="/upload", tags=["Upload"])
app.include_router(chat.router, prefix="/chat", tags=["AI Chat"])

@app.on_event("startup")
def startup_event():
    init_db()
    print("üöÄ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng (CH·∫æ ƒê·ªò KH√îNG LOGIN)!")

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
==============================
FILE: .\package-lock.json
==============================
{
  "name": "ai-tro-giang",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "dependencies": {
        "axios": "^1.13.4"
      }
    },
    "node_modules/asynckit": {
      "version": "0.4.0",
      "resolved": "https://registry.npmjs.org/asynckit/-/asynckit-0.4.0.tgz",
      "integrity": "sha512-Oei9OH4tRh0YqU3GxhX79dM/mwVgvbZJaSNaRk+bshkj0S5cfHcgYakreBjrHwatXKbz+IoIdYLxrKim2MjW0Q==",
      "license": "MIT"
    },
    "node_modules/axios": {
      "version": "1.13.4",
      "resolved": "https://registry.npmjs.org/axios/-/axios-1.13.4.tgz",
      "integrity": "sha512-1wVkUaAO6WyaYtCkcYCOx12ZgpGf9Zif+qXa4n+oYzK558YryKqiL6UWwd5DqiH3VRW0GYhTZQ/vlgJrCoNQlg==",
      "license": "MIT",
      "dependencies": {
        "follow-redirects": "^1.15.6",
        "form-data": "^4.0.4",
        "proxy-from-env": "^1.1.0"
      }
    },
    "node_modules/call-bind-apply-helpers": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz",
      "integrity": "sha512-Sp1ablJ0ivDkSzjcaJdxEunN5/XvksFJ2sMBFfq6x0ryhQV/2b/KwFe21cMpmHtPOSij8K99/wSfoEuTObmuMQ==",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0",
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/combined-stream": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/combined-stream/-/combined-stream-1.0.8.tgz",
      "integrity": "sha512-FQN4MRfuJeHf7cBbBMJFXhKSDq+2kAArBlmRBvcvFE5BB1HZKXtSFASDhdlz9zOYwxh8lDdnvmMOe/+5cdoEdg==",
      "license": "MIT",
      "dependencies": {
        "delayed-stream": "~1.0.0"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/delayed-stream": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/delayed-stream/-/delayed-stream-1.0.0.tgz",
      "integrity": "sha512-ZySD7Nf91aLB0RxL4KGrKHBXl7Eds1DAmEdcoVawXnLD7SDhpNgtuII2aAkg7a7QS41jxPSZ17p4VdGnMHk3MQ==",
      "license": "MIT",
      "engines": {
        "node": ">=0.4.0"
      }
    },
    "node_modules/dunder-proto": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/dunder-proto/-/dunder-proto-1.0.1.tgz",
      "integrity": "sha512-KIN/nDJBQRcXw0MLVhZE9iQHmG68qAVIBg9CqmUYjmQIhgij9U5MFvrqkUL5FbtyyzZuOeOt0zdeRe4UY7ct+A==",
      "license": "MIT",
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.1",
        "es-errors": "^1.3.0",
        "gopd": "^1.2.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-define-property": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/es-define-property/-/es-define-property-1.0.1.tgz",
      "integrity": "sha512-e3nRfgfUZ4rNGL232gUgX06QNyyez04KdjFrF+LTRoOXmrOgFKDg4BCdsjW8EnT69eqdYGmRpJwiPVYNrCaW3g==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-errors": {
      "version": "1.3.0",
      "resolved": "https://registry.npmjs.org/es-errors/-/es-errors-1.3.0.tgz",
      "integrity": "sha512-Zf5H2Kxt2xjTvbJvP2ZWLEICxA6j+hAmMzIlypy4xcBg1vKVnx89Wy0GbS+kf5cwCVFFzdCFh2XSCFNULS6csw==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-object-atoms": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/es-object-atoms/-/es-object-atoms-1.1.1.tgz",
      "integrity": "sha512-FGgH2h8zKNim9ljj7dankFPcICIK9Cp5bm+c2gQSYePhpaG5+esrLODihIorn+Pe6FGJzWhXQotPv73jTaldXA==",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-set-tostringtag": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/es-set-tostringtag/-/es-set-tostringtag-2.1.0.tgz",
      "integrity": "sha512-j6vWzfrGVfyXxge+O0x5sh6cvxAog0a/4Rdd2K36zCMV5eJ+/+tOAngRO8cODMNWbVRdVlmGZQL2YS3yR8bIUA==",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0",
        "get-intrinsic": "^1.2.6",
        "has-tostringtag": "^1.0.2",
        "hasown": "^2.0.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/follow-redirects": {
      "version": "1.15.11",
      "resolved": "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.15.11.tgz",
      "integrity": "sha512-deG2P0JfjrTxl50XGCDyfI97ZGVCxIpfKYmfyrQ54n5FO/0gfIES8C/Psl6kWVDolizcaaxZJnTS0QSMxvnsBQ==",
      "funding": [
        {
          "type": "individual",
          "url": "https://github.com/sponsors/RubenVerborgh"
        }
      ],
      "license": "MIT",
      "engines": {
        "node": ">=4.0"
      },
      "peerDependenciesMeta": {
        "debug": {
          "optional": true
        }
      }
    },
    "node_modules/form-data": {
      "version": "4.0.5",
      "resolved": "https://registry.npmjs.org/form-data/-/form-data-4.0.5.tgz",
      "integrity": "sha512-8RipRLol37bNs2bhoV67fiTEvdTrbMUYcFTiy3+wuuOnUog2QBHCZWXDRijWQfAkhBj2Uf5UnVaiWwA5vdd82w==",
      "license": "MIT",
      "dependencies": {
        "asynckit": "^0.4.0",
        "combined-stream": "^1.0.8",
        "es-set-tostringtag": "^2.1.0",
        "hasown": "^2.0.2",
        "mime-types": "^2.1.12"
      },
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/function-bind": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz",
      "integrity": "sha512-7XHNxH7qX9xG5mIwxkhumTox/MIRNcOgDrxWsMt2pAr23WHp6MrRlN7FBSFpCpr+oVO0F744iUgR82nJMfG2SA==",
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-intrinsic": {
      "version": "1.3.0",
      "resolved": "https://registry.npmjs.org/get-intrinsic/-/get-intrinsic-1.3.0.tgz",
      "integrity": "sha512-9fSjSaos/fRIVIp+xSJlE6lfwhES7LNtKaCBIamHsjr2na1BiABJPo0mOjjz8GJDURarmCPGqaiVg5mfjb98CQ==",
      "license": "MIT",
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.2",
        "es-define-property": "^1.0.1",
        "es-errors": "^1.3.0",
        "es-object-atoms": "^1.1.1",
        "function-bind": "^1.1.2",
        "get-proto": "^1.0.1",
        "gopd": "^1.2.0",
        "has-symbols": "^1.1.0",
        "hasown": "^2.0.2",
        "math-intrinsics": "^1.1.0"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-proto": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/get-proto/-/get-proto-1.0.1.tgz",
      "integrity": "sha512-sTSfBjoXBp89JvIKIefqw7U2CCebsc74kiY6awiGogKtoSGbgjYE/G/+l9sF3MWFPNc9IcoOC4ODfKHfxFmp0g==",
      "license": "MIT",
      "dependencies": {
        "dunder-proto": "^1.0.1",
        "es-object-atoms": "^1.0.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/gopd": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/gopd/-/gopd-1.2.0.tgz",
      "integrity": "sha512-ZUKRh6/kUFoAiTAtTYPZJ3hw9wNxx+BIBOijnlG9PnrJsCcSjs1wyyD6vJpaYtgnzDrKYRSqf3OO6Rfa93xsRg==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-symbols": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz",
      "integrity": "sha512-1cDNdwJ2Jaohmb3sg4OmKaMBwuC48sYni5HUw2DvsC8LjGTLK9h+eb1X6RyuOHe4hT0ULCW68iomhjUoKUqlPQ==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-tostringtag": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/has-tostringtag/-/has-tostringtag-1.0.2.tgz",
      "integrity": "sha512-NqADB8VjPFLM2V0VvHUewwwsw0ZWBaIdgo+ieHtK3hasLz4qeCRjYcqfB6AQrBggRKppKF8L52/VqdVsO47Dlw==",
      "license": "MIT",
      "dependencies": {
        "has-symbols": "^1.0.3"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/hasown": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz",
      "integrity": "sha512-0hJU9SCPvmMzIBdZFqNPXWa6dqh7WdH0cII9y+CyS8rG3nL48Bclra9HmKhVVUHyPWNH5Y7xDwAB7bfgSjkUMQ==",
      "license": "MIT",
      "dependencies": {
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/math-intrinsics": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/math-intrinsics/-/math-intrinsics-1.1.0.tgz",
      "integrity": "sha512-/IXtbwEk5HTPyEwyKX6hGkYXxM9nbj64B+ilVJnC/R6B0pH5G4V3b0pVbL7DBj4tkhBAppbQUlf6F6Xl9LHu1g==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/mime-db": {
      "version": "1.52.0",
      "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.52.0.tgz",
      "integrity": "sha512-sPU4uV7dYlvtWJxwwxHD0PuihVNiE7TyAbQ5SWxDCB9mUYvOgroQOwYQQOKPJ8CIbE+1ETVlOoK1UC2nU3gYvg==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/mime-types": {
      "version": "2.1.35",
      "resolved": "https://registry.npmjs.org/mime-types/-/mime-types-2.1.35.tgz",
      "integrity": "sha512-ZDY+bPm5zTTF+YpCrAU9nK0UgICYPT0QtT1NZWFv4s++TNkcgVaT0g6+4R2uI4MjQjzysHB1zxuWL50hzaeXiw==",
      "license": "MIT",
      "dependencies": {
        "mime-db": "1.52.0"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/proxy-from-env": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/proxy-from-env/-/proxy-from-env-1.1.0.tgz",
      "integrity": "sha512-D+zkORCbA9f1tdWRK0RaCR3GPv50cMxcrz4X8k5LTSUD1Dkw47mKJEZQNunItRTkWwgtaUSo1RVFRIG9ZXiFYg==",
      "license": "MIT"
    }
  }
}

==============================
FILE: .\project_tree.txt
==============================
‚îú‚îÄ‚îÄ D:\ai-tro-giang/
‚îÇ   ‚îú‚îÄ‚îÄ bundle.py
‚îÇ   ‚îú‚îÄ‚îÄ check_imports.py
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ package-lock.json
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ project_code.txt
‚îÇ   ‚îú‚îÄ‚îÄ project_tree.txt
‚îÇ   ‚îú‚îÄ‚îÄ quick_test.py
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ test.txt
‚îÇ   ‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qa.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ courses.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pedagogy.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deps.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ roles.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ users.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ courses/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ access.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db_config.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faithfulness.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ groundedness.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ runner.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guardrails/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ citation.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grounding.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rate_limit.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audit.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pedagogy/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bloom.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generator.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rubric.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ templates.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validators.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_retriever.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunking.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ citation.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_extraction.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vectorstore/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bm25_store.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faiss_store.py
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app_database.db
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audit.log
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bm25_Ml101.pkl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bm25/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faiss_index/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Ml101/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.faiss
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.pkl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raw_docs/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 694467e3464045b9803fbba62508179e.txt
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 83dee78d344649c393659de375ce52e0.txt
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project_export.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ uploads/
‚îÇ   ‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package-lock.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vite.config.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatBox.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FileUpload.jsx
‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_eval.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_eval_pipeline.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_guardrails.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_rag_guardrail.py
‚îÇ   ‚îú‚îÄ‚îÄ uploads/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_knowledge.txt
==============================
FILE: .\quick_test.py
==============================
import os
import sys
from pathlib import Path

# ƒê·∫£m b·∫£o Python t√¨m th·∫•y module backend
sys.path.append(str(Path(__file__).parent))

def setup_and_test():
    print("üöÄ B·∫Øt ƒë·∫ßu ki·ªÉm tra h·ªá th·ªëng...")

    # 1. T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt
    folders = ["data/faiss_index", "data/bm25", "uploads"]
    for f in folders:
        os.makedirs(f, exist_ok=True)
        print(f"‚úÖ ƒê√£ t·∫°o/ki·ªÉm tra th∆∞ m·ª•c: {f}")

    # 2. T·∫°o file d·ªØ li·ªáu m·∫´u ƒë·ªÉ test
    sample_file = "uploads/test_knowledge.txt"
    with open(sample_file, "w", encoding="utf-8") as f:
        f.write("Th·ªß ƒë√¥ c·ªßa Vi·ªát Nam l√† H√† N·ªôi. Kh√≥a h·ªçc ML101 d·∫°y v·ªÅ Machine Learning c∆° b·∫£n.")
    print(f"‚úÖ ƒê√£ t·∫°o file d·ªØ li·ªáu m·∫´u: {sample_file}")

    # 3. Test Ingest
    print("üì• ƒêang n·∫°p d·ªØ li·ªáu v√†o Vector DB...")
    from backend.rag.ingest import ingest_document
    try:
        ingest_document(sample_file, "ML101")
        print("‚úÖ Ingest th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå L·ªói Ingest: {e}")
        return

    # 4. Test Query
    print("üîç ƒêang th·ª≠ h·ªèi AI...")
    from backend.agent.qa import answer_question
    result = answer_question("Th·ªß ƒë√¥ c·ªßa Vi·ªát Nam l√† g√¨?", "ML101")
    
    print("\n--- K·∫æT QU·∫¢ TEST ---")
    print(f"C√¢u h·ªèi: Th·ªß ƒë√¥ c·ªßa Vi·ªát Nam l√† g√¨?")
    print(f"AI tr·∫£ l·ªùi: {result['answer']}")
    if result['sources']:
        print(f"Ngu·ªìn t√¨m th·∫•y: {len(result['sources'])} ƒëo·∫°n vƒÉn.")
        print("üéâ H·ªÜ TH·ªêNG ƒê√É S·∫¥N S√ÄNG!")
    else:
        print("‚ö†Ô∏è AI tr·∫£ l·ªùi nh∆∞ng kh√¥ng t√¨m th·∫•y ngu·ªìn. Ki·ªÉm tra l·∫°i hybrid_retriever.")

if __name__ == "__main__":
    setup_and_test()
==============================
FILE: .\README.md
==============================
# AI Tr·ª£ Gi·∫£ng (RAG-based Tutor Assistant)

D·ª± √°n AI tr·ª£ gi·∫£ng s·ª≠ d·ª•ng ki·∫øn tr√∫c **RAG (Retrieval-Augmented Generation)**:
- Ingest t√†i li·ªáu h·ªçc t·∫≠p (TXT / PDF / DOCX)
- L∆∞u vector b·∫±ng FAISS
- Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu (kh√¥ng b·ªãa)

---

## 1. C·∫•u tr√∫c d·ª± √°n

backend/
agent/ # AI tutor logic
rag/ # ingest + retriever
api/ # FastAPI endpoints
main.py
data/
raw_docs/ # t√†i li·ªáu ƒë·∫ßu v√†o (GV upload)
faiss_index/ # vector database


---

## 2. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt

==============================
FILE: .\requirements.txt
==============================
/*************  ‚ú® Windsurf Command üåü  *************/
fastapi==0.109.0
uvicorn==0.27.0
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
bcrypt==4.1.2
langchain==0.1.0
langchain-google-genai==0.0.5
langchain-community==0.0.10
langchain-huggingface==0.0.1
faiss-cpu==1.7.4
rank_bm25==0.2.2
pydantic==2.5.3
python-dotenv==1.0.0
sentence-transformers==2.2.2
/*******  58c2cc60-25cb-4ee9-95aa-e526eee02690  *******/
==============================
FILE: .\test.txt
==============================
H·ªçc m√°y l√† m·ªôt lƒ©nh v·ª±c c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o.

==============================
FILE: .\backend\__init__.py
==============================

==============================
FILE: .\backend\agent\prompt.py
==============================
SYSTEM_PROMPT = """
B·∫°n l√† AI tr·ª£ gi·∫£ng h·ªçc thu·∫≠t.

Quy t·∫Øc b·∫Øt bu·ªôc:
- Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu, h√£y tr·∫£ l·ªùi: 
  "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu."
- Kh√¥ng ƒë∆∞·ª£c suy ƒëo√°n ho·∫∑c b·ªãa.
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ƒë√∫ng tr·ªçng t√¢m.
"""

==============================
FILE: .\backend\agent\qa.py
==============================
from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import llm_instance
from backend.agent.prompt import SYSTEM_PROMPT

def answer_question(question: str, course_id: str = "default_course") -> dict:
    """
    Logic tr·∫£ l·ªùi c√¢u h·ªèi RAG.
    FIX: Th√™m default course_id v√† handle out-of-scope.
    """
    # 1. T√¨m ki·∫øm context
    context_docs = hybrid_search(query=question, course_id=course_id)

    # 2. Guardrail: N·∫øu kh√¥ng t√¨m th·∫•y t√†i li·ªáu (test_out_of_scope_question)
    if not context_docs:
        return {
            "answer": "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu ho·∫∑c c√¢u h·ªèi n·∫±m ngo√†i ph·∫°m vi h·ªó tr·ª£.",
            "sources": []
        }

    # 3. Chu·∫©n b·ªã Prompt
    context_text = "\n---\n".join([doc["content"] for doc in context_docs[:3]])
    full_prompt = f"{SYSTEM_PROMPT}\n\nNg·ªØ c·∫£nh:\n{context_text}\n\nC√¢u h·ªèi: {question}\nTr·∫£ l·ªùi:"

    try:
        response = llm_instance.invoke(full_prompt)
        return {
            "answer": response,
            "sources": context_docs
        }
    except Exception as e:
        return {
            "answer": f"ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω: {str(e)}",
            "sources": []
        }
==============================
FILE: .\backend\agent\__init__.py
==============================

==============================
FILE: .\backend\api\auth.py
==============================
from fastapi import APIRouter, Depends, HTTPException, status, Request
from fastapi.security import OAuth2PasswordRequestForm
from backend.auth.users import get_user, verify_password
from backend.auth.security import create_access_token
from pydantic import BaseModel

router = APIRouter()

class LoginRequest(BaseModel):
    username: str
    password: str

@router.post("/login")
async def login(request: Request, form_data: OAuth2PasswordRequestForm = Depends()):
    """
    H·ªó tr·ª£ c·∫£ OAuth2 Form (Swagger) v√† JSON (Frontend)
    """
    username, password = None, None
    
    # Ki·ªÉm tra xem l√† JSON hay Form
    content_type = request.headers.get("Content-Type", "")
    if "application/json" in content_type:
        data = await request.json()
        username = data.get("username")
        password = data.get("password")
    else:
        username = form_data.username
        password = form_data.password

    user = get_user(username)
    if not user or not verify_password(password, user["password_hash"]):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="T√™n ƒëƒÉng nh·∫≠p ho·∫∑c m·∫≠t kh·∫©u kh√¥ng ch√≠nh x√°c",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    access_token = create_access_token(data={"sub": user["username"], "role": user["role"]})
    
    return {
        "access_token": access_token, 
        "token_type": "bearer",
        "role": user["role"]
    }
==============================
FILE: .\backend\api\chat.py
==============================
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from backend.agent.qa import answer_question

router = APIRouter()

class ChatRequest(BaseModel):
    course_id: str
    question: str

@router.post("/")
async def chat(request: ChatRequest):
    try:
        # G·ªçi tr·ª±c ti·∫øp kh√¥ng c·∫ßn current_user
        result = answer_question(request.question, request.course_id)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω chat: {str(e)}")
==============================
FILE: .\backend\api\courses.py
==============================
from fastapi import APIRouter, UploadFile, File, Depends

router = APIRouter()

# ƒê√É V√î HI·ªÜU H√ìA ƒê·ªÇ D√ôNG CHUNG /upload/ CHO MVP
# @router.post("/{course_id}/upload")
# async def upload_to_course(course_id: str, file: UploadFile = File(...)):
#     return {"message": "Deprecated. Use /upload/ instead"}
==============================
FILE: .\backend\api\eval.py
==============================
from fastapi import APIRouter
from backend.eval.runner import run_evaluation

router = APIRouter(prefix="/eval", tags=["Eval"])

@router.post(
    "/run",
    operation_id="eval_run"
)
def run_eval():
    return run_evaluation()

==============================
FILE: .\backend\api\pedagogy.py
==============================
from fastapi import APIRouter

router = APIRouter(prefix="/pedagogy", tags=["Pedagogy"])

@router.post(
    "/generate",
    operation_id="pedagogy_generate"
)
def generate():
    return {"status": "ok"}

==============================
FILE: .\backend\api\upload.py
==============================
import os
import uuid
from fastapi import APIRouter, UploadFile, File, HTTPException
from backend.rag.ingest import ingest_document

router = APIRouter()

UPLOAD_DIR = "data/raw_docs"

@router.post("/")
async def upload_document(course_id: str, file: UploadFile = File(...)):
    if not os.path.exists(UPLOAD_DIR):
        os.makedirs(UPLOAD_DIR)

    # FIX: T·∫°o t√™n file duy nh·∫•t tr√°nh ghi ƒë√® [cite: 20]
    file_extension = os.path.splitext(file.filename)[1]
    unique_filename = f"{uuid.uuid4().hex}{file_extension}"
    file_path = os.path.join(UPLOAD_DIR, unique_filename)
    
    try:
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        # Ingest d·ªØ li·ªáu v√†o vector store theo course_id [cite: 24, 84]
        ingest_document(file_path, course_id)

        return {
            "message": f"Upload th√†nh c√¥ng file {file.filename}",
            "saved_as": unique_filename
        }
    except Exception as e:
        if os.path.exists(file_path):
            os.remove(file_path)
        raise HTTPException(status_code=500, detail=str(e))
==============================
FILE: .\backend\api\__init__.py
==============================

==============================
FILE: .\backend\auth\deps.py
==============================
from fastapi import Depends
from backend.auth.users import USERS_DB

# Mock user ƒë·ªÉ c√°c logic ph√≠a sau kh√¥ng b·ªã crash
DEFAULT_USER = USERS_DB["teacher1"]

def get_current_user():
    """Bypass ho√†n to√†n - Lu√¥n tr·∫£ v·ªÅ user m·∫∑c ƒë·ªãnh"""
    return DEFAULT_USER

def require_teacher(current_user=Depends(get_current_user)):
    """Bypass role check"""
    return current_user

def require_student(current_user=Depends(get_current_user)):
    """Bypass role check"""
    return current_user
==============================
FILE: .\backend\auth\roles.py
==============================
from enum import Enum

class UserRole(str, Enum):
    STUDENT = "student"
    TEACHER = "teacher"

==============================
FILE: .\backend\auth\security.py
==============================
from datetime import datetime, timedelta
from jose import jwt, JWTError

# C·∫•u h√¨nh c·ª©ng ƒë·ªÉ dev local kh√¥ng b·ªã invalid token khi restart
SECRET_KEY = "DEV_SECRET_KEY_LOCAL_123"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 1440 # 24 gi·ªù

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def verify_token(token: str):
    """Gi·∫£i m√£ token ƒë·ªÉ l·∫•y payload"""
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload if payload.get("sub") else None
    except JWTError:
        return None
==============================
FILE: .\backend\auth\users.py
==============================
from passlib.context import CryptContext

# Kh·ªüi t·∫°o context: passlib t·ª± x·ª≠ l√Ω encoding v√† salt chu·∫©n cho bcrypt
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """So s√°nh m·∫≠t kh·∫©u plaintext v√† m·∫≠t kh·∫©u ƒë√£ hash"""
    try:
        return pwd_context.verify(plain_password, hashed_password)
    except Exception:
        return False

def get_password_hash(password: str) -> str:
    """T·∫°o hash m·ªõi cho m·∫≠t kh·∫©u"""
    return pwd_context.hash(password)

# M·∫¨T KH·∫®U TEST L√Ä: 123456
# Hash n√†y ƒë√£ ƒë∆∞·ª£c t·∫°o v√† ki·ªÉm tra b·∫±ng passlib
CLEAN_HASH = "$2b$12$R9h/lIPzMZ7E22NVjtORV.p6zif6oUaQW.mHnc6vSOfKy8nS.E8S."

USERS_DB = {
    "teacher1": {
        "username": "teacher1",
        "password_hash": CLEAN_HASH,
        "role": "teacher"
    },
    "student1": {
        "username": "student1",
        "password_hash": CLEAN_HASH,
        "role": "student"
    }
}

def get_user(username: str):
    return USERS_DB.get(username.lower())
==============================
FILE: .\backend\courses\access.py
==============================
def check_course_access(user, course):
    if user["role"] == "teacher":
        return course.owner_id == user["id"]
    if user["role"] == "student":
        # MVP: cho ph√©p t·∫•t c·∫£ SV (sau n√†y g·∫Øn class)
        return True
    return False

==============================
FILE: .\backend\courses\models.py
==============================
from dataclasses import dataclass

@dataclass
class Course:
    course_id: str
    name: str
    owner_id: str  # gi·∫£ng vi√™n

==============================
FILE: .\backend\courses\service.py
==============================
from backend.courses.models import Course

# MVP: in-memory, sau n√†y thay DB
COURSES = {}

def create_course(course_id: str, name: str, owner_id: str):
    course = Course(course_id, name, owner_id)
    COURSES[course_id] = course
    return course

def get_course(course_id: str):
    return COURSES.get(course_id)

def list_courses_by_owner(owner_id: str):
    return [c for c in COURSES.values() if c.owner_id == owner_id]

==============================
FILE: .\backend\courses\__init__.py
==============================

==============================
FILE: .\backend\database\db_config.py
==============================
import sqlite3
from pathlib import Path

DB_PATH = Path("data/app_database.db")

def get_db_connection():
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row # ƒê·ªÉ truy xu·∫•t theo t√™n c·ªôt
    return conn

def init_db():
    conn = get_db_connection()
    # T·∫°o b·∫£ng kh√≥a h·ªçc v√† b·∫£ng ng∆∞·ªùi d√πng n·∫øu ch∆∞a c√≥
    conn.execute('''
        CREATE TABLE IF NOT EXISTS courses (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            teacher_id TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    conn.close()
==============================
FILE: .\backend\eval\dataset.json
==============================
{
  "version": "1.0",
  "description": "Evaluation dataset for AI Tro Giang ‚Äì Groundedness & Faithfulness",
  "samples": [
    {
      "id": "Q1",
      "question": "H·ªçc m√°y l√† g√¨?",
      "expected_answer_type": "definition",
      "expected_chunks": [
        {
          "doc_id": "ANY",
          "chunk_id": "ANY"
        }
      ],
      "notes": "C√¢u h·ªèi ƒë·ªãnh nghƒ©a c∆° b·∫£n, ph·∫£i c√≥ tr√≠ch d·∫´n t·ª´ t√†i li·ªáu."
    },
    {
      "id": "Q2",
      "question": "M√¥ h√¨nh h·ªçc m√°y ƒë∆∞·ª£c hu·∫•n luy·ªán b·∫±ng c√°ch n√†o?",
      "expected_answer_type": "explanation",
      "expected_chunks": [
        {
          "doc_id": "ANY",
          "chunk_id": "ANY"
        }
      ],
      "notes": "Y√™u c·∫ßu gi·∫£i th√≠ch quy tr√¨nh hu·∫•n luy·ªán d·ª±a tr√™n d·ªØ li·ªáu."
    },
    {
      "id": "Q3",
      "question": "S·ª± kh√°c nhau gi·ªØa h·ªçc c√≥ gi√°m s√°t v√† kh√¥ng gi√°m s√°t?",
      "expected_answer_type": "comparison",
      "expected_chunks": [
        {
          "doc_id": "ANY",
          "chunk_id": "ANY"
        }
      ],
      "notes": "Ph·∫£i c√≥ √≠t nh·∫•t 2 chunk kh√°c nhau cho so s√°nh."
    },
    {
      "id": "Q4",
      "question": "Thu·∫≠t to√°n KNN ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?",
      "expected_answer_type": "algorithm",
      "expected_chunks": [
        {
          "doc_id": "ANY",
          "chunk_id": "ANY"
        }
      ],
      "notes": "N·∫øu t√†i li·ªáu kh√¥ng c√≥ KNN th√¨ h·ªá th·ªëng PH·∫¢I t·ª´ ch·ªëi."
    },
    {
      "id": "Q5",
      "question": "Th·ªùi ti·∫øt h√¥m nay ·ªü H√† N·ªôi nh∆∞ th·∫ø n√†o?",
      "expected_answer_type": "out_of_scope",
      "expected_chunks": [],
      "notes": "C√¢u h·ªèi ngo√†i ph·∫°m vi ‚Äì ph·∫£i b·ªã t·ª´ ch·ªëi."
    }
  ]
}

==============================
FILE: .\backend\eval\datasets.py
==============================
from dataclasses import dataclass
from typing import List

@dataclass
class EvalSample:
    id: int
    question: str
    expected_answer: str | None = None

EVAL_DATASET: List[EvalSample] = [
    EvalSample(
        id=1,
        question="FAISS l√† g√¨ trong h·ªá th·ªëng RAG?"
    ),
    EvalSample(
        id=2,
        question="BM25 d√πng ƒë·ªÉ l√†m g√¨?"
    ),
]

==============================
FILE: .\backend\eval\evaluator.py
==============================
from backend.rag.hybrid_retriever import hybrid_search

MIN_RETRIEVAL_SCORE = 0.15

def evaluate_sample(sample: dict) -> dict:
    """
    sample = {
        id: int,
        question: str
    }
    """

    question = sample.get("question")
    if not question:
        raise ValueError("Missing question in eval sample")

    results = hybrid_search(question)

    grounded = False
    used_chunks = []

    for doc, score in results:
        if score >= MIN_RETRIEVAL_SCORE:
            grounded = True
            used_chunks.append({
                "chunk_id": doc.metadata.get("chunk_id"),
                "source_file": doc.metadata.get("source_file"),
                "score": round(score, 4)
            })

    return {
        "id": sample.get("id"),
        "question": question,
        "grounded": grounded,
        "num_chunks": len(used_chunks),
        "chunks": used_chunks,
        "score": 1.0 if grounded else 0.0
    }

==============================
FILE: .\backend\eval\faithfulness.py
==============================
import re
from typing import Dict, Any, List


STOPWORDS = {
    "l√†", "v√†", "c·ªßa", "trong", "cho", "v·ªõi", "m·ªôt", "c√°c",
    "ƒë∆∞·ª£c", "khi", "n√†y", "ƒë√≥", "nh∆∞", "ƒë·ªÉ"
}


def normalize(text: str) -> List[str]:
    """
    Normalize text into keyword tokens
    """
    text = text.lower()
    text = re.sub(r"[^a-z0-9√†-·ªπ\s]", " ", text)
    tokens = text.split()
    return [t for t in tokens if t not in STOPWORDS and len(t) > 2]


def extract_claim_sentences(answer: str) -> List[str]:
    """
    Split answer into claim sentences (remove chunk tags)
    """
    clean = re.sub(r"\[CHUNK_\d+\]", "", answer)
    sentences = re.split(r"[.\n]", clean)
    return [s.strip() for s in sentences if len(s.strip()) > 20]


def check_faithfulness(
    answer: str,
    contexts: List[str],
    min_overlap_ratio: float = 0.3
) -> Dict[str, Any]:
    """
    Faithfulness check:
    - Each claim sentence must overlap sufficiently with context keywords
    """

    context_text = " ".join(contexts)
    context_tokens = set(normalize(context_text))

    claims = extract_claim_sentences(answer)

    if not claims:
        return {
            "faithful": False,
            "reason": "NO_CLAIM_SENTENCE",
            "details": "No valid claim sentence found in answer."
        }

    unfaithful_claims = []

    for claim in claims:
        claim_tokens = normalize(claim)

        if not claim_tokens:
            continue

        overlap = set(claim_tokens) & context_tokens
        overlap_ratio = len(overlap) / len(set(claim_tokens))

        if overlap_ratio < min_overlap_ratio:
            unfaithful_claims.append({
                "claim": claim,
                "overlap_ratio": round(overlap_ratio, 2)
            })

    if unfaithful_claims:
        return {
            "faithful": False,
            "reason": "LOW_CONTEXT_OVERLAP",
            "details": unfaithful_claims
        }

    return {
        "faithful": True,
        "reason": "OK",
        "details": "All claims sufficiently supported by context."
    }

==============================
FILE: .\backend\eval\groundedness.py
==============================
import re
from typing import Dict, Any, List


CHUNK_PATTERN = re.compile(r"\[CHUNK_(\d+)\]")


def extract_chunk_ids(answer: str) -> List[int]:
    """
    Extract chunk indices from answer text.
    Example: [CHUNK_0], [CHUNK_2] ‚Üí [0, 2]
    """
    return [int(x) for x in CHUNK_PATTERN.findall(answer)]


def check_groundedness(
    answer: str,
    sources: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Groundedness check:
    - Answer must contain chunk tags
    - Chunk tags must exist in retrieved sources
    """

    chunk_ids_in_answer = extract_chunk_ids(answer)

    if not chunk_ids_in_answer:
        return {
            "grounded": False,
            "reason": "NO_CHUNK_TAG",
            "details": "Answer does not contain any [CHUNK_x] tags."
        }

    valid_chunk_ids = {
        src.get("chunk_id") for src in sources
    }

    invalid_refs = [
        cid for cid in chunk_ids_in_answer
        if cid not in valid_chunk_ids
    ]

    if invalid_refs:
        return {
            "grounded": False,
            "reason": "INVALID_CHUNK_REFERENCE",
            "details": f"Referenced chunk(s) not in sources: {invalid_refs}"
        }

    return {
        "grounded": True,
        "reason": "OK",
        "details": f"All referenced chunks are valid: {chunk_ids_in_answer}"
    }

==============================
FILE: .\backend\eval\metrics.py
==============================
def groundedness_score(sources, expected_sources):
    if not sources:
        return 0.0

    matched = 0
    for src in sources:
        source_file = src.get("source_file", "")
        if any(exp in source_file for exp in expected_sources):
            matched += 1

    return matched / len(expected_sources)


def citation_coverage(sources):
    if not sources:
        return 0.0

    cited = [s for s in sources if s.get("page") is not None]
    return len(cited) / len(sources)


def hallucination_flag(sources):
    return len(sources) == 0

==============================
FILE: .\backend\eval\runner.py
==============================
from backend.eval.datasets import EVAL_DATASET
from backend.eval.evaluator import evaluate_sample
from backend.logging.audit import audit_log

def run_evaluation():
    results = []

    for sample in EVAL_DATASET:
        result = evaluate_sample({
            "id": sample.id,
            "question": sample.question
        })

        results.append(result)

        audit_log(
            user="system_eval",
            action="evaluation_run",
            payload=result
        )

    return results

==============================
FILE: .\backend\eval\__init__.py
==============================

==============================
FILE: .\backend\guardrails\citation.py
==============================
def format_citations(source_documents):
    citations = []
    for doc in source_documents:
        meta = doc.metadata
        citations.append({
            "source": meta.get("source", "unknown"),
            "page": meta.get("page", None)
        })
    return citations

==============================
FILE: .\backend\guardrails\grounding.py
==============================
from typing import List

MIN_DOCS_REQUIRED = 1

def check_grounding(source_documents: List):
    if not source_documents or len(source_documents) < MIN_DOCS_REQUIRED:
        raise ValueError(
            "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan. "
            "H·ªá th·ªëng kh√¥ng th·ªÉ tr·∫£ l·ªùi ƒë·ªÉ tr√°nh sai l·ªách h·ªçc thu·∫≠t."
        )

==============================
FILE: .\backend\guardrails\rate_limit.py
==============================
import time

REQUEST_LIMIT = 20
WINDOW_SECONDS = 60

_user_requests = {}

def check_rate_limit(user_id: str):
    now = time.time()
    timestamps = _user_requests.get(user_id, [])

    timestamps = [t for t in timestamps if now - t < WINDOW_SECONDS]

    if len(timestamps) >= REQUEST_LIMIT:
        raise ValueError("B·∫°n g·ª≠i qu√° nhi·ªÅu y√™u c·∫ßu. Vui l√≤ng th·ª≠ l·∫°i sau.")

    timestamps.append(now)
    _user_requests[user_id] = timestamps

==============================
FILE: .\backend\llm\llm.py
==============================
import re

class LocalMockLLM:
    """
    Mock LLM Engine: X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi d·ª±a tr√™n Retrieval Context.
    ƒê·∫£m b·∫£o kh√¥ng ph·ª• thu·ªôc v√†o th∆∞ vi·ªán b√™n ngo√†i (g4f, openai, etc.)
    """
    def __init__(self, model_name: str = "offline-pedagogy-v1"):
        self.model_name = model_name

    def invoke(self, prompt: str) -> str:
        # 1. Tr√≠ch xu·∫•t ng·ªØ c·∫£nh t·ª´ prompt (D·ª±a tr√™n c·∫•u tr√∫c trong qa.py)
        # T√¨m n·ªôi dung n·∫±m gi·ªØa 'Ng·ªØ c·∫£nh t√†i li·ªáu:' v√† 'C√¢u h·ªèi ng∆∞·ªùi d√πng:'
        context_match = re.search(r"Ng·ªØ c·∫£nh t√†i li·ªáu:(.*?)C√¢u h·ªèi ng∆∞·ªùi d√πng:", prompt, re.DOTALL)
        
        if not context_match:
            return "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong t√†i li·ªáu c·ªßa kh√≥a h·ªçc n√†y."

        context = context_match.group(1).strip()
        
        # 2. Logic Rule-based: Tr·∫£ v·ªÅ ƒëo·∫°n vƒÉn b·∫£n ph√π h·ª£p nh·∫•t t·ª´ context
        # ·ªû ƒë√¢y ta ∆∞u ti√™n tr·∫£ v·ªÅ to√†n b·ªô ng·ªØ c·∫£nh ƒë√£ ƒë∆∞·ª£c t√¨m th·∫•y b·ªüi RAG
        if len(context) > 5:
            return f"D·ª±a tr√™n t√†i li·ªáu kh√≥a h·ªçc, t√¥i xin gi·∫£i ƒë√°p nh∆∞ sau:\n\n{context}"
        
        return "T√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p kh√¥ng ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

# Kh·ªüi t·∫°o instance duy nh·∫•t cho to√†n h·ªá th·ªëng
llm_instance = LocalMockLLM()

def get_llm():
    return llm_instance
==============================
FILE: .\backend\llm\__init__.py
==============================

==============================
FILE: .\backend\logging\audit.py
==============================
import json
import uuid
from datetime import datetime
from pathlib import Path
from typing import List

LOG_PATH = Path("data/audit.log")

def audit_log(
    user: str,
    action: str,
    payload: dict,
    course_id: str | None = None,
    chunk_ids: List[str] | None = None,
    model_name: str | None = None,
    request_id: str | None = None
):
    LOG_PATH.parent.mkdir(parents=True, exist_ok=True)

    record = {
        "timestamp": datetime.utcnow().isoformat(),
        "request_id": request_id or str(uuid.uuid4()),
        "user": user,
        "action": action,
        "course_id": course_id,
        "chunk_ids": chunk_ids or [],
        "model": model_name or "offline_stub",
        "payload": payload,
        "schema": "academic_audit_v3"
    }

    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

==============================
FILE: .\backend\pedagogy\bloom.py
==============================
from enum import Enum
from typing import List


class BloomLevel(str, Enum):
    """
    Bloom's Taxonomy ‚Äì Cognitive Domain
    """

    REMEMBER = "remember"
    UNDERSTAND = "understand"
    APPLY = "apply"
    ANALYZE = "analyze"
    EVALUATE = "evaluate"
    CREATE = "create"

    @property
    def description(self) -> str:
        return {
            self.REMEMBER: "Nh·ªõ l·∫°i th√¥ng tin, ƒë·ªãnh nghƒ©a, thu·∫≠t ng·ªØ",
            self.UNDERSTAND: "Gi·∫£i th√≠ch, m√¥ t·∫£, di·ªÖn gi·∫£i √Ω nghƒ©a",
            self.APPLY: "√Åp d·ª•ng ki·∫øn th·ª©c v√†o b√†i to√°n c·ª• th·ªÉ",
            self.ANALYZE: "Ph√¢n t√≠ch, so s√°nh, ch·ªâ ra m·ªëi quan h·ªá",
            self.EVALUATE: "ƒê√°nh gi√°, nh·∫≠n x√©t, ph·∫£n bi·ªán",
            self.CREATE: "T·ªïng h·ª£p, thi·∫øt k·∫ø, x√¢y d·ª±ng m·ªõi"
        }[self]

    @property
    def action_verbs(self) -> List[str]:
        """
        G·ª£i √Ω ƒë·ªông t·ª´ d√πng trong c√¢u h·ªèi / b√†i t·∫≠p
        """
        return {
            self.REMEMBER: ["li·ªát k√™", "n√™u", "ƒë·ªãnh nghƒ©a", "k·ªÉ t√™n"],
            self.UNDERSTAND: ["gi·∫£i th√≠ch", "tr√¨nh b√†y", "m√¥ t·∫£", "t√≥m t·∫Øt"],
            self.APPLY: ["√°p d·ª•ng", "t√≠nh to√°n", "gi·∫£i", "th·ª±c hi·ªán"],
            self.ANALYZE: ["ph√¢n t√≠ch", "so s√°nh", "ph√¢n lo·∫°i", "l√†m r√µ"],
            self.EVALUATE: ["ƒë√°nh gi√°", "nh·∫≠n x√©t", "ph·∫£n bi·ªán", "so s√°nh ∆∞u nh∆∞·ª£c ƒëi·ªÉm"],
            self.CREATE: ["thi·∫øt k·∫ø", "x√¢y d·ª±ng", "ƒë·ªÅ xu·∫•t", "ph√°t tri·ªÉn"]
        }[self]

    @classmethod
    def ordered_levels(cls) -> List["BloomLevel"]:
        """
        Tr·∫£ v·ªÅ Bloom levels theo th·ª© t·ª± tƒÉng d·∫ßn ƒë·ªô kh√≥
        """
        return [
            cls.REMEMBER,
            cls.UNDERSTAND,
            cls.APPLY,
            cls.ANALYZE,
            cls.EVALUATE,
            cls.CREATE
        ]

==============================
FILE: .\backend\pedagogy\generator.py
==============================
import json
import re
from backend.llm.llm import llm_instance

def clean_json_response(raw_text: str) -> str:
    """
    Lo·∫°i b·ªè c√°c k√Ω t·ª± Markdown th·ª´a [cite: 65]
    """
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p LLM tr·∫£ v·ªÅ ```json ... ``` ho·∫∑c vƒÉn b·∫£n bao quanh JSON
    json_match = re.search(r"(\{.*\}|\[.*\])", raw_text, re.DOTALL)
    if json_match:
        return json_match.group(1).strip()
    return raw_text.strip()

def generate_lesson_plan(topic: str, context: str = ""):
    prompt = f"""
    H√£y ƒë√≥ng vai l√† m·ªôt chuy√™n gia s∆∞ ph·∫°m.
    Ch·ªß ƒë·ªÅ: {topic}
    Ng·ªØ c·∫£nh t√†i li·ªáu: {context}
    
    H√£y t·∫°o m·ªôt gi√°o √°n chi ti·∫øt d∆∞·ªõi ƒë·ªãnh d·∫°ng JSON v·ªõi c√°c tr∆∞·ªùng:
    - title, objectives, activities
    Ch·ªâ tr·∫£ v·ªÅ JSON. [cite: 66, 67, 68]
    """
    
    raw_response = llm_instance.invoke(prompt)
    cleaned_json = clean_json_response(raw_response)
    
    try:
        return json.loads(cleaned_json)
    except json.JSONDecodeError:
        # Fallback n·∫øu JSON v·∫´n l·ªói [cite: 69]
        return {
            "title": topic,
            "objectives": ["Kh√¥ng th·ªÉ t·∫°o m·ª•c ti√™u t·ª± ƒë·ªông"],
            "activities": [],
            "error": "AI tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá"
        }
==============================
FILE: .\backend\pedagogy\prompts.py
==============================
from backend.pedagogy.bloom import BloomLevel


QUESTION_PROMPT_TEMPLATE = """
B·∫°n l√† AI Tr·ª£ Gi·∫£ng.

NHI·ªÜM V·ª§:
Sinh {num_items} C√ÇU H·ªéI h·ªçc t·∫≠p theo m·ª©c ƒë·ªô Bloom: {bloom_level}

M√î T·∫¢ M·ª®C ƒê·ªò:
{bloom_description}

Y√äU C·∫¶U B·∫ÆT BU·ªòC:
- CH·ªà s·ª≠ d·ª•ng th√¥ng tin trong T√ÄI LI·ªÜU b√™n d∆∞·ªõi
- Kh√¥ng suy ƒëo√°n, kh√¥ng th√™m ki·∫øn th·ª©c b√™n ngo√†i
- M·ªói c√¢u h·ªèi ph·∫£i b√°m s√°t n·ªôi dung t√†i li·ªáu
- M·ª©c ƒë·ªô nh·∫≠n th·ª©c ph·∫£i ƒë√∫ng Bloom level
- Kh√¥ng tr√πng l·∫∑p c√¢u h·ªèi

T√ÄI LI·ªÜU:
{context}

ƒê·ªäNH D·∫†NG TR·∫¢ V·ªÄ (JSON):
[
  {{
    "question": "...",
    "bloom_level": "{bloom_level}",
    "learning_objective": "...",
    "difficulty": "easy | medium | hard"
  }}
]
"""


ASSIGNMENT_PROMPT_TEMPLATE = """
B·∫°n l√† AI Tr·ª£ Gi·∫£ng.

NHI·ªÜM V·ª§:
Sinh {num_items} B√ÄI T·∫¨P h·ªçc t·∫≠p theo m·ª©c ƒë·ªô Bloom: {bloom_level}

M√î T·∫¢ M·ª®C ƒê·ªò:
{bloom_description}

Y√äU C·∫¶U B·∫ÆT BU·ªòC:
- CH·ªà s·ª≠ d·ª•ng th√¥ng tin trong T√ÄI LI·ªÜU b√™n d∆∞·ªõi
- Kh√¥ng suy ƒëo√°n, kh√¥ng th√™m ki·∫øn th·ª©c b√™n ngo√†i
- B√†i t·∫≠p ph·∫£i y√™u c·∫ßu ng∆∞·ªùi h·ªçc v·∫≠n d·ª•ng ƒë√∫ng m·ª©c Bloom
- C√≥ m√¥ t·∫£ r√µ y√™u c·∫ßu ƒë·∫ßu ra

T√ÄI LI·ªÜU:
{context}

ƒê·ªäNH D·∫†NG TR·∫¢ V·ªÄ (JSON):
[
  {{
    "title": "...",
    "task": "...",
    "expected_output": "...",
    "bloom_level": "{bloom_level}",
    "difficulty": "easy | medium | hard"
  }}
]
"""


def build_question_prompt(
    context: str,
    bloom: BloomLevel,
    num_items: int = 3
) -> str:
    return QUESTION_PROMPT_TEMPLATE.format(
        num_items=num_items,
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )


def build_assignment_prompt(
    context: str,
    bloom: BloomLevel,
    num_items: int = 2
) -> str:
    return ASSIGNMENT_PROMPT_TEMPLATE.format(
        num_items=num_items,
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )

==============================
FILE: .\backend\pedagogy\rubric.py
==============================
import json
from typing import List, Dict, Any

from backend.pedagogy.bloom import BloomLevel
from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import get_llm


CONTEXT_TOP_K = 5


def retrieve_context(topic: str) -> str:
    results = hybrid_search(topic)
    contexts = []

    for doc, _ in results[:CONTEXT_TOP_K]:
        contexts.append(doc.page_content)

    return "\n\n".join(contexts)


RUBRIC_PROMPT_TEMPLATE = """
B·∫°n l√† GI·∫¢NG VI√äN.

NHI·ªÜM V·ª§:
X√¢y d·ª±ng RUBRIC CH·∫§M ƒêI·ªÇM cho b√†i h·ªçc v·ªõi m·ª©c ƒë·ªô Bloom: {bloom_level}

M√î T·∫¢ M·ª®C ƒê·ªò:
{bloom_description}

Y√äU C·∫¶U:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin trong T√ÄI LI·ªÜU
- Rubric ph·∫£i ƒë√°nh gi√° ƒë√∫ng m·ª©c Bloom
- C√≥ nhi·ªÅu ti√™u ch√≠ ch·∫•m ƒëi·ªÉm
- C√≥ m√¥ t·∫£ r√µ cho t·ª´ng m·ª©c ƒëi·ªÉm

T√ÄI LI·ªÜU:
{context}

ƒê·ªäNH D·∫†NG TR·∫¢ V·ªÄ (JSON):
[
  {{
    "criterion": "...",
    "levels": {{
      "excellent": "...",
      "good": "...",
      "average": "...",
      "poor": "..."
    }}
  }}
]
"""


def generate_rubric(
    topic: str,
    bloom: BloomLevel
) -> List[Dict[str, Any]]:
    """
    Generate grading rubric based on Bloom level
    """

    context = retrieve_context(topic)

    if not context.strip():
        raise ValueError("No relevant learning material found.")

    prompt = RUBRIC_PROMPT_TEMPLATE.format(
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )

    llm = get_llm()
    raw = llm.invoke(prompt)

    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        raise ValueError("LLM output is not valid JSON.")

==============================
FILE: .\backend\pedagogy\schemas.py
==============================
from pydantic import BaseModel
from typing import List, Optional

class TaskRequest(BaseModel):
    task_type: str   # lesson_plan | assignment | exam | rubric | quiz
    subject: str
    clo: Optional[List[str]] = None
    bloom_level: Optional[str] = None
    difficulty: Optional[str] = None
    num_items: Optional[int] = None
    duration_minutes: Optional[int] = None

==============================
FILE: .\backend\pedagogy\templates.py
==============================
LESSON_PLAN_TEMPLATE = """
B·∫°n l√† tr·ª£ gi·∫£ng ƒë·∫°i h·ªçc.
D·ª±a CH·ªà tr√™n t√†i li·ªáu sau:
{context}

H√£y so·∫°n gi√°o √°n cho m√¥n: {subject}
CLO: {clo}
Th·ªùi l∆∞·ª£ng: {duration} ph√∫t

Y√™u c·∫ßu:
- Kh√¥ng th√™m ki·∫øn th·ª©c ngo√†i t√†i li·ªáu
- C·∫•u tr√∫c r√µ r√†ng
- C√≥ ho·∫°t ƒë·ªông GV/SV
"""

==============================
FILE: .\backend\pedagogy\validators.py
==============================
def validate_groundedness(answer, sources):
    if not sources:
        raise ValueError(
            "N·ªôi dung sinh ra kh√¥ng c√≥ cƒÉn c·ª© t·ª´ t√†i li·ªáu."
        )

==============================
FILE: .\backend\pedagogy\__init__.py
==============================

==============================
FILE: .\backend\rag\hybrid_retriever.py
==============================
from backend.vectorstore.faiss_store import get_faiss_store
from backend.vectorstore.bm25_store import BM25Store
import os

def hybrid_search(query: str, course_id: str = "default_course", k: int = 5):
    """
    T√¨m ki·∫øm k·∫øt h·ª£p Vector + BM25.
    FIX: Th√™m default course_id ƒë·ªÉ pass test v√† tr√°nh TypeError.
    """
    # B. GUARDRAILS: X·ª≠ l√Ω query r·ªóng ho·∫∑c qu√° ng·∫Øn (test_empty_query, test_short_query)
    if not query or len(query.strip()) < 2:
        return []

    # ƒê∆∞·ªùng d·∫´n index d·ª±a tr√™n course_id
    index_path = os.path.join("data", "faiss_index", course_id)
    vector_store = get_faiss_store(index_path)
    bm25_store = BM25Store.load(course_id)

    vector_results = []
    if vector_store:
        try:
            vector_results = vector_store.similarity_search_with_score(query, k=k)
        except Exception:
            vector_results = []

    bm25_results = []
    if bm25_store:
        try:
            bm25_results = bm25_store.search(query, k=k)
        except Exception:
            bm25_results = []

    combined = []
    # Format k·∫øt qu·∫£ chu·∫©n h√≥a
    for doc, score in vector_results:
        combined.append({
            "content": doc.page_content,
            "source_file": doc.metadata.get("source_file", "Unknown"),
            "score": float(score),
            "type": "vector"
        })

    for doc, score in bm25_results:
        combined.append({
            "content": doc.page_content,
            "source_file": doc.metadata.get("source_file", "Unknown"),
            "score": float(score),
            "type": "bm25"
        })

    return combined[:k]
==============================
FILE: .\backend\rag\ingest.py
==============================
import os
from backend.vectorstore.faiss_store import embeddings_instance
from langchain_community.vectorstores import FAISS
from backend.utils.text_extraction import extract_text
from backend.utils.chunking import chunk_text
from backend.vectorstore.bm25_store import BM25Store

def ingest_document(file_path: str, course_id: str):
    # 1. Tr√≠ch xu·∫•t vƒÉn b·∫£n
    text = extract_text(file_path)
    file_name = os.path.basename(file_path)

    # 2. Chia nh·ªè vƒÉn b·∫£n (S·ª≠ d·ª•ng utils ƒë·ªÉ c√≥ metadata chu·∫©n cho Eval) [cite: 78, 79]
    docs = chunk_text(text, source_file=file_name)

    # 3. X·ª≠ l√Ω Vector Store (FAISS)
    index_path = os.path.join("data", "faiss_index", course_id)
    
    if os.path.exists(os.path.join(index_path, "index.faiss")):
        vector_store = FAISS.load_local(
            index_path, 
            embeddings_instance, 
            allow_dangerous_deserialization=True
        )
        vector_store.add_documents(docs)
    else:
        vector_store = FAISS.from_documents(docs, embeddings_instance)

    if not os.path.exists(os.path.dirname(index_path)):
        os.makedirs(os.path.dirname(index_path))
    
    vector_store.save_local(index_path)

    # 4. X·ª≠ l√Ω Keyword Store (BM25) - Quan tr·ªçng ƒë·ªÉ hybrid_search kh√¥ng l·ªói [cite: 85, 87]
    bm25_store = BM25Store.load(course_id)
    bm25_store.add_documents(docs)
    bm25_store.save()

    print(f"Th√†nh c√¥ng: ƒê√£ c·∫≠p nh·∫≠t FAISS v√† BM25 cho m√¥n {course_id}")
==============================
FILE: .\backend\rag\__init__.py
==============================

==============================
FILE: .\backend\utils\chunking.py
==============================
# backend/utils/chunking.py

import uuid
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document



def chunk_text(
    text: str,
    source_file: str,
    page: int = None,
    section: str = None,
    chunk_size: int = 400,
    chunk_overlap: int = 80
) -> List[Document]:
    """
    Chunk text into Documents with full metadata
    """

    doc_id = str(uuid.uuid4())

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    chunks = splitter.split_text(text)

    documents = []

    for idx, chunk in enumerate(chunks):
        metadata = {
            "doc_id": doc_id,
            "source_file": source_file,
            "page": page,
            "section": section,
            "chunk_id": idx
        }

        documents.append(
            Document(
                page_content=chunk,
                metadata=metadata
            )
        )

    return documents

==============================
FILE: .\backend\utils\citation.py
==============================
def format_apa(source: dict) -> str:
    """
    APA 7th basic format
    """
    author = "Unknown"
    year = "n.d."
    title = source.get("source_file", "Unknown document")
    page = source.get("page")

    page_part = f" (p. {page})" if page is not None else ""

    return f"{author} ({year}). {title}{page_part}."


def format_ieee(source: dict, index: int) -> str:
    """
    IEEE basic format
    """
    title = source.get("source_file", "Unknown document")
    page = source.get("page")

    page_part = f", p. {page}" if page is not None else ""

    return f"[{index}] {title}{page_part}."

==============================
FILE: .\backend\utils\text_extraction.py
==============================
from pathlib import Path
from pypdf import PdfReader
from docx import Document as DocxDocument


def extract_text(file_path: str) -> str:
    ext = Path(file_path).suffix.lower()

    if ext == ".pdf":
        reader = PdfReader(file_path)
        return "\n".join(
            page.extract_text() or ""
            for page in reader.pages
        )

    if ext in [".docx"]:
        doc = DocxDocument(file_path)
        return "\n".join(p.text for p in doc.paragraphs)

    if ext in [".txt"]:
        with open(file_path, encoding="utf-8", errors="ignore") as f:
            return f.read()

    raise ValueError(f"Unsupported file type: {ext}")

==============================
FILE: .\backend\vectorstore\bm25_store.py
==============================
import pickle
from pathlib import Path
from typing import List, Tuple
from rank_bm25 import BM25Okapi
from langchain_core.documents import Document

class BM25Store:
    def __init__(self, course_id: str):
        self.course_id = course_id
        self.documents: List[Document] = []
        self.corpus = []
        self.bm25 = None
        self.storage_path = Path(f"data/bm25_{course_id}.pkl") # T√°ch bi·ªát theo course [cite: 82]

    def add_documents(self, docs: List[Document]):
        for doc in docs:
            tokens = doc.page_content.lower().split()
            self.documents.append(doc)
            self.corpus.append(tokens)
        self.bm25 = BM25Okapi(self.corpus)

    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        if not self.bm25:
            return []
        query_tokens = query.lower().split()
        scores = self.bm25.get_scores(query_tokens)
        ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:k]
        return [(self.documents[idx], score) for idx, score in ranked if score > 0]

    def save(self):
        self.storage_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.storage_path, "wb") as f:
            pickle.dump(self, f)

    @classmethod
    def load(cls, course_id: str):
        """
        FIX: Tr·∫£ v·ªÅ m·ªôt instance ƒë√£ load d·ªØ li·ªáu thay v√¨ instance tr·ªëng 
        """
        instance = cls(course_id)
        if instance.storage_path.exists():
            with open(instance.storage_path, "rb") as f:
                loaded_data = pickle.load(f)
                instance.documents = loaded_data.documents
                instance.corpus = loaded_data.corpus
                instance.bm25 = loaded_data.bm25
        return instance
==============================
FILE: .\backend\vectorstore\faiss_store.py
==============================
import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# S·ª≠ d·ª•ng model nh·ªè ƒë·ªÉ ti·∫øt ki·ªám RAM
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# Kh·ªüi t·∫°o singleton ƒë·ªÉ d√πng chung to√†n h·ªá th·ªëng, tr√°nh t·ªën RAM [cite: 99]
embeddings_instance = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

def get_faiss_store(index_path: str):
    """
    Load FAISS store t·ª´ ƒëƒ©a n·∫øu t·ªìn t·∫°i, n·∫øu kh√¥ng tr·∫£ v·ªÅ None [cite: 99]
    """
    if os.path.exists(os.path.join(index_path, "index.faiss")):
        return FAISS.load_local(
            index_path, 
            embeddings_instance, 
            allow_dangerous_deserialization=True
        )
    return None
==============================
FILE: .\data\raw_docs\694467e3464045b9803fbba62508179e.txt
==============================

====================
FILE: .\bundle.py
====================
import os

def bundle_code(output_file="project_code.txt"):
    extensions = ['.py', '.jsx', '.js', '.css', '.html']
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for root, dirs, files in os.walk("."):
            # B·ªè qua c√°c th∆∞ m·ª•c r√°c
            if 'venv' in root or 'node_modules' in root or '.git' in root:
                continue
            for file in files:
                if any(file.endswith(ext) for ext in extensions):
                    full_path = os.path.join(root, file)
                    outfile.write(f"\n{'='*20}\nFILE: {full_path}\n{'='*20}\n")
                    with open(full_path, 'r', encoding='utf-8', errors='ignore') as infile:
                        outfile.write(infile.read())
    print(f"ƒê√£ gom code xong v√†o file: {output_file}")

if __name__ == "__main__":
    bundle_code()
====================
FILE: .\check_imports.py
====================
import pkgutil
import importlib
import traceback

BASE_PACKAGE = "backend"

errors = []

for module in pkgutil.walk_packages([BASE_PACKAGE], BASE_PACKAGE + "."):
    name = module.name
    try:
        importlib.import_module(name)
    except Exception as e:
        errors.append((name, e))

print("\n====== IMPORT ERRORS ======")
for name, err in errors:
    print(f"\n‚ùå {name}")
    traceback.print_exception(type(err), err, err.__traceback__)

print(f"\nTotal errors: {len(errors)}")

====================
FILE: .\main.py
====================
import uvicorn
import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles

# Import c√°c router
from backend.api import auth, upload, chat, courses
from backend.database.db_config import init_db

app = FastAPI(title="AI Teaching Assistant System")

# C·∫•u h√¨nh CORS ƒë·ªÉ Frontend (Port 5173) g·ªçi ƒë∆∞·ª£c Backend (Port 8000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Trong production n√™n ƒë·ªïi th√†nh ["http://localhost:5173"]
    allow_methods=["*"],
    allow_headers=["*"],
)

# ƒêƒÉng k√Ω Router
app.include_router(auth.router, prefix="/auth", tags=["Authentication"])
app.include_router(courses.router, prefix="/courses", tags=["Courses"])
app.include_router(upload.router, prefix="/upload", tags=["Upload"])
app.include_router(chat.router, prefix="/chat", tags=["AI Chat"])

# Mount th∆∞ m·ª•c upload ƒë·ªÉ c√≥ th·ªÉ truy c·∫≠p file n·∫øu c·∫ßn (Optional)
if not os.path.exists("uploads"):
    os.makedirs("uploads")
# app.mount("/static", StaticFiles(directory="uploads"), name="static")

@app.on_event("startup")
def startup_event():
    init_db()
    print("üöÄ H·ªá th·ªëng AI Tr·ª£ gi·∫£ng ƒë√£ s·∫µn s√†ng!")
    print("üëâ Swagger UI: http://localhost:8000/docs")

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
====================
FILE: .\quick_test.py
====================
import os
import sys
from pathlib import Path

# ƒê·∫£m b·∫£o Python t√¨m th·∫•y module backend
sys.path.append(str(Path(__file__).parent))

def setup_and_test():
    print("üöÄ B·∫Øt ƒë·∫ßu ki·ªÉm tra h·ªá th·ªëng...")

    # 1. T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt
    folders = ["data/faiss_index", "data/bm25", "uploads"]
    for f in folders:
        os.makedirs(f, exist_ok=True)
        print(f"‚úÖ ƒê√£ t·∫°o/ki·ªÉm tra th∆∞ m·ª•c: {f}")

    # 2. T·∫°o file d·ªØ li·ªáu m·∫´u ƒë·ªÉ test
    sample_file = "uploads/test_knowledge.txt"
    with open(sample_file, "w", encoding="utf-8") as f:
        f.write("Th·ªß ƒë√¥ c·ªßa Vi·ªát Nam l√† H√† N·ªôi. Kh√≥a h·ªçc ML101 d·∫°y v·ªÅ Machine Learning c∆° b·∫£n.")
    print(f"‚úÖ ƒê√£ t·∫°o file d·ªØ li·ªáu m·∫´u: {sample_file}")

    # 3. Test Ingest
    print("üì• ƒêang n·∫°p d·ªØ li·ªáu v√†o Vector DB...")
    from backend.rag.ingest import ingest_document
    try:
        ingest_document(sample_file, "ML101")
        print("‚úÖ Ingest th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå L·ªói Ingest: {e}")
        return

    # 4. Test Query
    print("üîç ƒêang th·ª≠ h·ªèi AI...")
    from backend.agent.qa import answer_question
    result = answer_question("Th·ªß ƒë√¥ c·ªßa Vi·ªát Nam l√† g√¨?", "ML101")
    
    print("\n--- K·∫æT QU·∫¢ TEST ---")
    print(f"C√¢u h·ªèi: Th·ªß ƒë√¥ c·ªßa Vi·ªát Nam l√† g√¨?")
    print(f"AI tr·∫£ l·ªùi: {result['answer']}")
    if result['sources']:
        print(f"Ngu·ªìn t√¨m th·∫•y: {len(result['sources'])} ƒëo·∫°n vƒÉn.")
        print("üéâ H·ªÜ TH·ªêNG ƒê√É S·∫¥N S√ÄNG!")
    else:
        print("‚ö†Ô∏è AI tr·∫£ l·ªùi nh∆∞ng kh√¥ng t√¨m th·∫•y ngu·ªìn. Ki·ªÉm tra l·∫°i hybrid_retriever.")

if __name__ == "__main__":
    setup_and_test()
====================
FILE: .\backend\__init__.py
====================

====================
FILE: .\backend\agent\prompt.py
====================
SYSTEM_PROMPT = """
B·∫°n l√† AI tr·ª£ gi·∫£ng h·ªçc thu·∫≠t.

Quy t·∫Øc b·∫Øt bu·ªôc:
- Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu, h√£y tr·∫£ l·ªùi: 
  "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu."
- Kh√¥ng ƒë∆∞·ª£c suy ƒëo√°n ho·∫∑c b·ªãa.
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ƒë√∫ng tr·ªçng t√¢m.
"""

====================
FILE: .\backend\agent\qa.py
====================
from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import llm_instance
from backend.agent.prompt import SYSTEM_PROMPT

def answer_question(question: str, course_id: str) -> dict:
    # 1. Th·ª±c hi·ªán t√¨m ki·∫øm lai (Vector + BM25)
    context_docs = hybrid_search(query=question, course_id=course_id)

    if not context_docs:
        return {
            "answer": "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu c·ªßa kh√≥a h·ªçc n√†y.",
            "sources": []
        }

    # 2. Gom nh√≥m n·ªôi dung t√¨m ƒë∆∞·ª£c
    # L·∫•y top 3 ƒëo·∫°n vƒÉn b·∫£n c√≥ ƒëi·ªÉm s·ªë cao nh·∫•t ƒë·ªÉ l√†m ng·ªØ c·∫£nh
    context_text = "\n---\n".join([doc["content"] for doc in context_docs[:3]])

    # 3. T·∫°o Prompt (V·∫´n gi·ªØ c·∫•u tr√∫c ƒë·ªÉ sau n√†y d·ªÖ n√¢ng c·∫•p LLM th·∫≠t)
    full_prompt = f"""{SYSTEM_PROMPT}

Ng·ªØ c·∫£nh t√†i li·ªáu:
{context_text}

C√¢u h·ªèi ng∆∞·ªùi d√πng: {question}
Tr·∫£ l·ªùi:"""

    try:
        # G·ªçi Mock LLM x·ª≠ l√Ω local
        response = llm_instance.invoke(full_prompt)
    except Exception as e:
        response = f"H·ªá th·ªëng ƒëang b·∫£o tr√¨ ph·∫ßn x·ª≠ l√Ω ng√¥n ng·ªØ. Chi ti·∫øt: {str(e)}"

    return {
        "answer": response,
        "sources": context_docs
    }
====================
FILE: .\backend\agent\__init__.py
====================

====================
FILE: .\backend\api\auth.py
====================
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from backend.auth.users import get_user, verify_password
from backend.auth.security import create_access_token

router = APIRouter()

@router.post("/login")
async def login(form_data: OAuth2PasswordRequestForm = Depends()):
    """
    Endpoint Login chu·∫©n OAuth2.
    Y√™u c·∫ßu: G·ª≠i d·ªØ li·ªáu d∆∞·ªõi d·∫°ng FORM-DATA (kh√¥ng ph·∫£i JSON).
    Swagger UI s·∫Ω hi·ªÉn th·ªã 2 √¥ nh·∫≠p Username v√† Password ri√™ng bi·ªát.
    """
    # 1. L·∫•y user t·ª´ DB gi·∫£ l·∫≠p b·∫±ng form_data.username
    user = get_user(form_data.username)
    
    # 2. Ki·ªÉm tra t·ªìn t·∫°i v√† verify password (bcrypt)
    if not user or not verify_password(form_data.password, user["password_hash"]):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="T√™n ƒëƒÉng nh·∫≠p ho·∫∑c m·∫≠t kh·∫©u kh√¥ng ch√≠nh x√°c",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # 3. T·∫°o Token mang th√¥ng tin ƒë·ªãnh danh
    access_token = create_access_token(
        data={"sub": user["username"], "role": user["role"]}
    )
    
    # 4. Tr·∫£ v·ªÅ ƒë√∫ng schema OAuth2 (access_token v√† token_type l√† b·∫Øt bu·ªôc)
    return {
        "access_token": access_token, 
        "token_type": "bearer",
        "role": user["role"]
    }
====================
FILE: .\backend\api\chat.py
====================
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from backend.agent.qa import answer_question

router = APIRouter()

class ChatRequest(BaseModel):
    course_id: str
    question: str

@router.post("/")
async def chat(request: ChatRequest):
    # ƒê√£ b·ªè ph·∫ßn check_course_access ƒë·ªÉ ∆∞u ti√™n t√≠nh nƒÉng
    try:
        # G·ªçi th·∫≥ng v√†o logic x·ª≠ l√Ω RAG
        result = answer_question(request.question, request.course_id)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω: {str(e)}")
====================
FILE: .\backend\api\courses.py
====================
import os
import uuid
from fastapi import APIRouter, UploadFile, File, Depends, HTTPException
from backend.rag.ingest import ingest_document
from backend.auth.deps import require_teacher

router = APIRouter(prefix="/courses", tags=["Courses"])

@router.post("/{course_id}/upload")
def upload_document(
    course_id: str, 
    file: UploadFile = File(...), 
    user=Depends(require_teacher) # Ch·ªâ Gi√°o vi√™n m·ªõi ƒë∆∞·ª£c upload
):
    """
    T·∫£i l√™n t√†i li·ªáu v√† x·ª≠ l√Ω ƒë∆∞a v√†o Vector Store.
    S·ª≠a l·ªói: Tr√°nh tr√πng t√™n file v√† ki·ªÉm tra th∆∞ m·ª•c t·ªìn t·∫°i.
    """
    upload_dir = "data/raw_docs"
    
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c l∆∞u tr·ªØ t·ªìn t·∫°i
    if not os.path.exists(upload_dir):
        os.makedirs(upload_dir)

    # FIX: T·∫°o t√™n file duy nh·∫•t b·∫±ng UUID ƒë·ªÉ kh√¥ng b·ªã ghi ƒë√®
    file_extension = os.path.splitext(file.filename)[1]
    unique_filename = f"{uuid.uuid4().hex}{file_extension}"
    file_location = os.path.join(upload_dir, unique_filename)

    try:
        # L∆∞u file v√†o ·ªï ƒëƒ©a
        with open(file_location, "wb") as f:
            content = file.file.read()
            f.write(content)
        
        # G·ªçi module RAG ƒë·ªÉ ƒë√°nh index (H√†m ingest n√†y ƒë√£ ƒë∆∞·ª£c fix ·ªü b∆∞·ªõc 1)
        ingest_document(file_location, course_id)
        
        return {
            "message": "T·∫£i l√™n v√† x·ª≠ l√Ω t√†i li·ªáu th√†nh c√¥ng",
            "original_name": file.filename,
            "saved_as": unique_filename
        }
    except Exception as e:
        # X√≥a file n·∫øu qu√° tr√¨nh x·ª≠ l√Ω th·∫•t b·∫°i ƒë·ªÉ tr√°nh r√°c d·ªØ li·ªáu
        if os.path.exists(file_location):
            os.remove(file_location)
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω file: {str(e)}")
====================
FILE: .\backend\api\eval.py
====================
from fastapi import APIRouter
from backend.eval.runner import run_evaluation

router = APIRouter(prefix="/eval", tags=["Eval"])

@router.post(
    "/run",
    operation_id="eval_run"
)
def run_eval():
    return run_evaluation()

====================
FILE: .\backend\api\pedagogy.py
====================
from fastapi import APIRouter

router = APIRouter(prefix="/pedagogy", tags=["Pedagogy"])

@router.post(
    "/generate",
    operation_id="pedagogy_generate"
)
def generate():
    return {"status": "ok"}

====================
FILE: .\backend\api\upload.py
====================
import os
import uuid
from fastapi import APIRouter, UploadFile, File, HTTPException
from backend.rag.ingest import ingest_document

router = APIRouter()

UPLOAD_DIR = "data/raw_docs"

@router.post("/")
async def upload_document(course_id: str, file: UploadFile = File(...)):
    if not os.path.exists(UPLOAD_DIR):
        os.makedirs(UPLOAD_DIR)

    # FIX: T·∫°o t√™n file duy nh·∫•t tr√°nh ghi ƒë√® [cite: 20]
    file_extension = os.path.splitext(file.filename)[1]
    unique_filename = f"{uuid.uuid4().hex}{file_extension}"
    file_path = os.path.join(UPLOAD_DIR, unique_filename)
    
    try:
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        # Ingest d·ªØ li·ªáu v√†o vector store theo course_id [cite: 24, 84]
        ingest_document(file_path, course_id)

        return {
            "message": f"Upload th√†nh c√¥ng file {file.filename}",
            "saved_as": unique_filename
        }
    except Exception as e:
        if os.path.exists(file_path):
            os.remove(file_path)
        raise HTTPException(status_code=500, detail=str(e))
====================
FILE: .\backend\api\__init__.py
====================

====================
FILE: .\backend\auth\deps.py
====================
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from backend.auth.security import verify_token

# URL n√†y ph·∫£i kh·ªõp v·ªõi prefix trong main.py + route trong auth.py
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="auth/login")

def get_current_user(token: str = Depends(oauth2_scheme)):
    payload = verify_token(token)
    if not payload:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Token kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ h·∫øt h·∫°n",
        )
    return payload
====================
FILE: .\backend\auth\roles.py
====================
from enum import Enum

class UserRole(str, Enum):
    STUDENT = "student"
    TEACHER = "teacher"

====================
FILE: .\backend\auth\security.py
====================
from datetime import datetime, timedelta
from jose import jwt, JWTError

# C·∫•u h√¨nh c·ª©ng ƒë·ªÉ dev local kh√¥ng b·ªã invalid token khi restart
SECRET_KEY = "DEV_SECRET_KEY_LOCAL_123"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 1440 # 24 gi·ªù

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def verify_token(token: str):
    """Gi·∫£i m√£ token ƒë·ªÉ l·∫•y payload"""
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload if payload.get("sub") else None
    except JWTError:
        return None
====================
FILE: .\backend\auth\users.py
====================
from passlib.context import CryptContext

# Kh·ªüi t·∫°o context: passlib t·ª± x·ª≠ l√Ω encoding v√† salt chu·∫©n cho bcrypt
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """So s√°nh m·∫≠t kh·∫©u plaintext v√† m·∫≠t kh·∫©u ƒë√£ hash"""
    try:
        return pwd_context.verify(plain_password, hashed_password)
    except Exception:
        return False

def get_password_hash(password: str) -> str:
    """T·∫°o hash m·ªõi cho m·∫≠t kh·∫©u"""
    return pwd_context.hash(password)

# M·∫¨T KH·∫®U TEST L√Ä: 123456
# Hash n√†y ƒë√£ ƒë∆∞·ª£c t·∫°o v√† ki·ªÉm tra b·∫±ng passlib
CLEAN_HASH = "$2b$12$R9h/lIPzMZ7E22NVjtORV.p6zif6oUaQW.mHnc6vSOfKy8nS.E8S."

USERS_DB = {
    "teacher1": {
        "username": "teacher1",
        "password_hash": CLEAN_HASH,
        "role": "teacher"
    },
    "student1": {
        "username": "student1",
        "password_hash": CLEAN_HASH,
        "role": "student"
    }
}

def get_user(username: str):
    return USERS_DB.get(username.lower())
====================
FILE: .\backend\courses\access.py
====================
def check_course_access(user, course):
    if user["role"] == "teacher":
        return course.owner_id == user["id"]
    if user["role"] == "student":
        # MVP: cho ph√©p t·∫•t c·∫£ SV (sau n√†y g·∫Øn class)
        return True
    return False

====================
FILE: .\backend\courses\models.py
====================
from dataclasses import dataclass

@dataclass
class Course:
    course_id: str
    name: str
    owner_id: str  # gi·∫£ng vi√™n

====================
FILE: .\backend\courses\service.py
====================
from backend.courses.models import Course

# MVP: in-memory, sau n√†y thay DB
COURSES = {}

def create_course(course_id: str, name: str, owner_id: str):
    course = Course(course_id, name, owner_id)
    COURSES[course_id] = course
    return course

def get_course(course_id: str):
    return COURSES.get(course_id)

def list_courses_by_owner(owner_id: str):
    return [c for c in COURSES.values() if c.owner_id == owner_id]

====================
FILE: .\backend\courses\__init__.py
====================

====================
FILE: .\backend\database\db_config.py
====================
import sqlite3
from pathlib import Path

DB_PATH = Path("data/app_database.db")

def get_db_connection():
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row # ƒê·ªÉ truy xu·∫•t theo t√™n c·ªôt
    return conn

def init_db():
    conn = get_db_connection()
    # T·∫°o b·∫£ng kh√≥a h·ªçc v√† b·∫£ng ng∆∞·ªùi d√πng n·∫øu ch∆∞a c√≥
    conn.execute('''
        CREATE TABLE IF NOT EXISTS courses (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            teacher_id TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    conn.close()
====================
FILE: .\backend\eval\datasets.py
====================
from dataclasses import dataclass
from typing import List

@dataclass
class EvalSample:
    id: int
    question: str
    expected_answer: str | None = None

EVAL_DATASET: List[EvalSample] = [
    EvalSample(
        id=1,
        question="FAISS l√† g√¨ trong h·ªá th·ªëng RAG?"
    ),
    EvalSample(
        id=2,
        question="BM25 d√πng ƒë·ªÉ l√†m g√¨?"
    ),
]

====================
FILE: .\backend\eval\evaluator.py
====================
from backend.rag.hybrid_retriever import hybrid_search

MIN_RETRIEVAL_SCORE = 0.15

def evaluate_sample(sample: dict) -> dict:
    """
    sample = {
        id: int,
        question: str
    }
    """

    question = sample.get("question")
    if not question:
        raise ValueError("Missing question in eval sample")

    results = hybrid_search(question)

    grounded = False
    used_chunks = []

    for doc, score in results:
        if score >= MIN_RETRIEVAL_SCORE:
            grounded = True
            used_chunks.append({
                "chunk_id": doc.metadata.get("chunk_id"),
                "source_file": doc.metadata.get("source_file"),
                "score": round(score, 4)
            })

    return {
        "id": sample.get("id"),
        "question": question,
        "grounded": grounded,
        "num_chunks": len(used_chunks),
        "chunks": used_chunks,
        "score": 1.0 if grounded else 0.0
    }

====================
FILE: .\backend\eval\faithfulness.py
====================
import re
from typing import Dict, Any, List


STOPWORDS = {
    "l√†", "v√†", "c·ªßa", "trong", "cho", "v·ªõi", "m·ªôt", "c√°c",
    "ƒë∆∞·ª£c", "khi", "n√†y", "ƒë√≥", "nh∆∞", "ƒë·ªÉ"
}


def normalize(text: str) -> List[str]:
    """
    Normalize text into keyword tokens
    """
    text = text.lower()
    text = re.sub(r"[^a-z0-9√†-·ªπ\s]", " ", text)
    tokens = text.split()
    return [t for t in tokens if t not in STOPWORDS and len(t) > 2]


def extract_claim_sentences(answer: str) -> List[str]:
    """
    Split answer into claim sentences (remove chunk tags)
    """
    clean = re.sub(r"\[CHUNK_\d+\]", "", answer)
    sentences = re.split(r"[.\n]", clean)
    return [s.strip() for s in sentences if len(s.strip()) > 20]


def check_faithfulness(
    answer: str,
    contexts: List[str],
    min_overlap_ratio: float = 0.3
) -> Dict[str, Any]:
    """
    Faithfulness check:
    - Each claim sentence must overlap sufficiently with context keywords
    """

    context_text = " ".join(contexts)
    context_tokens = set(normalize(context_text))

    claims = extract_claim_sentences(answer)

    if not claims:
        return {
            "faithful": False,
            "reason": "NO_CLAIM_SENTENCE",
            "details": "No valid claim sentence found in answer."
        }

    unfaithful_claims = []

    for claim in claims:
        claim_tokens = normalize(claim)

        if not claim_tokens:
            continue

        overlap = set(claim_tokens) & context_tokens
        overlap_ratio = len(overlap) / len(set(claim_tokens))

        if overlap_ratio < min_overlap_ratio:
            unfaithful_claims.append({
                "claim": claim,
                "overlap_ratio": round(overlap_ratio, 2)
            })

    if unfaithful_claims:
        return {
            "faithful": False,
            "reason": "LOW_CONTEXT_OVERLAP",
            "details": unfaithful_claims
        }

    return {
        "faithful": True,
        "reason": "OK",
        "details": "All claims sufficiently supported by context."
    }

====================
FILE: .\backend\eval\groundedness.py
====================
import re
from typing import Dict, Any, List


CHUNK_PATTERN = re.compile(r"\[CHUNK_(\d+)\]")


def extract_chunk_ids(answer: str) -> List[int]:
    """
    Extract chunk indices from answer text.
    Example: [CHUNK_0], [CHUNK_2] ‚Üí [0, 2]
    """
    return [int(x) for x in CHUNK_PATTERN.findall(answer)]


def check_groundedness(
    answer: str,
    sources: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Groundedness check:
    - Answer must contain chunk tags
    - Chunk tags must exist in retrieved sources
    """

    chunk_ids_in_answer = extract_chunk_ids(answer)

    if not chunk_ids_in_answer:
        return {
            "grounded": False,
            "reason": "NO_CHUNK_TAG",
            "details": "Answer does not contain any [CHUNK_x] tags."
        }

    valid_chunk_ids = {
        src.get("chunk_id") for src in sources
    }

    invalid_refs = [
        cid for cid in chunk_ids_in_answer
        if cid not in valid_chunk_ids
    ]

    if invalid_refs:
        return {
            "grounded": False,
            "reason": "INVALID_CHUNK_REFERENCE",
            "details": f"Referenced chunk(s) not in sources: {invalid_refs}"
        }

    return {
        "grounded": True,
        "reason": "OK",
        "details": f"All referenced chunks are valid: {chunk_ids_in_answer}"
    }

====================
FILE: .\backend\eval\metrics.py
====================
def groundedness_score(sources, expected_sources):
    if not sources:
        return 0.0

    matched = 0
    for src in sources:
        source_file = src.get("source_file", "")
        if any(exp in source_file for exp in expected_sources):
            matched += 1

    return matched / len(expected_sources)


def citation_coverage(sources):
    if not sources:
        return 0.0

    cited = [s for s in sources if s.get("page") is not None]
    return len(cited) / len(sources)


def hallucination_flag(sources):
    return len(sources) == 0

====================
FILE: .\backend\eval\runner.py
====================
from backend.eval.datasets import EVAL_DATASET
from backend.eval.evaluator import evaluate_sample
from backend.logging.audit import audit_log

def run_evaluation():
    results = []

    for sample in EVAL_DATASET:
        result = evaluate_sample({
            "id": sample.id,
            "question": sample.question
        })

        results.append(result)

        audit_log(
            user="system_eval",
            action="evaluation_run",
            payload=result
        )

    return results

====================
FILE: .\backend\eval\__init__.py
====================

====================
FILE: .\backend\guardrails\citation.py
====================
def format_citations(source_documents):
    citations = []
    for doc in source_documents:
        meta = doc.metadata
        citations.append({
            "source": meta.get("source", "unknown"),
            "page": meta.get("page", None)
        })
    return citations

====================
FILE: .\backend\guardrails\grounding.py
====================
from typing import List

MIN_DOCS_REQUIRED = 1

def check_grounding(source_documents: List):
    if not source_documents or len(source_documents) < MIN_DOCS_REQUIRED:
        raise ValueError(
            "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan. "
            "H·ªá th·ªëng kh√¥ng th·ªÉ tr·∫£ l·ªùi ƒë·ªÉ tr√°nh sai l·ªách h·ªçc thu·∫≠t."
        )

====================
FILE: .\backend\guardrails\rate_limit.py
====================
import time

REQUEST_LIMIT = 20
WINDOW_SECONDS = 60

_user_requests = {}

def check_rate_limit(user_id: str):
    now = time.time()
    timestamps = _user_requests.get(user_id, [])

    timestamps = [t for t in timestamps if now - t < WINDOW_SECONDS]

    if len(timestamps) >= REQUEST_LIMIT:
        raise ValueError("B·∫°n g·ª≠i qu√° nhi·ªÅu y√™u c·∫ßu. Vui l√≤ng th·ª≠ l·∫°i sau.")

    timestamps.append(now)
    _user_requests[user_id] = timestamps

====================
FILE: .\backend\llm\llm.py
====================
import re

class LocalMockLLM:
    """
    Mock LLM Engine: X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi d·ª±a tr√™n Retrieval Context.
    ƒê·∫£m b·∫£o kh√¥ng ph·ª• thu·ªôc v√†o th∆∞ vi·ªán b√™n ngo√†i (g4f, openai, etc.)
    """
    def __init__(self, model_name: str = "offline-pedagogy-v1"):
        self.model_name = model_name

    def invoke(self, prompt: str) -> str:
        # 1. Tr√≠ch xu·∫•t ng·ªØ c·∫£nh t·ª´ prompt (D·ª±a tr√™n c·∫•u tr√∫c trong qa.py)
        # T√¨m n·ªôi dung n·∫±m gi·ªØa 'Ng·ªØ c·∫£nh t√†i li·ªáu:' v√† 'C√¢u h·ªèi ng∆∞·ªùi d√πng:'
        context_match = re.search(r"Ng·ªØ c·∫£nh t√†i li·ªáu:(.*?)C√¢u h·ªèi ng∆∞·ªùi d√πng:", prompt, re.DOTALL)
        
        if not context_match:
            return "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong t√†i li·ªáu c·ªßa kh√≥a h·ªçc n√†y."

        context = context_match.group(1).strip()
        
        # 2. Logic Rule-based: Tr·∫£ v·ªÅ ƒëo·∫°n vƒÉn b·∫£n ph√π h·ª£p nh·∫•t t·ª´ context
        # ·ªû ƒë√¢y ta ∆∞u ti√™n tr·∫£ v·ªÅ to√†n b·ªô ng·ªØ c·∫£nh ƒë√£ ƒë∆∞·ª£c t√¨m th·∫•y b·ªüi RAG
        if len(context) > 5:
            return f"D·ª±a tr√™n t√†i li·ªáu kh√≥a h·ªçc, t√¥i xin gi·∫£i ƒë√°p nh∆∞ sau:\n\n{context}"
        
        return "T√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p kh√¥ng ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

# Kh·ªüi t·∫°o instance duy nh·∫•t cho to√†n h·ªá th·ªëng
llm_instance = LocalMockLLM()

def get_llm():
    return llm_instance
====================
FILE: .\backend\llm\__init__.py
====================

====================
FILE: .\backend\logging\audit.py
====================
import json
import uuid
from datetime import datetime
from pathlib import Path
from typing import List

LOG_PATH = Path("data/audit.log")

def audit_log(
    user: str,
    action: str,
    payload: dict,
    course_id: str | None = None,
    chunk_ids: List[str] | None = None,
    model_name: str | None = None,
    request_id: str | None = None
):
    LOG_PATH.parent.mkdir(parents=True, exist_ok=True)

    record = {
        "timestamp": datetime.utcnow().isoformat(),
        "request_id": request_id or str(uuid.uuid4()),
        "user": user,
        "action": action,
        "course_id": course_id,
        "chunk_ids": chunk_ids or [],
        "model": model_name or "offline_stub",
        "payload": payload,
        "schema": "academic_audit_v3"
    }

    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

====================
FILE: .\backend\pedagogy\bloom.py
====================
from enum import Enum
from typing import List


class BloomLevel(str, Enum):
    """
    Bloom's Taxonomy ‚Äì Cognitive Domain
    """

    REMEMBER = "remember"
    UNDERSTAND = "understand"
    APPLY = "apply"
    ANALYZE = "analyze"
    EVALUATE = "evaluate"
    CREATE = "create"

    @property
    def description(self) -> str:
        return {
            self.REMEMBER: "Nh·ªõ l·∫°i th√¥ng tin, ƒë·ªãnh nghƒ©a, thu·∫≠t ng·ªØ",
            self.UNDERSTAND: "Gi·∫£i th√≠ch, m√¥ t·∫£, di·ªÖn gi·∫£i √Ω nghƒ©a",
            self.APPLY: "√Åp d·ª•ng ki·∫øn th·ª©c v√†o b√†i to√°n c·ª• th·ªÉ",
            self.ANALYZE: "Ph√¢n t√≠ch, so s√°nh, ch·ªâ ra m·ªëi quan h·ªá",
            self.EVALUATE: "ƒê√°nh gi√°, nh·∫≠n x√©t, ph·∫£n bi·ªán",
            self.CREATE: "T·ªïng h·ª£p, thi·∫øt k·∫ø, x√¢y d·ª±ng m·ªõi"
        }[self]

    @property
    def action_verbs(self) -> List[str]:
        """
        G·ª£i √Ω ƒë·ªông t·ª´ d√πng trong c√¢u h·ªèi / b√†i t·∫≠p
        """
        return {
            self.REMEMBER: ["li·ªát k√™", "n√™u", "ƒë·ªãnh nghƒ©a", "k·ªÉ t√™n"],
            self.UNDERSTAND: ["gi·∫£i th√≠ch", "tr√¨nh b√†y", "m√¥ t·∫£", "t√≥m t·∫Øt"],
            self.APPLY: ["√°p d·ª•ng", "t√≠nh to√°n", "gi·∫£i", "th·ª±c hi·ªán"],
            self.ANALYZE: ["ph√¢n t√≠ch", "so s√°nh", "ph√¢n lo·∫°i", "l√†m r√µ"],
            self.EVALUATE: ["ƒë√°nh gi√°", "nh·∫≠n x√©t", "ph·∫£n bi·ªán", "so s√°nh ∆∞u nh∆∞·ª£c ƒëi·ªÉm"],
            self.CREATE: ["thi·∫øt k·∫ø", "x√¢y d·ª±ng", "ƒë·ªÅ xu·∫•t", "ph√°t tri·ªÉn"]
        }[self]

    @classmethod
    def ordered_levels(cls) -> List["BloomLevel"]:
        """
        Tr·∫£ v·ªÅ Bloom levels theo th·ª© t·ª± tƒÉng d·∫ßn ƒë·ªô kh√≥
        """
        return [
            cls.REMEMBER,
            cls.UNDERSTAND,
            cls.APPLY,
            cls.ANALYZE,
            cls.EVALUATE,
            cls.CREATE
        ]

====================
FILE: .\backend\pedagogy\generator.py
====================
import json
import re
from backend.llm.llm import llm_instance

def clean_json_response(raw_text: str) -> str:
    """
    Lo·∫°i b·ªè c√°c k√Ω t·ª± Markdown th·ª´a [cite: 65]
    """
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p LLM tr·∫£ v·ªÅ ```json ... ``` ho·∫∑c vƒÉn b·∫£n bao quanh JSON
    json_match = re.search(r"(\{.*\}|\[.*\])", raw_text, re.DOTALL)
    if json_match:
        return json_match.group(1).strip()
    return raw_text.strip()

def generate_lesson_plan(topic: str, context: str = ""):
    prompt = f"""
    H√£y ƒë√≥ng vai l√† m·ªôt chuy√™n gia s∆∞ ph·∫°m.
    Ch·ªß ƒë·ªÅ: {topic}
    Ng·ªØ c·∫£nh t√†i li·ªáu: {context}
    
    H√£y t·∫°o m·ªôt gi√°o √°n chi ti·∫øt d∆∞·ªõi ƒë·ªãnh d·∫°ng JSON v·ªõi c√°c tr∆∞·ªùng:
    - title, objectives, activities
    Ch·ªâ tr·∫£ v·ªÅ JSON. [cite: 66, 67, 68]
    """
    
    raw_response = llm_instance.invoke(prompt)
    cleaned_json = clean_json_response(raw_response)
    
    try:
        return json.loads(cleaned_json)
    except json.JSONDecodeError:
        # Fallback n·∫øu JSON v·∫´n l·ªói [cite: 69]
        return {
            "title": topic,
            "objectives": ["Kh√¥ng th·ªÉ t·∫°o m·ª•c ti√™u t·ª± ƒë·ªông"],
            "activities": [],
            "error": "AI tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá"
        }
====================
FILE: .\backend\pedagogy\prompts.py
====================
from backend.pedagogy.bloom import BloomLevel


QUESTION_PROMPT_TEMPLATE = """
B·∫°n l√† AI Tr·ª£ Gi·∫£ng.

NHI·ªÜM V·ª§:
Sinh {num_items} C√ÇU H·ªéI h·ªçc t·∫≠p theo m·ª©c ƒë·ªô Bloom: {bloom_level}

M√î T·∫¢ M·ª®C ƒê·ªò:
{bloom_description}

Y√äU C·∫¶U B·∫ÆT BU·ªòC:
- CH·ªà s·ª≠ d·ª•ng th√¥ng tin trong T√ÄI LI·ªÜU b√™n d∆∞·ªõi
- Kh√¥ng suy ƒëo√°n, kh√¥ng th√™m ki·∫øn th·ª©c b√™n ngo√†i
- M·ªói c√¢u h·ªèi ph·∫£i b√°m s√°t n·ªôi dung t√†i li·ªáu
- M·ª©c ƒë·ªô nh·∫≠n th·ª©c ph·∫£i ƒë√∫ng Bloom level
- Kh√¥ng tr√πng l·∫∑p c√¢u h·ªèi

T√ÄI LI·ªÜU:
{context}

ƒê·ªäNH D·∫†NG TR·∫¢ V·ªÄ (JSON):
[
  {{
    "question": "...",
    "bloom_level": "{bloom_level}",
    "learning_objective": "...",
    "difficulty": "easy | medium | hard"
  }}
]
"""


ASSIGNMENT_PROMPT_TEMPLATE = """
B·∫°n l√† AI Tr·ª£ Gi·∫£ng.

NHI·ªÜM V·ª§:
Sinh {num_items} B√ÄI T·∫¨P h·ªçc t·∫≠p theo m·ª©c ƒë·ªô Bloom: {bloom_level}

M√î T·∫¢ M·ª®C ƒê·ªò:
{bloom_description}

Y√äU C·∫¶U B·∫ÆT BU·ªòC:
- CH·ªà s·ª≠ d·ª•ng th√¥ng tin trong T√ÄI LI·ªÜU b√™n d∆∞·ªõi
- Kh√¥ng suy ƒëo√°n, kh√¥ng th√™m ki·∫øn th·ª©c b√™n ngo√†i
- B√†i t·∫≠p ph·∫£i y√™u c·∫ßu ng∆∞·ªùi h·ªçc v·∫≠n d·ª•ng ƒë√∫ng m·ª©c Bloom
- C√≥ m√¥ t·∫£ r√µ y√™u c·∫ßu ƒë·∫ßu ra

T√ÄI LI·ªÜU:
{context}

ƒê·ªäNH D·∫†NG TR·∫¢ V·ªÄ (JSON):
[
  {{
    "title": "...",
    "task": "...",
    "expected_output": "...",
    "bloom_level": "{bloom_level}",
    "difficulty": "easy | medium | hard"
  }}
]
"""


def build_question_prompt(
    context: str,
    bloom: BloomLevel,
    num_items: int = 3
) -> str:
    return QUESTION_PROMPT_TEMPLATE.format(
        num_items=num_items,
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )


def build_assignment_prompt(
    context: str,
    bloom: BloomLevel,
    num_items: int = 2
) -> str:
    return ASSIGNMENT_PROMPT_TEMPLATE.format(
        num_items=num_items,
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )

====================
FILE: .\backend\pedagogy\rubric.py
====================
import json
from typing import List, Dict, Any

from backend.pedagogy.bloom import BloomLevel
from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import get_llm


CONTEXT_TOP_K = 5


def retrieve_context(topic: str) -> str:
    results = hybrid_search(topic)
    contexts = []

    for doc, _ in results[:CONTEXT_TOP_K]:
        contexts.append(doc.page_content)

    return "\n\n".join(contexts)


RUBRIC_PROMPT_TEMPLATE = """
B·∫°n l√† GI·∫¢NG VI√äN.

NHI·ªÜM V·ª§:
X√¢y d·ª±ng RUBRIC CH·∫§M ƒêI·ªÇM cho b√†i h·ªçc v·ªõi m·ª©c ƒë·ªô Bloom: {bloom_level}

M√î T·∫¢ M·ª®C ƒê·ªò:
{bloom_description}

Y√äU C·∫¶U:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin trong T√ÄI LI·ªÜU
- Rubric ph·∫£i ƒë√°nh gi√° ƒë√∫ng m·ª©c Bloom
- C√≥ nhi·ªÅu ti√™u ch√≠ ch·∫•m ƒëi·ªÉm
- C√≥ m√¥ t·∫£ r√µ cho t·ª´ng m·ª©c ƒëi·ªÉm

T√ÄI LI·ªÜU:
{context}

ƒê·ªäNH D·∫†NG TR·∫¢ V·ªÄ (JSON):
[
  {{
    "criterion": "...",
    "levels": {{
      "excellent": "...",
      "good": "...",
      "average": "...",
      "poor": "..."
    }}
  }}
]
"""


def generate_rubric(
    topic: str,
    bloom: BloomLevel
) -> List[Dict[str, Any]]:
    """
    Generate grading rubric based on Bloom level
    """

    context = retrieve_context(topic)

    if not context.strip():
        raise ValueError("No relevant learning material found.")

    prompt = RUBRIC_PROMPT_TEMPLATE.format(
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )

    llm = get_llm()
    raw = llm.invoke(prompt)

    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        raise ValueError("LLM output is not valid JSON.")

====================
FILE: .\backend\pedagogy\schemas.py
====================
from pydantic import BaseModel
from typing import List, Optional

class TaskRequest(BaseModel):
    task_type: str   # lesson_plan | assignment | exam | rubric | quiz
    subject: str
    clo: Optional[List[str]] = None
    bloom_level: Optional[str] = None
    difficulty: Optional[str] = None
    num_items: Optional[int] = None
    duration_minutes: Optional[int] = None

====================
FILE: .\backend\pedagogy\templates.py
====================
LESSON_PLAN_TEMPLATE = """
B·∫°n l√† tr·ª£ gi·∫£ng ƒë·∫°i h·ªçc.
D·ª±a CH·ªà tr√™n t√†i li·ªáu sau:
{context}

H√£y so·∫°n gi√°o √°n cho m√¥n: {subject}
CLO: {clo}
Th·ªùi l∆∞·ª£ng: {duration} ph√∫t

Y√™u c·∫ßu:
- Kh√¥ng th√™m ki·∫øn th·ª©c ngo√†i t√†i li·ªáu
- C·∫•u tr√∫c r√µ r√†ng
- C√≥ ho·∫°t ƒë·ªông GV/SV
"""

====================
FILE: .\backend\pedagogy\validators.py
====================
def validate_groundedness(answer, sources):
    if not sources:
        raise ValueError(
            "N·ªôi dung sinh ra kh√¥ng c√≥ cƒÉn c·ª© t·ª´ t√†i li·ªáu."
        )

====================
FILE: .\backend\pedagogy\__init__.py
====================

====================
FILE: .\backend\rag\hybrid_retriever.py
====================
from backend.vectorstore.faiss_store import get_faiss_store
from backend.vectorstore.bm25_store import BM25Store
import os

def hybrid_search(query: str, course_id: str, k: int = 5):
    if not query or len(query.strip()) < 2:
        return []

    # FIX: ƒê∆∞·ªùng d·∫´n ph·∫£i kh·ªõp v·ªõi logic trong ingest.py
    index_path = os.path.join("data", "faiss_index", course_id)
    
    vector_store = get_faiss_store(index_path)
    bm25_store = BM25Store.load(course_id)

    vector_results = []
    if vector_store:
        # Tr·∫£ v·ªÅ Document v√† score
        vector_results = vector_store.similarity_search_with_score(query, k=k)

    bm25_results = []
    if bm25_store:
        bm25_results = bm25_store.search(query, k=k)

    combined = []
    # Chu·∫©n h√≥a format k·∫øt qu·∫£ tr·∫£ v·ªÅ cho Frontend v√† Agent
    for doc, score in vector_results:
        combined.append({
            "content": doc.page_content, 
            "source_file": doc.metadata.get("source_file", "Unknown"),
            "page": doc.metadata.get("page"),
            "section": doc.metadata.get("section"),
            "chunk_id": doc.metadata.get("chunk_id"),
            "score": float(score),
            "type": "vector"
        })
    
    for doc, score in bm25_results:
        combined.append({
            "content": doc.page_content, 
            "source_file": doc.metadata.get("source_file", "Unknown"),
            "page": doc.metadata.get("page"),
            "section": doc.metadata.get("section"),
            "chunk_id": doc.metadata.get("chunk_id"),
            "score": float(score),
            "type": "bm25"
        })

    # S·∫Øp x·∫øp theo score v√† l·∫•y k k·∫øt qu·∫£ ƒë·∫ßu ti√™n
    return combined[:k]
====================
FILE: .\backend\rag\ingest.py
====================
import os
from backend.vectorstore.faiss_store import embeddings_instance
from langchain_community.vectorstores import FAISS
from backend.utils.text_extraction import extract_text
from backend.utils.chunking import chunk_text
from backend.vectorstore.bm25_store import BM25Store

def ingest_document(file_path: str, course_id: str):
    # 1. Tr√≠ch xu·∫•t vƒÉn b·∫£n
    text = extract_text(file_path)
    file_name = os.path.basename(file_path)

    # 2. Chia nh·ªè vƒÉn b·∫£n (S·ª≠ d·ª•ng utils ƒë·ªÉ c√≥ metadata chu·∫©n cho Eval) [cite: 78, 79]
    docs = chunk_text(text, source_file=file_name)

    # 3. X·ª≠ l√Ω Vector Store (FAISS)
    index_path = os.path.join("data", "faiss_index", course_id)
    
    if os.path.exists(os.path.join(index_path, "index.faiss")):
        vector_store = FAISS.load_local(
            index_path, 
            embeddings_instance, 
            allow_dangerous_deserialization=True
        )
        vector_store.add_documents(docs)
    else:
        vector_store = FAISS.from_documents(docs, embeddings_instance)

    if not os.path.exists(os.path.dirname(index_path)):
        os.makedirs(os.path.dirname(index_path))
    
    vector_store.save_local(index_path)

    # 4. X·ª≠ l√Ω Keyword Store (BM25) - Quan tr·ªçng ƒë·ªÉ hybrid_search kh√¥ng l·ªói [cite: 85, 87]
    bm25_store = BM25Store.load(course_id)
    bm25_store.add_documents(docs)
    bm25_store.save()

    print(f"Th√†nh c√¥ng: ƒê√£ c·∫≠p nh·∫≠t FAISS v√† BM25 cho m√¥n {course_id}")
====================
FILE: .\backend\rag\__init__.py
====================

====================
FILE: .\backend\utils\chunking.py
====================
# backend/utils/chunking.py

import uuid
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document



def chunk_text(
    text: str,
    source_file: str,
    page: int = None,
    section: str = None,
    chunk_size: int = 400,
    chunk_overlap: int = 80
) -> List[Document]:
    """
    Chunk text into Documents with full metadata
    """

    doc_id = str(uuid.uuid4())

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    chunks = splitter.split_text(text)

    documents = []

    for idx, chunk in enumerate(chunks):
        metadata = {
            "doc_id": doc_id,
            "source_file": source_file,
            "page": page,
            "section": section,
            "chunk_id": idx
        }

        documents.append(
            Document(
                page_content=chunk,
                metadata=metadata
            )
        )

    return documents

====================
FILE: .\backend\utils\citation.py
====================
def format_apa(source: dict) -> str:
    """
    APA 7th basic format
    """
    author = "Unknown"
    year = "n.d."
    title = source.get("source_file", "Unknown document")
    page = source.get("page")

    page_part = f" (p. {page})" if page is not None else ""

    return f"{author} ({year}). {title}{page_part}."


def format_ieee(source: dict, index: int) -> str:
    """
    IEEE basic format
    """
    title = source.get("source_file", "Unknown document")
    page = source.get("page")

    page_part = f", p. {page}" if page is not None else ""

    return f"[{index}] {title}{page_part}."

====================
FILE: .\backend\utils\text_extraction.py
====================
from pathlib import Path
from pypdf import PdfReader
from docx import Document as DocxDocument


def extract_text(file_path: str) -> str:
    ext = Path(file_path).suffix.lower()

    if ext == ".pdf":
        reader = PdfReader(file_path)
        return "\n".join(
            page.extract_text() or ""
            for page in reader.pages
        )

    if ext in [".docx"]:
        doc = DocxDocument(file_path)
        return "\n".join(p.text for p in doc.paragraphs)

    if ext in [".txt"]:
        with open(file_path, encoding="utf-8", errors="ignore") as f:
            return f.read()

    raise ValueError(f"Unsupported file type: {ext}")

====================
FILE: .\backend\vectorstore\bm25_store.py
====================
import pickle
from pathlib import Path
from typing import List, Tuple
from rank_bm25 import BM25Okapi
from langchain_core.documents import Document

class BM25Store:
    def __init__(self, course_id: str):
        self.course_id = course_id
        self.documents: List[Document] = []
        self.corpus = []
        self.bm25 = None
        self.storage_path = Path(f"data/bm25_{course_id}.pkl") # T√°ch bi·ªát theo course [cite: 82]

    def add_documents(self, docs: List[Document]):
        for doc in docs:
            tokens = doc.page_content.lower().split()
            self.documents.append(doc)
            self.corpus.append(tokens)
        self.bm25 = BM25Okapi(self.corpus)

    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        if not self.bm25:
            return []
        query_tokens = query.lower().split()
        scores = self.bm25.get_scores(query_tokens)
        ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:k]
        return [(self.documents[idx], score) for idx, score in ranked if score > 0]

    def save(self):
        self.storage_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.storage_path, "wb") as f:
            pickle.dump(self, f)

    @classmethod
    def load(cls, course_id: str):
        """
        FIX: Tr·∫£ v·ªÅ m·ªôt instance ƒë√£ load d·ªØ li·ªáu thay v√¨ instance tr·ªëng 
        """
        instance = cls(course_id)
        if instance.storage_path.exists():
            with open(instance.storage_path, "rb") as f:
                loaded_data = pickle.load(f)
                instance.documents = loaded_data.documents
                instance.corpus = loaded_data.corpus
                instance.bm25 = loaded_data.bm25
        return instance
====================
FILE: .\backend\vectorstore\faiss_store.py
====================
import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# S·ª≠ d·ª•ng model nh·ªè ƒë·ªÉ ti·∫øt ki·ªám RAM
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# Kh·ªüi t·∫°o singleton ƒë·ªÉ d√πng chung to√†n h·ªá th·ªëng, tr√°nh t·ªën RAM [cite: 99]
embeddings_instance = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

def get_faiss_store(index_path: str):
    """
    Load FAISS store t·ª´ ƒëƒ©a n·∫øu t·ªìn t·∫°i, n·∫øu kh√¥ng tr·∫£ v·ªÅ None [cite: 99]
    """
    if os.path.exists(os.path.join(index_path, "index.faiss")):
        return FAISS.load_local(
            index_path, 
            embeddings_instance, 
            allow_dangerous_deserialization=True
        )
    return None
====================
FILE: .\frontend\index.html
====================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>AI Tr·ª£ Gi·∫£ng</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>

====================
FILE: .\frontend\vite.config.js
====================
import { defineConfig } from "vite";
import react from "@vitejs/plugin-react";

export default defineConfig({
  plugins: [react()],
  server: {
    port: 5173
  }
});

====================
FILE: .\frontend\src\api.js
====================
import axios from "axios";

export const api = axios.create({
  baseURL: "http://127.0.0.1:8000"
});

export function setToken(token) {
  api.defaults.headers.common["Authorization"] = `Bearer ${token}`;
}

====================
FILE: .\frontend\src\App.jsx
====================
import { useState } from "react";
import { api, setToken } from "./api";
import ChatBox from "./components/ChatBox";

export default function App() {
  const [username, setUsername] = useState("");
  const [password, setPassword] = useState("");
  const [role, setRole] = useState(null);
  const [file, setFile] = useState(null);

  async function login() {
    const res = await api.post("/auth/login", {
      username,
      password,
    });

    setToken(res.data.access_token);
    setRole(res.data.role);

    alert("Login th√†nh c√¥ng v·ªõi role: " + res.data.role);
  }

  async function upload() {
    const form = new FormData();
    form.append("file", file);

    await api.post("/upload/", form);
    alert("Upload & ingest xong");
  }

  return (
    <div style={{ padding: 30, fontFamily: "Arial" }}>
      <h2>AI Tr·ª£ Gi·∫£ng</h2>

      {/* LOGIN */}
      <h3>Login</h3>
      <input
        placeholder="username"
        onChange={(e) => setUsername(e.target.value)}
      />
      <br />
      <input
        type="password"
        placeholder="password"
        onChange={(e) => setPassword(e.target.value)}
      />
      <br />
      <button onClick={login}>Login</button>

      {/* UPLOAD ‚Äì TEACHER ONLY */}
      {role === "teacher" && (
        <>
          <h3>Upload t√†i li·ªáu (GV)</h3>
          <input
            type="file"
            onChange={(e) => setFile(e.target.files[0])}
          />
          <br />
          <button onClick={upload}>Upload</button>
        </>
      )}

      <hr />

      {/* CHAT ‚Äì D√ôNG CHATBOX PHASE 3 */}
      <ChatBox />
    </div>
  );
}

====================
FILE: .\frontend\src\main.jsx
====================
import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App";

ReactDOM.createRoot(document.getElementById("root")).render(<App />);

====================
FILE: .\frontend\src\components\ChatBox.jsx
====================
import { useState } from "react";

export default function ChatBox() {
  const [question, setQuestion] = useState("");
  const [answer, setAnswer] = useState("");
  const [sources, setSources] = useState([]);
  const [loading, setLoading] = useState(false);

  const ask = async () => {
    if (!question.trim()) return;
    setLoading(true);
    try {
      // FIX: URL ph·∫£i kh·ªõp v·ªõi router backend (main.py ƒë·ªãnh nghƒ©a prefix="/chat") [cite: 5]
      const res = await fetch("http://localhost:8000/chat/", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ 
          question,
          course_id: "ML101" // MVP: Fix c·ª©ng ho·∫∑c l·∫•y t·ª´ state kh√≥a h·ªçc
        }),
      });
      const data = await res.json();
      setAnswer(data.answer);
      setSources(data.sources || []);
    } catch (err) {
      console.error("L·ªói:", err);
      setAnswer("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn server.");
    } finally {
      setLoading(false);
    }
  };

  return (
    <div style={{ maxWidth: 800, margin: "40px auto", padding: "20px", border: "1px solid #ddd" }}>
      <h3>H·ªèi ƒë√°p t√†i li·ªáu</h3>
      <textarea
        value={question}
        onChange={(e) => setQuestion(e.target.value)}
        rows={3}
        style={{ width: "100%", padding: "10px" }}
        placeholder="Nh·∫≠p c√¢u h·ªèi v·ªÅ b√†i h·ªçc..."
      />
      <button onClick={ask} disabled={loading} style={{ marginTop: 10, padding: "10px 20px" }}>
        {loading ? "ƒêang suy nghƒ©..." : "G·ª≠i c√¢u h·ªèi"}
      </button>

      {answer && (
        <div style={{ marginTop: 20, backgroundColor: "#f9f9f9", padding: "15px" }}>
          <strong>Tr·ª£ gi·∫£ng AI:</strong>
          <p style={{ whiteSpace: "pre-wrap" }}>{answer}</p>
          
          {sources.length > 0 && (
            <div style={{ marginTop: 15, borderTop: "1px solid #eee", paddingTop: "10px" }}>
              <small>Ngu·ªìn tham kh·∫£o:</small>
              <ul style={{ fontSize: "0.85em" }}>
                {sources.map((s, idx) => (
                  <li key={idx}>
                    {s.source_file} {s.page ? `- Trang ${s.page}` : ""} 
                    <details>
                      <summary>Xem tr√≠ch ƒëo·∫°n</summary>
                      <i>"...{s.content.substring(0, 200)}..."</i>
                    </details>
                  </li>
                ))}
              </ul>
            </div>
          )}
        </div>
      )}
    </div>
  );
}
====================
FILE: .\frontend\src\components\FileUpload.jsx
====================
import React, { useState } from "react";
import axios from "axios";

function FileUpload() {
  const [selectedFile, setSelectedFile] = useState(null);
  const [courseId, setCourseId] = useState("ML101");
  const [loading, setLoading] = useState(false);

  const handleFileChange = (e) => {
    setSelectedFile(e.target.files[0]);
  };

  const handleUpload = async () => {
    if (!selectedFile) {
      alert("Vui l√≤ng ch·ªçn m·ªôt file!");
      return;
    }

    setLoading(true);
    const formData = new FormData();
    // Quan tr·ªçng: T√™n 'file' ph·∫£i kh·ªõp v·ªõi backend
    formData.append("file", selectedFile);

    try {
      const response = await axios.post(
        `http://localhost:8000/upload/?course_id=${courseId}`,
        formData,
        {
          headers: { "Content-Type": "multipart/form-data" },
        }
      );
      alert("Upload th√†nh c√¥ng: " + response.data.message);
    } catch (error) {
      console.error("L·ªói upload:", error);
      alert("L·ªói upload: " + (error.response?.data?.detail || "Kh√¥ng r√µ l·ªói"));
    } finally {
      setLoading(false);
    }
  };

  return (
    <div style={{ padding: "20px", border: "1px solid #ccc", marginBottom: "20px" }}>
      <h3>Upload T√†i Li·ªáu (GV)</h3>
      <input
        type="text"
        placeholder="Course ID (vd: ML101)"
        value={courseId}
        onChange={(e) => setCourseId(e.target.value)}
      />
      <br /><br />
      <input type="file" onChange={handleFileChange} accept=".txt" />
      <button onClick={handleUpload} disabled={loading}>
        {loading ? "ƒêang x·ª≠ l√Ω..." : "T·∫£i l√™n & Ingest"}
      </button>
    </div>
  );
}

export default FileUpload;
====================
FILE: .\tests\conftest.py
====================
import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

====================
FILE: .\tests\test_eval.py
====================
from backend.eval.runner import run_evaluation

def test_eval_pipeline_runs():
    results = run_evaluation()
    assert isinstance(results, list)
    assert len(results) > 0
    assert "score" in results[0]

====================
FILE: .\tests\test_eval_pipeline.py
====================
from backend.eval.runner import run_evaluation

def test_eval_runs():
    results = run_evaluation()
    assert isinstance(results, list)
    assert "grounded" in results[0]

====================
FILE: .\tests\test_guardrails.py
====================
from backend.rag.hybrid_retriever import hybrid_search
from backend.agent.qa import answer_question

def test_empty_query():
    assert hybrid_search("") == []

def test_short_query():
    assert hybrid_search("hi") == []

def test_out_of_scope_question():
    result = answer_question("Th·ªß ƒë√¥ c·ªßa Ph√°p l√† g√¨?")
    assert "kh√¥ng t√¨m th·∫•y" in result["answer"].lower() \
        or "v∆∞·ª£t ngo√†i ph·∫°m vi" in result["answer"].lower()

====================
FILE: .\tests\test_rag_guardrail.py
====================
from backend.agent.qa import answer_question

def test_out_of_scope():
    r = answer_question("Ai l√† t·ªïng th·ªëng M·ªπ?")
    assert "kh√¥ng t√¨m th·∫•y" in r["answer"].lower() \
        or "v∆∞·ª£t ngo√†i ph·∫°m vi" in r["answer"].lower()

==============================
FILE: .\data\raw_docs\83dee78d344649c393659de375ce52e0.txt
==============================
H·ªçc m√°y l√† m·ªôt lƒ©nh v·ª±c c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o.

==============================
FILE: .\data\raw_docs\project_export.txt
==============================

==============================
FILE: D:\ai-tro-giang\.pytest_cache\README.md
==============================

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.

==============================
FILE: D:\ai-tro-giang\backend\__init__.py
==============================


==============================
FILE: D:\ai-tro-giang\backend\agent\__init__.py
==============================


==============================
FILE: D:\ai-tro-giang\backend\agent\prompt.py
==============================

SYSTEM_PROMPT = """
B?n l AI tr? gi?ng h?c thu?t.

Quy t?c b?t bu?c:
- Ch? tr? l?i d?a trn n?i dung ti li?u du?c cung c?p.
- N?u khng tm th?y thng tin trong ti li?u, hy tr? l?i: 
  "Ti khng tm th?y thng tin trong ti li?u."
- Khng du?c suy don ho?c b?a.
- Tr? l?i ng?n g?n, dng tr?ng tm.
"""

==============================
FILE: D:\ai-tro-giang\backend\agent\qa.py
==============================

from typing import Dict, Any, List, Tuple
from langchain_core.documents import Document

from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import get_llm
from backend.utils.citation import format_apa, format_ieee


TOP_K = 5
HYBRID_THRESHOLD = 0.15

WATERMARK = (
    "\n\n AI Tr? Gi?ng\n"
    "N?i dung ny du?c t?o t? d?ng d?a trn ti li?u h?c t?p n?i b?. "
    "Khng thay th? ti li?u chnh th?c."
)


def answer_question(question: str) -> Dict[str, Any]:
    """
    Answer question using Hybrid Search with strict academic guardrails.

    Guardrails:
    - No retrieval ? refuse
    - Low-score retrieval ? refuse
    - LLM is NEVER called without grounded context
    """

    # === BASIC INPUT CHECK ===
    if not question or len(question.strip()) < 5:
        return {
            "answer": "Cu h?i khng h?p l? ho?c qu ng?n.",
            "sources": [],
            "citations": {"apa": [], "ieee": []}
        }

    # === RETRIEVAL ===
    results: List[Tuple[Document, float]] = hybrid_search(question)

    # === GUARD 1: NO DOCUMENT FOUND ===
    if not results:
        return {
            "answer": "Ti khng tm th?y thng tin trong ti li?u d du?c cung c?p.",
            "sources": [],
            "citations": {"apa": [], "ieee": []}
        }

    # === APPLY THRESHOLD ===
    filtered: List[Tuple[Document, float]] = [
        (doc, score)
        for doc, score in results[:TOP_K]
        if score >= HYBRID_THRESHOLD
    ]

    # === GUARD 2: OUT-OF-SCOPE QUESTION ===
    if not filtered:
        return {
            "answer": (
                "Cu h?i ny vu?t ngoi ph?m vi cc ti li?u hi?n c. "
                "Ti khng d? can c? h?c thu?t d? tr? l?i."
            ),
            "sources": [],
            "citations": {"apa": [], "ieee": []}
        }

    # === BUILD CONTEXT & SOURCES ===
    sources = []
    context_parts = []

    for idx, (doc, score) in enumerate(filtered):
        sources.append({
            "source_file": doc.metadata.get("source_file"),
            "page": doc.metadata.get("page"),
            "section": doc.metadata.get("section"),
            "chunk_id": doc.metadata.get("chunk_id"),
            "score": round(score, 4),
            "preview": doc.page_content[:200]
        })

        context_parts.append(
            f"[CHUNK_{doc.metadata.get('chunk_id')}]\n{doc.page_content}"
        )

    context = "\n\n".join(context_parts)

    # === PROMPT (STRICT GROUNDED) ===
    prompt = f"""
B?n l AI Tr? Gi?ng h?c thu?t.

CH? s? d?ng thng tin trong cc CHUNK bn du?i.
M?i  tr? l?i PH?I k?t thc b?ng tag [CHUNK_x] tuong ?ng.
N?u khng d? thng tin ? t? ch?i tr? l?i.

TI LI?U:
{context}

CU H?I:
{question}

TR? L?I:
"""

    # === CALL LLM (SAFE POINT) ===
    llm = get_llm()
    answer = llm.invoke(prompt).strip() + WATERMARK

    citations = {
        "apa": [format_apa(s) for s in sources],
        "ieee": [format_ieee(s, i + 1) for i, s in enumerate(sources)]
    }

    return {
        "answer": answer,
        "sources": sources,
        "citations": citations
    }

==============================
FILE: D:\ai-tro-giang\backend\api\__init__.py
==============================


==============================
FILE: D:\ai-tro-giang\backend\api\auth.py
==============================

from fastapi import APIRouter

router = APIRouter(prefix="/auth", tags=["Auth"])

@router.post(
    "/login",
    operation_id="auth_login"
)
def login(username: str, password: str):
    return {"status": "ok"}

==============================
FILE: D:\ai-tro-giang\backend\api\chat.py
==============================

from fastapi import APIRouter
from backend.agent.qa import answer_question

router = APIRouter(prefix="/chat", tags=["Chat"])

@router.post(
    "/",
    operation_id="chat_answer_question"
)
def chat(question: str):
    return answer_question(question)

==============================
FILE: D:\ai-tro-giang\backend\api\courses.py
==============================

from fastapi import APIRouter

router = APIRouter(prefix="/courses", tags=["Courses"])

@router.get(
    "/",
    operation_id="courses_list"
)
def list_courses():
    return []

==============================
FILE: D:\ai-tro-giang\backend\api\eval.py
==============================

from fastapi import APIRouter
from backend.eval.runner import run_evaluation

router = APIRouter(prefix="/eval", tags=["Eval"])

@router.post(
    "/run",
    operation_id="eval_run"
)
def run_eval():
    return run_evaluation()

==============================
FILE: D:\ai-tro-giang\backend\api\pedagogy.py
==============================

from fastapi import APIRouter

router = APIRouter(prefix="/pedagogy", tags=["Pedagogy"])

@router.post(
    "/generate",
    operation_id="pedagogy_generate"
)
def generate():
    return {"status": "ok"}

==============================
FILE: D:\ai-tro-giang\backend\api\upload.py
==============================

import os
from fastapi import APIRouter, UploadFile, File, HTTPException
from backend.rag.ingest import ingest_document

router = APIRouter()

# Thu m?c luu tr? file
UPLOAD_DIR = "data/raw_docs"

@router.post("/")
async def upload_document(course_id: str, file: UploadFile = File(...)):
    # 1. T?o thu m?c n?u chua t?n t?i
    if not os.path.exists(UPLOAD_DIR):
        os.makedirs(UPLOAD_DIR)

    # 2. Ki?m tra d?nh d?ng file (ch? nh?n .txt trong b?n hi?n t?i)
    if not file.filename.endswith('.txt'):
        raise HTTPException(status_code=400, detail="Ch? h? tr? file .txt")

    file_path = os.path.join(UPLOAD_DIR, file.filename)
    
    try:
        # 3. Luu file vo ? dia
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        # 4. Ingest vo Vector Database
        ingest_document(file_path, course_id)

        return {"message": f" upload v x? l file {file.filename} thnh cng cho kha h?c {course_id}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

==============================
FILE: D:\ai-tro-giang\backend\auth\deps.py
==============================

from backend.auth.roles import UserRole

def get_current_user():
    # MVP gi? l?p, sau ny n?i JWT
    return {
        "id": "demo_user",
        "role": UserRole.TEACHER
    }

==============================
FILE: D:\ai-tro-giang\backend\auth\roles.py
==============================

from enum import Enum

class UserRole(str, Enum):
    STUDENT = "student"
    TEACHER = "teacher"

==============================
FILE: D:\ai-tro-giang\backend\auth\security.py
==============================

from datetime import datetime, timedelta
from jose import jwt, JWTError

SECRET_KEY = "CHANGE_ME_SECRET"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60


def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)


def verify_token(token: str):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    except JWTError:
        return None

==============================
FILE: D:\ai-tro-giang\backend\auth\users.py
==============================

# backend/auth/users.py

from passlib.context import CryptContext

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def hash_password(password: str) -> str:
    return pwd_context.hash(password)

def verify_password(plain, hashed):
    return pwd_context.verify(plain, hashed)

# Hash s?n (t?o 1 l?n r?i hardcode)
USERS_DB = {
    "teacher1": {
        "username": "teacher1",
        "password": "$2b$12$6b8KJ7u9yM3mY8qY1bQ3KOUxM8Y1zA0k9Ff5M9XqZxQxJw5s5Oe9a",  # 123456
        "role": "teacher"
    },
    "student1": {
        "username": "student1",
        "password": "$2b$12$6b8KJ7u9yM3mY8qY1bQ3KOUxM8Y1zA0k9Ff5M9XqZxQxJw5s5Oe9a",  # 123456
        "role": "student"
    }
}

def authenticate(username: str, password: str):
    user = USERS_DB.get(username)
    if not user:
        return None
    if not verify_password(password, user["password"]):
        return None
    return user

==============================
FILE: D:\ai-tro-giang\backend\courses\__init__.py
==============================


==============================
FILE: D:\ai-tro-giang\backend\courses\access.py
==============================

def check_course_access(user, course):
    if user["role"] == "teacher":
        return course.owner_id == user["id"]
    if user["role"] == "student":
        # MVP: cho php t?t c? SV (sau ny g?n class)
        return True
    return False

==============================
FILE: D:\ai-tro-giang\backend\courses\models.py
==============================

from dataclasses import dataclass

@dataclass
class Course:
    course_id: str
    name: str
    owner_id: str  # gi?ng vin

==============================
FILE: D:\ai-tro-giang\backend\courses\service.py
==============================

from backend.courses.models import Course

# MVP: in-memory, sau ny thay DB
COURSES = {}

def create_course(course_id: str, name: str, owner_id: str):
    course = Course(course_id, name, owner_id)
    COURSES[course_id] = course
    return course

def get_course(course_id: str):
    return COURSES.get(course_id)

def list_courses_by_owner(owner_id: str):
    return [c for c in COURSES.values() if c.owner_id == owner_id]

==============================
FILE: D:\ai-tro-giang\backend\eval\__init__.py
==============================


==============================
FILE: D:\ai-tro-giang\backend\eval\datasets.py
==============================

from dataclasses import dataclass
from typing import List

@dataclass
class EvalSample:
    id: int
    question: str
    expected_answer: str | None = None

EVAL_DATASET: List[EvalSample] = [
    EvalSample(
        id=1,
        question="FAISS l g trong h? th?ng RAG?"
    ),
    EvalSample(
        id=2,
        question="BM25 dng d? lm g?"
    ),
]

==============================
FILE: D:\ai-tro-giang\backend\eval\evaluator.py
==============================

from backend.rag.hybrid_retriever import hybrid_search

MIN_RETRIEVAL_SCORE = 0.15

def evaluate_sample(sample: dict) -> dict:
    """
    sample = {
        id: int,
        question: str
    }
    """

    question = sample.get("question")
    if not question:
        raise ValueError("Missing question in eval sample")

    results = hybrid_search(question)

    grounded = False
    used_chunks = []

    for doc, score in results:
        if score >= MIN_RETRIEVAL_SCORE:
            grounded = True
            used_chunks.append({
                "chunk_id": doc.metadata.get("chunk_id"),
                "source_file": doc.metadata.get("source_file"),
                "score": round(score, 4)
            })

    return {
        "id": sample.get("id"),
        "question": question,
        "grounded": grounded,
        "num_chunks": len(used_chunks),
        "chunks": used_chunks,
        "score": 1.0 if grounded else 0.0
    }

==============================
FILE: D:\ai-tro-giang\backend\eval\faithfulness.py
==============================

import re
from typing import Dict, Any, List


STOPWORDS = {
    "l", "v", "c?a", "trong", "cho", "v?i", "m?t", "cc",
    "du?c", "khi", "ny", "d", "nhu", "d?"
}


def normalize(text: str) -> List[str]:
    """
    Normalize text into keyword tokens
    """
    text = text.lower()
    text = re.sub(r"[^a-z0-9-?\s]", " ", text)
    tokens = text.split()
    return [t for t in tokens if t not in STOPWORDS and len(t) > 2]


def extract_claim_sentences(answer: str) -> List[str]:
    """
    Split answer into claim sentences (remove chunk tags)
    """
    clean = re.sub(r"\[CHUNK_\d+\]", "", answer)
    sentences = re.split(r"[.\n]", clean)
    return [s.strip() for s in sentences if len(s.strip()) > 20]


def check_faithfulness(
    answer: str,
    contexts: List[str],
    min_overlap_ratio: float = 0.3
) -> Dict[str, Any]:
    """
    Faithfulness check:
    - Each claim sentence must overlap sufficiently with context keywords
    """

    context_text = " ".join(contexts)
    context_tokens = set(normalize(context_text))

    claims = extract_claim_sentences(answer)

    if not claims:
        return {
            "faithful": False,
            "reason": "NO_CLAIM_SENTENCE",
            "details": "No valid claim sentence found in answer."
        }

    unfaithful_claims = []

    for claim in claims:
        claim_tokens = normalize(claim)

        if not claim_tokens:
            continue

        overlap = set(claim_tokens) & context_tokens
        overlap_ratio = len(overlap) / len(set(claim_tokens))

        if overlap_ratio < min_overlap_ratio:
            unfaithful_claims.append({
                "claim": claim,
                "overlap_ratio": round(overlap_ratio, 2)
            })

    if unfaithful_claims:
        return {
            "faithful": False,
            "reason": "LOW_CONTEXT_OVERLAP",
            "details": unfaithful_claims
        }

    return {
        "faithful": True,
        "reason": "OK",
        "details": "All claims sufficiently supported by context."
    }

==============================
FILE: D:\ai-tro-giang\backend\eval\groundedness.py
==============================

import re
from typing import Dict, Any, List


CHUNK_PATTERN = re.compile(r"\[CHUNK_(\d+)\]")


def extract_chunk_ids(answer: str) -> List[int]:
    """
    Extract chunk indices from answer text.
    Example: [CHUNK_0], [CHUNK_2] ? [0, 2]
    """
    return [int(x) for x in CHUNK_PATTERN.findall(answer)]


def check_groundedness(
    answer: str,
    sources: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Groundedness check:
    - Answer must contain chunk tags
    - Chunk tags must exist in retrieved sources
    """

    chunk_ids_in_answer = extract_chunk_ids(answer)

    if not chunk_ids_in_answer:
        return {
            "grounded": False,
            "reason": "NO_CHUNK_TAG",
            "details": "Answer does not contain any [CHUNK_x] tags."
        }

    valid_chunk_ids = {
        src.get("chunk_id") for src in sources
    }

    invalid_refs = [
        cid for cid in chunk_ids_in_answer
        if cid not in valid_chunk_ids
    ]

    if invalid_refs:
        return {
            "grounded": False,
            "reason": "INVALID_CHUNK_REFERENCE",
            "details": f"Referenced chunk(s) not in sources: {invalid_refs}"
        }

    return {
        "grounded": True,
        "reason": "OK",
        "details": f"All referenced chunks are valid: {chunk_ids_in_answer}"
    }

==============================
FILE: D:\ai-tro-giang\backend\eval\metrics.py
==============================

def groundedness_score(sources, expected_sources):
    if not sources:
        return 0.0

    matched = 0
    for src in sources:
        source_file = src.get("source_file", "")
        if any(exp in source_file for exp in expected_sources):
            matched += 1

    return matched / len(expected_sources)


def citation_coverage(sources):
    if not sources:
        return 0.0

    cited = [s for s in sources if s.get("page") is not None]
    return len(cited) / len(sources)


def hallucination_flag(sources):
    return len(sources) == 0

==============================
FILE: D:\ai-tro-giang\backend\eval\runner.py
==============================

from backend.eval.datasets import EVAL_DATASET
from backend.eval.evaluator import evaluate_sample
from backend.logging.audit import audit_log

def run_evaluation():
    results = []

    for sample in EVAL_DATASET:
        result = evaluate_sample({
            "id": sample.id,
            "question": sample.question
        })

        results.append(result)

        audit_log(
            user="system_eval",
            action="evaluation_run",
            payload=result
        )

    return results

==============================
FILE: D:\ai-tro-giang\backend\guardrails\citation.py
==============================

def format_citations(source_documents):
    citations = []
    for doc in source_documents:
        meta = doc.metadata
        citations.append({
            "source": meta.get("source", "unknown"),
            "page": meta.get("page", None)
        })
    return citations

==============================
FILE: D:\ai-tro-giang\backend\guardrails\grounding.py
==============================

from typing import List

MIN_DOCS_REQUIRED = 1

def check_grounding(source_documents: List):
    if not source_documents or len(source_documents) < MIN_DOCS_REQUIRED:
        raise ValueError(
            "Khng tm th?y ti li?u lin quan. "
            "H? th?ng khng th? tr? l?i d? trnh sai l?ch h?c thu?t."
        )

==============================
FILE: D:\ai-tro-giang\backend\guardrails\rate_limit.py
==============================

import time

REQUEST_LIMIT = 20
WINDOW_SECONDS = 60

_user_requests = {}

def check_rate_limit(user_id: str):
    now = time.time()
    timestamps = _user_requests.get(user_id, [])

    timestamps = [t for t in timestamps if now - t < WINDOW_SECONDS]

    if len(timestamps) >= REQUEST_LIMIT:
        raise ValueError("B?n g?i qu nhi?u yu c?u. Vui lng th? l?i sau.")

    timestamps.append(now)
    _user_requests[user_id] = timestamps

==============================
FILE: D:\ai-tro-giang\backend\llm\__init__.py
==============================


==============================
FILE: D:\ai-tro-giang\backend\llm\llm.py
==============================

import os
try:
    from google import genai
    HAS_GENAI = True
except ImportError:
    HAS_GENAI = False

class GeminiLLM:
    def __init__(self, api_key: str):
        self.client = None
        if HAS_GENAI and api_key:
            # S? d?ng thu vi?n google-genai m?i nh?t
            self.client = genai.Client(api_key=api_key)
            self.model_id = "gemini-1.5-flash"
        
    def generate(self, prompt: str) -> str:
        if not self.client:
            return "H? th?ng dang ch?y Offline ho?c thi?u API Key. (Vui lng c?u hnh l?i llm.py)"
        
        try:
            response = self.client.models.generate_content(
                model=self.model_id,
                contents=prompt
            )
            return response.text
        except Exception as e:
            return f"L?i Gemini API: {str(e)}"

# --- C?U HNH ---
# 1. N?u mu?n dng th?t: Dn key vo dy
MY_API_KEY = "" 

# 2. Kh?i t?o instance
if MY_API_KEY:
    llm_instance = GeminiLLM(api_key=MY_API_KEY)
else:
    # N?u khng c key, tr? v? class gi? l?p d? khng l?i h? th?ng
    class OfflineLLM:
        def generate(self, prompt: str):
            return "AI Tr? Gi?ng (Offline): Ti li?u d du?c tm th?y nhung c?n API Key d? tm t?t cu tr? l?i."
    llm_instance = OfflineLLM()

def get_llm():
    return llm_instance

==============================
FILE: D:\ai-tro-giang\backend\logging\audit.py
==============================

import json
import uuid
from datetime import datetime
from pathlib import Path
from typing import List

LOG_PATH = Path("data/audit.log")

def audit_log(
    user: str,
    action: str,
    payload: dict,
    course_id: str | None = None,
    chunk_ids: List[str] | None = None,
    model_name: str | None = None,
    request_id: str | None = None
):
    LOG_PATH.parent.mkdir(parents=True, exist_ok=True)

    record = {
        "timestamp": datetime.utcnow().isoformat(),
        "request_id": request_id or str(uuid.uuid4()),
        "user": user,
        "action": action,
        "course_id": course_id,
        "chunk_ids": chunk_ids or [],
        "model": model_name or "offline_stub",
        "payload": payload,
        "schema": "academic_audit_v3"
    }

    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

==============================
FILE: D:\ai-tro-giang\backend\main.py
==============================

from fastapi import FastAPI

from backend.api.chat import router as chat_router
from backend.api.upload import router as upload_router
from backend.api.auth import router as auth_router
from backend.api.pedagogy import router as pedagogy_router
from backend.api.eval import router as eval_router
from backend.api.courses import router as courses_router

app = FastAPI(title="AI Tr? Gi?ng")

app.include_router(auth_router)
app.include_router(upload_router)
app.include_router(chat_router)
app.include_router(pedagogy_router)
app.include_router(eval_router)
app.include_router(courses_router)

==============================
FILE: D:\ai-tro-giang\backend\pedagogy\__init__.py
==============================


==============================
FILE: D:\ai-tro-giang\backend\pedagogy\bloom.py
==============================

from enum import Enum
from typing import List


class BloomLevel(str, Enum):
    """
    Bloom's Taxonomy  Cognitive Domain
    """

    REMEMBER = "remember"
    UNDERSTAND = "understand"
    APPLY = "apply"
    ANALYZE = "analyze"
    EVALUATE = "evaluate"
    CREATE = "create"

    @property
    def description(self) -> str:
        return {
            self.REMEMBER: "Nh? l?i thng tin, d?nh nghia, thu?t ng?",
            self.UNDERSTAND: "Gi?i thch, m t?, di?n gi?i  nghia",
            self.APPLY: "p d?ng ki?n th?c vo bi ton c? th?",
            self.ANALYZE: "Phn tch, so snh, ch? ra m?i quan h?",
            self.EVALUATE: "nh gi, nh?n xt, ph?n bi?n",
            self.CREATE: "T?ng h?p, thi?t k?, xy d?ng m?i"
        }[self]

    @property
    def action_verbs(self) -> List[str]:
        """
        G?i  d?ng t? dng trong cu h?i / bi t?p
        """
        return {
            self.REMEMBER: ["li?t k", "nu", "d?nh nghia", "k? tn"],
            self.UNDERSTAND: ["gi?i thch", "trnh by", "m t?", "tm t?t"],
            self.APPLY: ["p d?ng", "tnh ton", "gi?i", "th?c hi?n"],
            self.ANALYZE: ["phn tch", "so snh", "phn lo?i", "lm r"],
            self.EVALUATE: ["dnh gi", "nh?n xt", "ph?n bi?n", "so snh uu nhu?c di?m"],
            self.CREATE: ["thi?t k?", "xy d?ng", "d? xu?t", "pht tri?n"]
        }[self]

    @classmethod
    def ordered_levels(cls) -> List["BloomLevel"]:
        """
        Tr? v? Bloom levels theo th? t? tang d?n d? kh
        """
        return [
            cls.REMEMBER,
            cls.UNDERSTAND,
            cls.APPLY,
            cls.ANALYZE,
            cls.EVALUATE,
            cls.CREATE
        ]

==============================
FILE: D:\ai-tro-giang\backend\pedagogy\generator.py
==============================

import json
from typing import Dict, Any, List

from backend.pedagogy.bloom import BloomLevel
from backend.pedagogy.prompts import (
    build_question_prompt,
    build_assignment_prompt
)
from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import get_llm


CONTEXT_TOP_K = 5


def retrieve_context(query: str) -> str:
    """
    Retrieve learning context using Hybrid Search
    """
    results = hybrid_search(query)
    contexts = []

    for doc, score in results[:CONTEXT_TOP_K]:
        contexts.append(doc.page_content)

    return "\n\n".join(contexts)


def _invoke_llm(prompt: str) -> List[Dict[str, Any]]:
    """
    Invoke LLM and parse JSON output safely
    """
    llm = get_llm()
    raw = llm.invoke(prompt)

    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        raise ValueError(
            "LLM output is not valid JSON. "
            "Please refine prompt or model configuration."
        )


def generate_questions(
    topic: str,
    bloom: BloomLevel,
    num_items: int = 3
) -> List[Dict[str, Any]]:
    """
    Generate pedagogical questions based on Bloom level
    """

    context = retrieve_context(topic)

    if not context.strip():
        raise ValueError("No relevant learning material found.")

    prompt = build_question_prompt(
        context=context,
        bloom=bloom,
        num_items=num_items
    )

    return _invoke_llm(prompt)


def generate_assignments(
    topic: str,
    bloom: BloomLevel,
    num_items: int = 2
) -> List[Dict[str, Any]]:
    """
    Generate pedagogical assignments based on Bloom level
    """

    context = retrieve_context(topic)

    if not context.strip():
        raise ValueError("No relevant learning material found.")

    prompt = build_assignment_prompt(
        context=context,
        bloom=bloom,
        num_items=num_items
    )

    return _invoke_llm(prompt)

==============================
FILE: D:\ai-tro-giang\backend\pedagogy\prompts.py
==============================

from backend.pedagogy.bloom import BloomLevel


QUESTION_PROMPT_TEMPLATE = """
B?n l AI Tr? Gi?ng.

NHI?M V?:
Sinh {num_items} CU H?I h?c t?p theo m?c d? Bloom: {bloom_level}

M T? M?C ?:
{bloom_description}

YU C?U B?T BU?C:
- CH? s? d?ng thng tin trong TI LI?U bn du?i
- Khng suy don, khng thm ki?n th?c bn ngoi
- M?i cu h?i ph?i bm st n?i dung ti li?u
- M?c d? nh?n th?c ph?i dng Bloom level
- Khng trng l?p cu h?i

TI LI?U:
{context}

?NH D?NG TR? V? (JSON):
[
  {{
    "question": "...",
    "bloom_level": "{bloom_level}",
    "learning_objective": "...",
    "difficulty": "easy | medium | hard"
  }}
]
"""


ASSIGNMENT_PROMPT_TEMPLATE = """
B?n l AI Tr? Gi?ng.

NHI?M V?:
Sinh {num_items} BI T?P h?c t?p theo m?c d? Bloom: {bloom_level}

M T? M?C ?:
{bloom_description}

YU C?U B?T BU?C:
- CH? s? d?ng thng tin trong TI LI?U bn du?i
- Khng suy don, khng thm ki?n th?c bn ngoi
- Bi t?p ph?i yu c?u ngu?i h?c v?n d?ng dng m?c Bloom
- C m t? r yu c?u d?u ra

TI LI?U:
{context}

?NH D?NG TR? V? (JSON):
[
  {{
    "title": "...",
    "task": "...",
    "expected_output": "...",
    "bloom_level": "{bloom_level}",
    "difficulty": "easy | medium | hard"
  }}
]
"""


def build_question_prompt(
    context: str,
    bloom: BloomLevel,
    num_items: int = 3
) -> str:
    return QUESTION_PROMPT_TEMPLATE.format(
        num_items=num_items,
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )


def build_assignment_prompt(
    context: str,
    bloom: BloomLevel,
    num_items: int = 2
) -> str:
    return ASSIGNMENT_PROMPT_TEMPLATE.format(
        num_items=num_items,
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )

==============================
FILE: D:\ai-tro-giang\backend\pedagogy\rubric.py
==============================

import json
from typing import List, Dict, Any

from backend.pedagogy.bloom import BloomLevel
from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import get_llm


CONTEXT_TOP_K = 5


def retrieve_context(topic: str) -> str:
    results = hybrid_search(topic)
    contexts = []

    for doc, _ in results[:CONTEXT_TOP_K]:
        contexts.append(doc.page_content)

    return "\n\n".join(contexts)


RUBRIC_PROMPT_TEMPLATE = """
B?n l GI?NG VIN.

NHI?M V?:
Xy d?ng RUBRIC CH?M I?M cho bi h?c v?i m?c d? Bloom: {bloom_level}

M T? M?C ?:
{bloom_description}

YU C?U:
- Ch? s? d?ng thng tin trong TI LI?U
- Rubric ph?i dnh gi dng m?c Bloom
- C nhi?u tiu ch ch?m di?m
- C m t? r cho t?ng m?c di?m

TI LI?U:
{context}

?NH D?NG TR? V? (JSON):
[
  {{
    "criterion": "...",
    "levels": {{
      "excellent": "...",
      "good": "...",
      "average": "...",
      "poor": "..."
    }}
  }}
]
"""


def generate_rubric(
    topic: str,
    bloom: BloomLevel
) -> List[Dict[str, Any]]:
    """
    Generate grading rubric based on Bloom level
    """

    context = retrieve_context(topic)

    if not context.strip():
        raise ValueError("No relevant learning material found.")

    prompt = RUBRIC_PROMPT_TEMPLATE.format(
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )

    llm = get_llm()
    raw = llm.invoke(prompt)

    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        raise ValueError("LLM output is not valid JSON.")

==============================
FILE: D:\ai-tro-giang\backend\pedagogy\schemas.py
==============================

from pydantic import BaseModel
from typing import List, Optional

class TaskRequest(BaseModel):
    task_type: str   # lesson_plan | assignment | exam | rubric | quiz
    subject: str
    clo: Optional[List[str]] = None
    bloom_level: Optional[str] = None
    difficulty: Optional[str] = None
    num_items: Optional[int] = None
    duration_minutes: Optional[int] = None

==============================
FILE: D:\ai-tro-giang\backend\pedagogy\templates.py
==============================

LESSON_PLAN_TEMPLATE = """
B?n l tr? gi?ng d?i h?c.
D?a CH? trn ti li?u sau:
{context}

Hy so?n gio n cho mn: {subject}
CLO: {clo}
Th?i lu?ng: {duration} pht

Yu c?u:
- Khng thm ki?n th?c ngoi ti li?u
- C?u trc r rng
- C ho?t d?ng GV/SV
"""

==============================
FILE: D:\ai-tro-giang\backend\pedagogy\validators.py
==============================

def validate_groundedness(answer, sources):
    if not sources:
        raise ValueError(
            "N?i dung sinh ra khng c can c? t? ti li?u."
        )

==============================
FILE: D:\ai-tro-giang\backend\rag\__init__.py
==============================


==============================
FILE: D:\ai-tro-giang\backend\rag\hybrid_retriever.py
==============================

from typing import List, Tuple, Dict
from langchain_core.documents import Document

from backend.vectorstore.faiss_store import get_faiss_store
from backend.vectorstore.bm25_store import BM25Store


FAISS_K = 5
BM25_K = 5

FAISS_WEIGHT = 0.6
BM25_WEIGHT = 0.4

MIN_QUERY_LENGTH = 5


def hybrid_search(query: str) -> List[Tuple[Document, float]]:
    """
    Hybrid retrieval: FAISS (semantic) + BM25 (keyword)

    Defensive rules:
    - Reject empty / too-short query
    - Safe when FAISS/BM25 are empty (cold-start)
    - Never throw exception
    """

    # === INPUT VALIDATION ===
    if not query or not isinstance(query, str):
        return []

    query = query.strip()
    if len(query) < MIN_QUERY_LENGTH:
        return []

    # === LOAD STORES (COLD-START SAFE) ===
    try:
        faiss_store = get_faiss_store()
    except Exception:
        faiss_store = None

    try:
        bm25_store = BM25Store.load()
    except Exception:
        bm25_store = None

    score_map: Dict[str, Dict] = {}

    # === FAISS SEARCH ===
    if faiss_store is not None:
        try:
            faiss_results = faiss_store.similarity_search_with_score(
                query,
                k=FAISS_K
            )
        except Exception:
            faiss_results = []

        for doc, score in faiss_results:
            doc_key = f"{doc.metadata.get('doc_id')}:{doc.metadata.get('chunk_id')}"
            semantic_score = 1 / (1 + score)  # distance ? similarity

            score_map[doc_key] = {
                "doc": doc,
                "semantic": semantic_score,
                "keyword": 0.0
            }

    # === BM25 SEARCH ===
    if bm25_store is not None:
        try:
            bm25_results = bm25_store.search(query, k=BM25_K)
        except Exception:
            bm25_results = []

        for doc, score in bm25_results:
            doc_key = f"{doc.metadata.get('doc_id')}:{doc.metadata.get('chunk_id')}"

            if doc_key not in score_map:
                score_map[doc_key] = {
                    "doc": doc,
                    "semantic": 0.0,
                    "keyword": score
                }
            else:
                score_map[doc_key]["keyword"] = score

    # === COMBINE SCORES ===
    results: List[Tuple[Document, float]] = []

    for entry in score_map.values():
        hybrid_score = (
            FAISS_WEIGHT * entry["semantic"]
            + BM25_WEIGHT * entry["keyword"]
        )

        if hybrid_score > 0:
            results.append((entry["doc"], hybrid_score))

    results.sort(key=lambda x: x[1], reverse=True)

    return results

==============================
FILE: D:\ai-tro-giang\backend\rag\ingest.py
==============================

import os
# C?p nh?t du?ng d?n import m?i
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter 
from backend.vectorstore.faiss_store import get_faiss_store

def ingest_document(file_path: str, course_id: str):
    # 1. Load document
    loader = TextLoader(file_path, encoding='utf-8')
    documents = loader.load()

    # 2. Chia nh? van b?n (S? d?ng class t? langchain_text_splitters)
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = text_splitter.split_documents(documents)

    # 3. Gn metadata
    for d in docs:
        d.metadata["course_id"] = course_id
        d.metadata["source_file"] = os.path.basename(file_path)

    # 4. Thm vo Vector Store
    vector_store = get_faiss_store()
    vector_store.add_documents(docs)
    
    # 5. Luu index xu?ng ? dia
    index_dir = "data/faiss_index"
    if not os.path.exists(index_dir):
        os.makedirs(index_dir)
        
    index_path = os.path.join(index_dir, course_id)
    vector_store.save_local(index_path)
    print(f"---  Ingest xong kha h?c: {course_id} ---")

==============================
FILE: D:\ai-tro-giang\backend\utils\chunking.py
==============================

# backend/utils/chunking.py

import uuid
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document



def chunk_text(
    text: str,
    source_file: str,
    page: int = None,
    section: str = None,
    chunk_size: int = 400,
    chunk_overlap: int = 80
) -> List[Document]:
    """
    Chunk text into Documents with full metadata
    """

    doc_id = str(uuid.uuid4())

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    chunks = splitter.split_text(text)

    documents = []

    for idx, chunk in enumerate(chunks):
        metadata = {
            "doc_id": doc_id,
            "source_file": source_file,
            "page": page,
            "section": section,
            "chunk_id": idx
        }

        documents.append(
            Document(
                page_content=chunk,
                metadata=metadata
            )
        )

    return documents

==============================
FILE: D:\ai-tro-giang\backend\utils\citation.py
==============================

def format_apa(source: dict) -> str:
    """
    APA 7th basic format
    """
    author = "Unknown"
    year = "n.d."
    title = source.get("source_file", "Unknown document")
    page = source.get("page")

    page_part = f" (p. {page})" if page is not None else ""

    return f"{author} ({year}). {title}{page_part}."


def format_ieee(source: dict, index: int) -> str:
    """
    IEEE basic format
    """
    title = source.get("source_file", "Unknown document")
    page = source.get("page")

    page_part = f", p. {page}" if page is not None else ""

    return f"[{index}] {title}{page_part}."

==============================
FILE: D:\ai-tro-giang\backend\utils\text_extraction.py
==============================

from pathlib import Path
from pypdf import PdfReader
from docx import Document as DocxDocument


def extract_text(file_path: str) -> str:
    ext = Path(file_path).suffix.lower()

    if ext == ".pdf":
        reader = PdfReader(file_path)
        return "\n".join(
            page.extract_text() or ""
            for page in reader.pages
        )

    if ext in [".docx"]:
        doc = DocxDocument(file_path)
        return "\n".join(p.text for p in doc.paragraphs)

    if ext in [".txt"]:
        with open(file_path, encoding="utf-8", errors="ignore") as f:
            return f.read()

    raise ValueError(f"Unsupported file type: {ext}")

==============================
FILE: D:\ai-tro-giang\backend\vectorstore\bm25_store.py
==============================

import pickle
from pathlib import Path
from typing import List, Tuple

from rank_bm25 import BM25Okapi
from langchain_core.documents import Document



BM25_PATH = Path("data/bm25.pkl")


class BM25Store:
    def __init__(self):
        self.documents: List[Document] = []
        self.corpus = []
        self.bm25 = None

    def add_documents(self, docs: List[Document]):
        for doc in docs:
            tokens = doc.page_content.lower().split()
            self.documents.append(doc)
            self.corpus.append(tokens)

        self.bm25 = BM25Okapi(self.corpus)

    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        if not self.bm25:
            return []

        query_tokens = query.lower().split()
        scores = self.bm25.get_scores(query_tokens)

        ranked = sorted(
            enumerate(scores),
            key=lambda x: x[1],
            reverse=True
        )[:k]

        return [
            (self.documents[idx], score)
            for idx, score in ranked
            if score > 0
        ]

    def save(self):
        BM25_PATH.parent.mkdir(parents=True, exist_ok=True)
        with open(BM25_PATH, "wb") as f:
            pickle.dump(self, f)

    @staticmethod
    def load():
        if not BM25_PATH.exists():
            return BM25Store()

        with open(BM25_PATH, "rb") as f:
            return pickle.load(f)

==============================
FILE: D:\ai-tro-giang\backend\vectorstore\faiss_store.py
==============================

# backend/vectorstore/faiss_store.py

import os
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

_FAISS_STORE = None
FAISS_PATH = "data/faiss_index"

def get_embeddings():
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

def get_faiss_store():
    global _FAISS_STORE

    if _FAISS_STORE is not None:
        return _FAISS_STORE

    embeddings = get_embeddings()

    index_file = os.path.join(FAISS_PATH, "index.faiss")

    if os.path.exists(index_file):
        # ? Load index cu
        _FAISS_STORE = FAISS.load_local(
            FAISS_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )
    else:
        # ? T?o index m?i (EMPTY, khng crash)
        _FAISS_STORE = FAISS.from_texts(
            texts=["__init__"],
            embedding=embeddings,
            metadatas=[{"system": True}]
        )

        os.makedirs(FAISS_PATH, exist_ok=True)
        _FAISS_STORE.save_local(FAISS_PATH)

    return _FAISS_STORE


def save_faiss_store():
    if _FAISS_STORE is not None:
        _FAISS_STORE.save_local(FAISS_PATH)

==============================
FILE: D:\ai-tro-giang\docker-compose.yml
==============================

version: "3.9"

services:
  backend:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data

==============================
FILE: D:\ai-tro-giang\Dockerfile
==============================

FROM python:3.10

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY backend backend
COPY data data

CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]

==============================
FILE: D:\ai-tro-giang\frontend\src\api.js
==============================

import axios from "axios";

export const api = axios.create({
  baseURL: "http://127.0.0.1:8000"
});

export function setToken(token) {
  api.defaults.headers.common["Authorization"] = `Bearer ${token}`;
}

==============================
FILE: D:\ai-tro-giang\frontend\src\App.jsx
==============================

import { useState } from "react";
import { api, setToken } from "./api";
import ChatBox from "./components/ChatBox";

export default function App() {
  const [username, setUsername] = useState("");
  const [password, setPassword] = useState("");
  const [role, setRole] = useState(null);
  const [file, setFile] = useState(null);

  async function login() {
    const res = await api.post("/auth/login", {
      username,
      password,
    });

    setToken(res.data.access_token);
    setRole(res.data.role);

    alert("Login thnh cng v?i role: " + res.data.role);
  }

  async function upload() {
    const form = new FormData();
    form.append("file", file);

    await api.post("/upload/", form);
    alert("Upload & ingest xong");
  }

  return (
    <div style={{ padding: 30, fontFamily: "Arial" }}>
      <h2>AI Tr? Gi?ng</h2>

      {/* LOGIN */}
      <h3>Login</h3>
      <input
        placeholder="username"
        onChange={(e) => setUsername(e.target.value)}
      />
      <br />
      <input
        type="password"
        placeholder="password"
        onChange={(e) => setPassword(e.target.value)}
      />
      <br />
      <button onClick={login}>Login</button>

      {/* UPLOAD  TEACHER ONLY */}
      {role === "teacher" && (
        <>
          <h3>Upload ti li?u (GV)</h3>
          <input
            type="file"
            onChange={(e) => setFile(e.target.files[0])}
          />
          <br />
          <button onClick={upload}>Upload</button>
        </>
      )}

      <hr />

      {/* CHAT  DNG CHATBOX PHASE 3 */}
      <ChatBox />
    </div>
  );
}

==============================
FILE: D:\ai-tro-giang\frontend\src\components\ChatBox.jsx
==============================

import { useState } from "react";

export default function ChatBox() {
  const [question, setQuestion] = useState("");
  const [answer, setAnswer] = useState("");
  const [sources, setSources] = useState([]);
  const [citations, setCitations] = useState(null);

  const ask = async () => {
    const res = await fetch("http://localhost:8000/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ question }),
    });

    const data = await res.json();
    setAnswer(data.answer);
    setSources(data.sources || []);
    setCitations(data.citations || null);
  };

  return (
    <div style={{ maxWidth: 800, margin: "40px auto" }}>
      <h2>AI Tr? Gi?ng</h2>

      <textarea
        value={question}
        onChange={(e) => setQuestion(e.target.value)}
        rows={3}
        style={{ width: "100%" }}
      />

      <button onClick={ask} style={{ marginTop: 10 }}>
        H?i
      </button>

      {answer && (
        <>
          <h3>Tr? l?i (c g?n ngu?n)</h3>
          <pre style={{ whiteSpace: "pre-wrap" }}>{answer}</pre>
        </>
      )}

      {sources.length > 0 && (
        <>
          <h4>Ngu?n tham kh?o theo Chunk</h4>
          <ul>
            {sources.map((s, idx) => (
              <li key={idx}>
                <b>[CHUNK_{idx}]</b>  {s.source_file}
                {s.page !== null && `  Trang ${s.page}`}
                {s.section && `  ${s.section}`}
                <details>
                  <summary>Xem trch do?n</summary>
                  <small>{s.preview}</small>
                </details>
              </li>
            ))}
          </ul>
        </>
      )}

      {citations && (
        <>
          <h4>References (APA)</h4>
          <ul>
            {citations.apa.map((c, i) => (
              <li key={i}>{c}</li>
            ))}
          </ul>

          <h4>References (IEEE)</h4>
          <ul>
            {citations.ieee.map((c, i) => (
              <li key={i}>{c}</li>
            ))}
          </ul>
        </>
      )}
    </div>
  );
}

==============================
FILE: D:\ai-tro-giang\frontend\src\components\FileUpload.jsx
==============================

import React, { useState } from "react";
import axios from "axios";

function FileUpload() {
  const [selectedFile, setSelectedFile] = useState(null);
  const [courseId, setCourseId] = useState("ML101");
  const [loading, setLoading] = useState(false);

  const handleFileChange = (e) => {
    setSelectedFile(e.target.files[0]);
  };

  const handleUpload = async () => {
    if (!selectedFile) {
      alert("Vui lng ch?n m?t file!");
      return;
    }

    setLoading(true);
    const formData = new FormData();
    // Quan tr?ng: Tn 'file' ph?i kh?p v?i backend
    formData.append("file", selectedFile);

    try {
      const response = await axios.post(
        `http://localhost:8000/upload/?course_id=${courseId}`,
        formData,
        {
          headers: { "Content-Type": "multipart/form-data" },
        }
      );
      alert("Upload thnh cng: " + response.data.message);
    } catch (error) {
      console.error("L?i upload:", error);
      alert("L?i upload: " + (error.response?.data?.detail || "Khng r l?i"));
    } finally {
      setLoading(false);
    }
  };

  return (
    <div style={{ padding: "20px", border: "1px solid #ccc", marginBottom: "20px" }}>
      <h3>Upload Ti Li?u (GV)</h3>
      <input
        type="text"
        placeholder="Course ID (vd: ML101)"
        value={courseId}
        onChange={(e) => setCourseId(e.target.value)}
      />
      <br /><br />
      <input type="file" onChange={handleFileChange} accept=".txt" />
      <button onClick={handleUpload} disabled={loading}>
        {loading ? "ang x? l..." : "T?i ln & Ingest"}
      </button>
    </div>
  );
}

export default FileUpload;

==============================
FILE: D:\ai-tro-giang\frontend\src\main.jsx
==============================

import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App";

ReactDOM.createRoot(document.getElementById("root")).render(<App />);

==============================
FILE: D:\ai-tro-giang\README.md
==============================

# AI Tr? Gi?ng (RAG-based Tutor Assistant)

D? n AI tr? gi?ng s? d?ng ki?n trc **RAG (Retrieval-Augmented Generation)**:
- Ingest ti li?u h?c t?p (TXT / PDF / DOCX)
- Luu vector b?ng FAISS
- Tr? l?i cu h?i d?a trn ti li?u (khng b?a)

---

## 1. C?u trc d? n

backend/
agent/ # AI tutor logic
rag/ # ingest + retriever
api/ # FastAPI endpoints
main.py
data/
raw_docs/ # ti li?u d?u vo (GV upload)
faiss_index/ # vector database


---

## 2. Ci d?t mi tru?ng

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt

==============================
FILE: D:\ai-tro-giang\requirements.txt
==============================

fastapi
uvicorn
python-jose
passlib[bcrypt]
python-multipart

langchain-core
langchain-community
langchain-text-splitters
faiss-cpu
sentence-transformers
transformers
torch
pypdf
python-docx

==============================
FILE: .\frontend\index.html
==============================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>AI Tr·ª£ Gi·∫£ng</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>

==============================
FILE: .\frontend\package-lock.json
==============================
{
  "name": "ai-trogiang-frontend",
  "version": "0.1.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "ai-trogiang-frontend",
      "version": "0.1.0",
      "dependencies": {
        "axios": "^1.6.0",
        "react": "^18.2.0",
        "react-dom": "^18.2.0"
      },
      "devDependencies": {
        "@vitejs/plugin-react": "^4.0.0",
        "vite": "^5.0.0"
      }
    },
    "node_modules/@babel/code-frame": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.28.6.tgz",
      "integrity": "sha512-JYgintcMjRiCvS8mMECzaEn+m3PfoQiyqukOMCCVQtoJGYJw8j/8LBJEiqkHLkfwCcs74E3pbAUFNg7d9VNJ+Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-validator-identifier": "^7.28.5",
        "js-tokens": "^4.0.0",
        "picocolors": "^1.1.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/compat-data": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/compat-data/-/compat-data-7.28.6.tgz",
      "integrity": "sha512-2lfu57JtzctfIrcGMz992hyLlByuzgIk58+hhGCxjKZ3rWI82NnVLjXcaTqkI2NvlcvOskZaiZ5kjUALo3Lpxg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/core": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/core/-/core-7.28.6.tgz",
      "integrity": "sha512-H3mcG6ZDLTlYfaSNi0iOKkigqMFvkTKlGUYlD8GW7nNOYRrevuA46iTypPyv+06V3fEmvvazfntkBU34L0azAw==",
      "dev": true,
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "@babel/code-frame": "^7.28.6",
        "@babel/generator": "^7.28.6",
        "@babel/helper-compilation-targets": "^7.28.6",
        "@babel/helper-module-transforms": "^7.28.6",
        "@babel/helpers": "^7.28.6",
        "@babel/parser": "^7.28.6",
        "@babel/template": "^7.28.6",
        "@babel/traverse": "^7.28.6",
        "@babel/types": "^7.28.6",
        "@jridgewell/remapping": "^2.3.5",
        "convert-source-map": "^2.0.0",
        "debug": "^4.1.0",
        "gensync": "^1.0.0-beta.2",
        "json5": "^2.2.3",
        "semver": "^6.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/babel"
      }
    },
    "node_modules/@babel/generator": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.28.6.tgz",
      "integrity": "sha512-lOoVRwADj8hjf7al89tvQ2a1lf53Z+7tiXMgpZJL3maQPDxh0DgLMN62B2MKUOFcoodBHLMbDM6WAbKgNy5Suw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.28.6",
        "@babel/types": "^7.28.6",
        "@jridgewell/gen-mapping": "^0.3.12",
        "@jridgewell/trace-mapping": "^0.3.28",
        "jsesc": "^3.0.2"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-compilation-targets": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/helper-compilation-targets/-/helper-compilation-targets-7.28.6.tgz",
      "integrity": "sha512-JYtls3hqi15fcx5GaSNL7SCTJ2MNmjrkHXg4FSpOA/grxK8KwyZ5bubHsCq8FXCkua6xhuaaBit+3b7+VZRfcA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/compat-data": "^7.28.6",
        "@babel/helper-validator-option": "^7.27.1",
        "browserslist": "^4.24.0",
        "lru-cache": "^5.1.1",
        "semver": "^6.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-globals": {
      "version": "7.28.0",
      "resolved": "https://registry.npmjs.org/@babel/helper-globals/-/helper-globals-7.28.0.tgz",
      "integrity": "sha512-+W6cISkXFa1jXsDEdYA8HeevQT/FULhxzR99pxphltZcVaugps53THCeiWA8SguxxpSp3gKPiuYfSWopkLQ4hw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-module-imports": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.28.6.tgz",
      "integrity": "sha512-l5XkZK7r7wa9LucGw9LwZyyCUscb4x37JWTPz7swwFE/0FMQAGpiWUZn8u9DzkSBWEcK25jmvubfpw2dnAMdbw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/traverse": "^7.28.6",
        "@babel/types": "^7.28.6"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-module-transforms": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/helper-module-transforms/-/helper-module-transforms-7.28.6.tgz",
      "integrity": "sha512-67oXFAYr2cDLDVGLXTEABjdBJZ6drElUSI7WKp70NrpyISso3plG9SAGEF6y7zbha/wOzUByWWTJvEDVNIUGcA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-module-imports": "^7.28.6",
        "@babel/helper-validator-identifier": "^7.28.5",
        "@babel/traverse": "^7.28.6"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0"
      }
    },
    "node_modules/@babel/helper-plugin-utils": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/helper-plugin-utils/-/helper-plugin-utils-7.28.6.tgz",
      "integrity": "sha512-S9gzZ/bz83GRysI7gAD4wPT/AI3uCnY+9xn+Mx/KPs2JwHJIz1W8PZkg2cqyt3RNOBM8ejcXhV6y8Og7ly/Dug==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-string-parser": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-string-parser/-/helper-string-parser-7.27.1.tgz",
      "integrity": "sha512-qMlSxKbpRlAridDExk92nSobyDdpPijUq2DW6oDnUqd0iOGxmQjyqhMIihI9+zv4LPyZdRje2cavWPbCbWm3eA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-validator-identifier": {
      "version": "7.28.5",
      "resolved": "https://registry.npmjs.org/@babel/helper-validator-identifier/-/helper-validator-identifier-7.28.5.tgz",
      "integrity": "sha512-qSs4ifwzKJSV39ucNjsvc6WVHs6b7S03sOh2OcHF9UHfVPqWWALUsNUVzhSBiItjRZoLHx7nIarVjqKVusUZ1Q==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helper-validator-option": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/helper-validator-option/-/helper-validator-option-7.27.1.tgz",
      "integrity": "sha512-YvjJow9FxbhFFKDSuFnVCe2WxXk1zWc22fFePVNEaWJEu8IrZVlda6N0uHwzZrUM1il7NC9Mlp4MaJYbYd9JSg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/helpers": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/helpers/-/helpers-7.28.6.tgz",
      "integrity": "sha512-xOBvwq86HHdB7WUDTfKfT/Vuxh7gElQ+Sfti2Cy6yIWNW05P8iUslOVcZ4/sKbE+/jQaukQAdz/gf3724kYdqw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/template": "^7.28.6",
        "@babel/types": "^7.28.6"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/parser": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.28.6.tgz",
      "integrity": "sha512-TeR9zWR18BvbfPmGbLampPMW+uW1NZnJlRuuHso8i87QZNq2JRF9i6RgxRqtEq+wQGsS19NNTWr2duhnE49mfQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.28.6"
      },
      "bin": {
        "parser": "bin/babel-parser.js"
      },
      "engines": {
        "node": ">=6.0.0"
      }
    },
    "node_modules/@babel/plugin-transform-react-jsx-self": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-self/-/plugin-transform-react-jsx-self-7.27.1.tgz",
      "integrity": "sha512-6UzkCs+ejGdZ5mFFC/OCUrv028ab2fp1znZmCZjAOBKiBK2jXD1O+BPSfX8X2qjJ75fZBMSnQn3Rq2mrBJK2mw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/plugin-transform-react-jsx-source": {
      "version": "7.27.1",
      "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-source/-/plugin-transform-react-jsx-source-7.27.1.tgz",
      "integrity": "sha512-zbwoTsBruTeKB9hSq73ha66iFeJHuaFkUbwvqElnygoNbj/jHRsSeokowZFN3CZ64IvEqcmmkVe89OPXc7ldAw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-plugin-utils": "^7.27.1"
      },
      "engines": {
        "node": ">=6.9.0"
      },
      "peerDependencies": {
        "@babel/core": "^7.0.0-0"
      }
    },
    "node_modules/@babel/template": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.28.6.tgz",
      "integrity": "sha512-YA6Ma2KsCdGb+WC6UpBVFJGXL58MDA6oyONbjyF/+5sBgxY/dwkhLogbMT2GXXyU84/IhRw/2D1Os1B/giz+BQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.28.6",
        "@babel/parser": "^7.28.6",
        "@babel/types": "^7.28.6"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/traverse": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.28.6.tgz",
      "integrity": "sha512-fgWX62k02qtjqdSNTAGxmKYY/7FSL9WAS1o2Hu5+I5m9T0yxZzr4cnrfXQ/MX0rIifthCSs6FKTlzYbJcPtMNg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/code-frame": "^7.28.6",
        "@babel/generator": "^7.28.6",
        "@babel/helper-globals": "^7.28.0",
        "@babel/parser": "^7.28.6",
        "@babel/template": "^7.28.6",
        "@babel/types": "^7.28.6",
        "debug": "^4.3.1"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@babel/types": {
      "version": "7.28.6",
      "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.28.6.tgz",
      "integrity": "sha512-0ZrskXVEHSWIqZM/sQZ4EV3jZJXRkio/WCxaqKZP1g//CEWEPSfeZFcms4XeKBCHU0ZKnIkdJeU/kF+eRp5lBg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/helper-string-parser": "^7.27.1",
        "@babel/helper-validator-identifier": "^7.28.5"
      },
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/@esbuild/aix-ppc64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.21.5.tgz",
      "integrity": "sha512-1SDgH6ZSPTlggy1yI6+Dbkiz8xzpHJEVAlF/AM1tHPLsf5STom9rwtjE4hKAF20FfXXNTFqEYXyJNWh1GiZedQ==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "aix"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/android-arm": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.21.5.tgz",
      "integrity": "sha512-vCPvzSjpPHEi1siZdlvAlsPxXl7WbOVUBBAowWug4rJHb68Ox8KualB+1ocNvT5fjv6wpkX6o/iEpbDrf68zcg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/android-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.21.5.tgz",
      "integrity": "sha512-c0uX9VAUBQ7dTDCjq+wdyGLowMdtR/GoC2U5IYk/7D1H1JYC0qseD7+11iMP2mRLN9RcCMRcjC4YMclCzGwS/A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/android-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.21.5.tgz",
      "integrity": "sha512-D7aPRUUNHRBwHxzxRvp856rjUHRFW1SdQATKXH2hqA0kAZb1hKmi02OpYRacl0TxIGz/ZmXWlbZgjwWYaCakTA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/darwin-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.21.5.tgz",
      "integrity": "sha512-DwqXqZyuk5AiWWf3UfLiRDJ5EDd49zg6O9wclZ7kUMv2WRFr4HKjXp/5t8JZ11QbQfUS6/cRCKGwYhtNAY88kQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/darwin-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.21.5.tgz",
      "integrity": "sha512-se/JjF8NlmKVG4kNIuyWMV/22ZaerB+qaSi5MdrXtd6R08kvs2qCN4C09miupktDitvh8jRFflwGFBQcxZRjbw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/freebsd-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.21.5.tgz",
      "integrity": "sha512-5JcRxxRDUJLX8JXp/wcBCy3pENnCgBR9bN6JsY4OmhfUtIHe3ZW0mawA7+RDAcMLrMIZaf03NlQiX9DGyB8h4g==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/freebsd-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.21.5.tgz",
      "integrity": "sha512-J95kNBj1zkbMXtHVH29bBriQygMXqoVQOQYA+ISs0/2l3T9/kj42ow2mpqerRBxDJnmkUDCaQT/dfNXWX/ZZCQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-arm": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.21.5.tgz",
      "integrity": "sha512-bPb5AHZtbeNGjCKVZ9UGqGwo8EUu4cLq68E95A53KlxAPRmUyYv2D6F0uUI65XisGOL1hBP5mTronbgo+0bFcA==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.21.5.tgz",
      "integrity": "sha512-ibKvmyYzKsBeX8d8I7MH/TMfWDXBF3db4qM6sy+7re0YXya+K1cem3on9XgdT2EQGMu4hQyZhan7TeQ8XkGp4Q==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-ia32": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.21.5.tgz",
      "integrity": "sha512-YvjXDqLRqPDl2dvRODYmmhz4rPeVKYvppfGYKSNGdyZkA01046pLWyRKKI3ax8fbJoK5QbxblURkwK/MWY18Tg==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-loong64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.21.5.tgz",
      "integrity": "sha512-uHf1BmMG8qEvzdrzAqg2SIG/02+4/DHB6a9Kbya0XDvwDEKCoC8ZRWI5JJvNdUjtciBGFQ5PuBlpEOXQj+JQSg==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-mips64el": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.21.5.tgz",
      "integrity": "sha512-IajOmO+KJK23bj52dFSNCMsz1QP1DqM6cwLUv3W1QwyxkyIWecfafnI555fvSGqEKwjMXVLokcV5ygHW5b3Jbg==",
      "cpu": [
        "mips64el"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-ppc64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.21.5.tgz",
      "integrity": "sha512-1hHV/Z4OEfMwpLO8rp7CvlhBDnjsC3CttJXIhBi+5Aj5r+MBvy4egg7wCbe//hSsT+RvDAG7s81tAvpL2XAE4w==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-riscv64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.21.5.tgz",
      "integrity": "sha512-2HdXDMd9GMgTGrPWnJzP2ALSokE/0O5HhTUvWIbD3YdjME8JwvSCnNGBnTThKGEB91OZhzrJ4qIIxk/SBmyDDA==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-s390x": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.21.5.tgz",
      "integrity": "sha512-zus5sxzqBJD3eXxwvjN1yQkRepANgxE9lgOW2qLnmr8ikMTphkjgXu1HR01K4FJg8h1kEEDAqDcZQtbrRnB41A==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/linux-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.21.5.tgz",
      "integrity": "sha512-1rYdTpyv03iycF1+BhzrzQJCdOuAOtaqHTWJZCWvijKD2N5Xu0TtVC8/+1faWqcP9iBCWOmjmhoH94dH82BxPQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/netbsd-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.21.5.tgz",
      "integrity": "sha512-Woi2MXzXjMULccIwMnLciyZH4nCIMpWQAs049KEeMvOcNADVxo0UBIQPfSmxB3CWKedngg7sWZdLvLczpe0tLg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/openbsd-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.21.5.tgz",
      "integrity": "sha512-HLNNw99xsvx12lFBUwoT8EVCsSvRNDVxNpjZ7bPn947b8gJPzeHWyNVhFsaerc0n3TsbOINvRP2byTZ5LKezow==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/sunos-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.21.5.tgz",
      "integrity": "sha512-6+gjmFpfy0BHU5Tpptkuh8+uw3mnrvgs+dSPQXQOv3ekbordwnzTVEb4qnIvQcYXq6gzkyTnoZ9dZG+D4garKg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "sunos"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/win32-arm64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.21.5.tgz",
      "integrity": "sha512-Z0gOTd75VvXqyq7nsl93zwahcTROgqvuAcYDUr+vOv8uHhNSKROyU961kgtCD1e95IqPKSQKH7tBTslnS3tA8A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/win32-ia32": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.21.5.tgz",
      "integrity": "sha512-SWXFF1CL2RVNMaVs+BBClwtfZSvDgtL//G/smwAc5oVK/UPu2Gu9tIaRgFmYFFKrmg3SyAjSrElf0TiJ1v8fYA==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@esbuild/win32-x64": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.21.5.tgz",
      "integrity": "sha512-tQd/1efJuzPC6rCFwEvLtci/xNFcTZknmXs98FYDfGE4wP9ClFV98nyKrzJKVPMhdDnjzLhdUyMX4PsQAPjwIw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@jridgewell/gen-mapping": {
      "version": "0.3.13",
      "resolved": "https://registry.npmjs.org/@jridgewell/gen-mapping/-/gen-mapping-0.3.13.tgz",
      "integrity": "sha512-2kkt/7niJ6MgEPxF0bYdQ6etZaA+fQvDcLKckhy1yIQOzaoKjBBjSj63/aLVjYE3qhRt5dvM+uUyfCg6UKCBbA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/sourcemap-codec": "^1.5.0",
        "@jridgewell/trace-mapping": "^0.3.24"
      }
    },
    "node_modules/@jridgewell/remapping": {
      "version": "2.3.5",
      "resolved": "https://registry.npmjs.org/@jridgewell/remapping/-/remapping-2.3.5.tgz",
      "integrity": "sha512-LI9u/+laYG4Ds1TDKSJW2YPrIlcVYOwi2fUC6xB43lueCjgxV4lffOCZCtYFiH6TNOX+tQKXx97T4IKHbhyHEQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/gen-mapping": "^0.3.5",
        "@jridgewell/trace-mapping": "^0.3.24"
      }
    },
    "node_modules/@jridgewell/resolve-uri": {
      "version": "3.1.2",
      "resolved": "https://registry.npmjs.org/@jridgewell/resolve-uri/-/resolve-uri-3.1.2.tgz",
      "integrity": "sha512-bRISgCIjP20/tbWSPWMEi54QVPRZExkuD9lJL+UIxUKtwVJA8wW1Trb1jMs1RFXo1CBTNZ/5hpC9QvmKWdopKw==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.0.0"
      }
    },
    "node_modules/@jridgewell/sourcemap-codec": {
      "version": "1.5.5",
      "resolved": "https://registry.npmjs.org/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.5.tgz",
      "integrity": "sha512-cYQ9310grqxueWbl+WuIUIaiUaDcj7WOq5fVhEljNVgRfOUhY9fy2zTvfoqWsnebh8Sl70VScFbICvJnLKB0Og==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@jridgewell/trace-mapping": {
      "version": "0.3.31",
      "resolved": "https://registry.npmjs.org/@jridgewell/trace-mapping/-/trace-mapping-0.3.31.tgz",
      "integrity": "sha512-zzNR+SdQSDJzc8joaeP8QQoCQr8NuYx2dIIytl1QeBEZHJ9uW6hebsrYgbz8hJwUQao3TWCMtmfV8Nu1twOLAw==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@jridgewell/resolve-uri": "^3.1.0",
        "@jridgewell/sourcemap-codec": "^1.4.14"
      }
    },
    "node_modules/@rolldown/pluginutils": {
      "version": "1.0.0-beta.27",
      "resolved": "https://registry.npmjs.org/@rolldown/pluginutils/-/pluginutils-1.0.0-beta.27.tgz",
      "integrity": "sha512-+d0F4MKMCbeVUJwG96uQ4SgAznZNSq93I3V+9NHA4OpvqG8mRCpGdKmK8l/dl02h2CCDHwW2FqilnTyDcAnqjA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@rollup/rollup-android-arm-eabi": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm-eabi/-/rollup-android-arm-eabi-4.56.0.tgz",
      "integrity": "sha512-LNKIPA5k8PF1+jAFomGe3qN3bbIgJe/IlpDBwuVjrDKrJhVWywgnJvflMt/zkbVNLFtF1+94SljYQS6e99klnw==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ]
    },
    "node_modules/@rollup/rollup-android-arm64": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm64/-/rollup-android-arm64-4.56.0.tgz",
      "integrity": "sha512-lfbVUbelYqXlYiU/HApNMJzT1E87UPGvzveGg2h0ktUNlOCxKlWuJ9jtfvs1sKHdwU4fzY7Pl8sAl49/XaEk6Q==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ]
    },
    "node_modules/@rollup/rollup-darwin-arm64": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.56.0.tgz",
      "integrity": "sha512-EgxD1ocWfhoD6xSOeEEwyE7tDvwTgZc8Bss7wCWe+uc7wO8G34HHCUH+Q6cHqJubxIAnQzAsyUsClt0yFLu06w==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ]
    },
    "node_modules/@rollup/rollup-darwin-x64": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-x64/-/rollup-darwin-x64-4.56.0.tgz",
      "integrity": "sha512-1vXe1vcMOssb/hOF8iv52A7feWW2xnu+c8BV4t1F//m9QVLTfNVpEdja5ia762j/UEJe2Z1jAmEqZAK42tVW3g==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ]
    },
    "node_modules/@rollup/rollup-freebsd-arm64": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-arm64/-/rollup-freebsd-arm64-4.56.0.tgz",
      "integrity": "sha512-bof7fbIlvqsyv/DtaXSck4VYQ9lPtoWNFCB/JY4snlFuJREXfZnm+Ej6yaCHfQvofJDXLDMTVxWscVSuQvVWUQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ]
    },
    "node_modules/@rollup/rollup-freebsd-x64": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-x64/-/rollup-freebsd-x64-4.56.0.tgz",
      "integrity": "sha512-KNa6lYHloW+7lTEkYGa37fpvPq+NKG/EHKM8+G/g9WDU7ls4sMqbVRV78J6LdNuVaeeK5WB9/9VAFbKxcbXKYg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm-gnueabihf": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-gnueabihf/-/rollup-linux-arm-gnueabihf-4.56.0.tgz",
      "integrity": "sha512-E8jKK87uOvLrrLN28jnAAAChNq5LeCd2mGgZF+fGF5D507WlG/Noct3lP/QzQ6MrqJ5BCKNwI9ipADB6jyiq2A==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm-musleabihf": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-musleabihf/-/rollup-linux-arm-musleabihf-4.56.0.tgz",
      "integrity": "sha512-jQosa5FMYF5Z6prEpTCCmzCXz6eKr/tCBssSmQGEeozA9tkRUty/5Vx06ibaOP9RCrW1Pvb8yp3gvZhHwTDsJw==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm64-gnu": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-gnu/-/rollup-linux-arm64-gnu-4.56.0.tgz",
      "integrity": "sha512-uQVoKkrC1KGEV6udrdVahASIsaF8h7iLG0U0W+Xn14ucFwi6uS539PsAr24IEF9/FoDtzMeeJXJIBo5RkbNWvQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-arm64-musl": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-musl/-/rollup-linux-arm64-musl-4.56.0.tgz",
      "integrity": "sha512-vLZ1yJKLxhQLFKTs42RwTwa6zkGln+bnXc8ueFGMYmBTLfNu58sl5/eXyxRa2RarTkJbXl8TKPgfS6V5ijNqEA==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-loong64-gnu": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-loong64-gnu/-/rollup-linux-loong64-gnu-4.56.0.tgz",
      "integrity": "sha512-FWfHOCub564kSE3xJQLLIC/hbKqHSVxy8vY75/YHHzWvbJL7aYJkdgwD/xGfUlL5UV2SB7otapLrcCj2xnF1dg==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-loong64-musl": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-loong64-musl/-/rollup-linux-loong64-musl-4.56.0.tgz",
      "integrity": "sha512-z1EkujxIh7nbrKL1lmIpqFTc/sr0u8Uk0zK/qIEFldbt6EDKWFk/pxFq3gYj4Bjn3aa9eEhYRlL3H8ZbPT1xvA==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-ppc64-gnu": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-ppc64-gnu/-/rollup-linux-ppc64-gnu-4.56.0.tgz",
      "integrity": "sha512-iNFTluqgdoQC7AIE8Q34R3AuPrJGJirj5wMUErxj22deOcY7XwZRaqYmB6ZKFHoVGqRcRd0mqO+845jAibKCkw==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-ppc64-musl": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-ppc64-musl/-/rollup-linux-ppc64-musl-4.56.0.tgz",
      "integrity": "sha512-MtMeFVlD2LIKjp2sE2xM2slq3Zxf9zwVuw0jemsxvh1QOpHSsSzfNOTH9uYW9i1MXFxUSMmLpeVeUzoNOKBaWg==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-riscv64-gnu": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-gnu/-/rollup-linux-riscv64-gnu-4.56.0.tgz",
      "integrity": "sha512-in+v6wiHdzzVhYKXIk5U74dEZHdKN9KH0Q4ANHOTvyXPG41bajYRsy7a8TPKbYPl34hU7PP7hMVHRvv/5aCSew==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-riscv64-musl": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-musl/-/rollup-linux-riscv64-musl-4.56.0.tgz",
      "integrity": "sha512-yni2raKHB8m9NQpI9fPVwN754mn6dHQSbDTwxdr9SE0ks38DTjLMMBjrwvB5+mXrX+C0npX0CVeCUcvvvD8CNQ==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-s390x-gnu": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-s390x-gnu/-/rollup-linux-s390x-gnu-4.56.0.tgz",
      "integrity": "sha512-zhLLJx9nQPu7wezbxt2ut+CI4YlXi68ndEve16tPc/iwoylWS9B3FxpLS2PkmfYgDQtosah07Mj9E0khc3Y+vQ==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-x64-gnu": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-gnu/-/rollup-linux-x64-gnu-4.56.0.tgz",
      "integrity": "sha512-MVC6UDp16ZSH7x4rtuJPAEoE1RwS8N4oK9DLHy3FTEdFoUTCFVzMfJl/BVJ330C+hx8FfprA5Wqx4FhZXkj2Kw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-linux-x64-musl": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-musl/-/rollup-linux-x64-musl-4.56.0.tgz",
      "integrity": "sha512-ZhGH1eA4Qv0lxaV00azCIS1ChedK0V32952Md3FtnxSqZTBTd6tgil4nZT5cU8B+SIw3PFYkvyR4FKo2oyZIHA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ]
    },
    "node_modules/@rollup/rollup-openbsd-x64": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-openbsd-x64/-/rollup-openbsd-x64-4.56.0.tgz",
      "integrity": "sha512-O16XcmyDeFI9879pEcmtWvD/2nyxR9mF7Gs44lf1vGGx8Vg2DRNx11aVXBEqOQhWb92WN4z7fW/q4+2NYzCbBA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ]
    },
    "node_modules/@rollup/rollup-openharmony-arm64": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-openharmony-arm64/-/rollup-openharmony-arm64-4.56.0.tgz",
      "integrity": "sha512-LhN/Reh+7F3RCgQIRbgw8ZMwUwyqJM+8pXNT6IIJAqm2IdKkzpCh/V9EdgOMBKuebIrzswqy4ATlrDgiOwbRcQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openharmony"
      ]
    },
    "node_modules/@rollup/rollup-win32-arm64-msvc": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-arm64-msvc/-/rollup-win32-arm64-msvc-4.56.0.tgz",
      "integrity": "sha512-kbFsOObXp3LBULg1d3JIUQMa9Kv4UitDmpS+k0tinPBz3watcUiV2/LUDMMucA6pZO3WGE27P7DsfaN54l9ing==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-ia32-msvc": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-ia32-msvc/-/rollup-win32-ia32-msvc-4.56.0.tgz",
      "integrity": "sha512-vSSgny54D6P4vf2izbtFm/TcWYedw7f8eBrOiGGecyHyQB9q4Kqentjaj8hToe+995nob/Wv48pDqL5a62EWtg==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-x64-gnu": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-x64-gnu/-/rollup-win32-x64-gnu-4.56.0.tgz",
      "integrity": "sha512-FeCnkPCTHQJFbiGG49KjV5YGW/8b9rrXAM2Mz2kiIoktq2qsJxRD5giEMEOD2lPdgs72upzefaUvS+nc8E3UzQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@rollup/rollup-win32-x64-msvc": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-x64-msvc/-/rollup-win32-x64-msvc-4.56.0.tgz",
      "integrity": "sha512-H8AE9Ur/t0+1VXujj90w0HrSOuv0Nq9r1vSZF2t5km20NTfosQsGGUXDaKdQZzwuLts7IyL1fYT4hM95TI9c4g==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ]
    },
    "node_modules/@types/babel__core": {
      "version": "7.20.5",
      "resolved": "https://registry.npmjs.org/@types/babel__core/-/babel__core-7.20.5.tgz",
      "integrity": "sha512-qoQprZvz5wQFJwMDqeseRXWv3rqMvhgpbXFfVyWhbx9X47POIA6i/+dXefEmZKoAgOaTdaIgNSMqMIU61yRyzA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.20.7",
        "@babel/types": "^7.20.7",
        "@types/babel__generator": "*",
        "@types/babel__template": "*",
        "@types/babel__traverse": "*"
      }
    },
    "node_modules/@types/babel__generator": {
      "version": "7.27.0",
      "resolved": "https://registry.npmjs.org/@types/babel__generator/-/babel__generator-7.27.0.tgz",
      "integrity": "sha512-ufFd2Xi92OAVPYsy+P4n7/U7e68fex0+Ee8gSG9KX7eo084CWiQ4sdxktvdl0bOPupXtVJPY19zk6EwWqUQ8lg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.0.0"
      }
    },
    "node_modules/@types/babel__template": {
      "version": "7.4.4",
      "resolved": "https://registry.npmjs.org/@types/babel__template/-/babel__template-7.4.4.tgz",
      "integrity": "sha512-h/NUaSyG5EyxBIp8YRxo4RMe2/qQgvyowRwVMzhYhBCONbW8PUsg4lkFMrhgZhUe5z3L3MiLDuvyJ/CaPa2A8A==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/parser": "^7.1.0",
        "@babel/types": "^7.0.0"
      }
    },
    "node_modules/@types/babel__traverse": {
      "version": "7.28.0",
      "resolved": "https://registry.npmjs.org/@types/babel__traverse/-/babel__traverse-7.28.0.tgz",
      "integrity": "sha512-8PvcXf70gTDZBgt9ptxJ8elBeBjcLOAcOtoO/mPJjtji1+CdGbHgm77om1GrsPxsiE+uXIpNSK64UYaIwQXd4Q==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/types": "^7.28.2"
      }
    },
    "node_modules/@types/estree": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/@types/estree/-/estree-1.0.8.tgz",
      "integrity": "sha512-dWHzHa2WqEXI/O1E9OjrocMTKJl2mSrEolh1Iomrv6U+JuNwaHXsXx9bLu5gG7BUWFIN0skIQJQ/L1rIex4X6w==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/@vitejs/plugin-react": {
      "version": "4.7.0",
      "resolved": "https://registry.npmjs.org/@vitejs/plugin-react/-/plugin-react-4.7.0.tgz",
      "integrity": "sha512-gUu9hwfWvvEDBBmgtAowQCojwZmJ5mcLn3aufeCsitijs3+f2NsrPtlAWIR6OPiqljl96GVCUbLe0HyqIpVaoA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@babel/core": "^7.28.0",
        "@babel/plugin-transform-react-jsx-self": "^7.27.1",
        "@babel/plugin-transform-react-jsx-source": "^7.27.1",
        "@rolldown/pluginutils": "1.0.0-beta.27",
        "@types/babel__core": "^7.20.5",
        "react-refresh": "^0.17.0"
      },
      "engines": {
        "node": "^14.18.0 || >=16.0.0"
      },
      "peerDependencies": {
        "vite": "^4.2.0 || ^5.0.0 || ^6.0.0 || ^7.0.0"
      }
    },
    "node_modules/asynckit": {
      "version": "0.4.0",
      "resolved": "https://registry.npmjs.org/asynckit/-/asynckit-0.4.0.tgz",
      "integrity": "sha512-Oei9OH4tRh0YqU3GxhX79dM/mwVgvbZJaSNaRk+bshkj0S5cfHcgYakreBjrHwatXKbz+IoIdYLxrKim2MjW0Q==",
      "license": "MIT"
    },
    "node_modules/axios": {
      "version": "1.13.2",
      "resolved": "https://registry.npmjs.org/axios/-/axios-1.13.2.tgz",
      "integrity": "sha512-VPk9ebNqPcy5lRGuSlKx752IlDatOjT9paPlm8A7yOuW2Fbvp4X3JznJtT4f0GzGLLiWE9W8onz51SqLYwzGaA==",
      "license": "MIT",
      "dependencies": {
        "follow-redirects": "^1.15.6",
        "form-data": "^4.0.4",
        "proxy-from-env": "^1.1.0"
      }
    },
    "node_modules/baseline-browser-mapping": {
      "version": "2.9.17",
      "resolved": "https://registry.npmjs.org/baseline-browser-mapping/-/baseline-browser-mapping-2.9.17.tgz",
      "integrity": "sha512-agD0MgJFUP/4nvjqzIB29zRPUuCF7Ge6mEv9s8dHrtYD7QWXRcx75rOADE/d5ah1NI+0vkDl0yorDd5U852IQQ==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "baseline-browser-mapping": "dist/cli.js"
      }
    },
    "node_modules/browserslist": {
      "version": "4.28.1",
      "resolved": "https://registry.npmjs.org/browserslist/-/browserslist-4.28.1.tgz",
      "integrity": "sha512-ZC5Bd0LgJXgwGqUknZY/vkUQ04r8NXnJZ3yYi4vDmSiZmC/pdSN0NbNRPxZpbtO4uAfDUAFffO8IZoM3Gj8IkA==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/browserslist"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "baseline-browser-mapping": "^2.9.0",
        "caniuse-lite": "^1.0.30001759",
        "electron-to-chromium": "^1.5.263",
        "node-releases": "^2.0.27",
        "update-browserslist-db": "^1.2.0"
      },
      "bin": {
        "browserslist": "cli.js"
      },
      "engines": {
        "node": "^6 || ^7 || ^8 || ^9 || ^10 || ^11 || ^12 || >=13.7"
      }
    },
    "node_modules/call-bind-apply-helpers": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz",
      "integrity": "sha512-Sp1ablJ0ivDkSzjcaJdxEunN5/XvksFJ2sMBFfq6x0ryhQV/2b/KwFe21cMpmHtPOSij8K99/wSfoEuTObmuMQ==",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0",
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/caniuse-lite": {
      "version": "1.0.30001765",
      "resolved": "https://registry.npmjs.org/caniuse-lite/-/caniuse-lite-1.0.30001765.tgz",
      "integrity": "sha512-LWcNtSyZrakjECqmpP4qdg0MMGdN368D7X8XvvAqOcqMv0RxnlqVKZl2V6/mBR68oYMxOZPLw/gO7DuisMHUvQ==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/caniuse-lite"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "CC-BY-4.0"
    },
    "node_modules/combined-stream": {
      "version": "1.0.8",
      "resolved": "https://registry.npmjs.org/combined-stream/-/combined-stream-1.0.8.tgz",
      "integrity": "sha512-FQN4MRfuJeHf7cBbBMJFXhKSDq+2kAArBlmRBvcvFE5BB1HZKXtSFASDhdlz9zOYwxh8lDdnvmMOe/+5cdoEdg==",
      "license": "MIT",
      "dependencies": {
        "delayed-stream": "~1.0.0"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/convert-source-map": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-2.0.0.tgz",
      "integrity": "sha512-Kvp459HrV2FEJ1CAsi1Ku+MY3kasH19TFykTz2xWmMeq6bk2NU3XXvfJ+Q61m0xktWwt+1HSYf3JZsTms3aRJg==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/debug": {
      "version": "4.4.3",
      "resolved": "https://registry.npmjs.org/debug/-/debug-4.4.3.tgz",
      "integrity": "sha512-RGwwWnwQvkVfavKVt22FGLw+xYSdzARwm0ru6DhTVA3umU5hZc28V3kO4stgYryrTlLpuvgI9GiijltAjNbcqA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "ms": "^2.1.3"
      },
      "engines": {
        "node": ">=6.0"
      },
      "peerDependenciesMeta": {
        "supports-color": {
          "optional": true
        }
      }
    },
    "node_modules/delayed-stream": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/delayed-stream/-/delayed-stream-1.0.0.tgz",
      "integrity": "sha512-ZySD7Nf91aLB0RxL4KGrKHBXl7Eds1DAmEdcoVawXnLD7SDhpNgtuII2aAkg7a7QS41jxPSZ17p4VdGnMHk3MQ==",
      "license": "MIT",
      "engines": {
        "node": ">=0.4.0"
      }
    },
    "node_modules/dunder-proto": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/dunder-proto/-/dunder-proto-1.0.1.tgz",
      "integrity": "sha512-KIN/nDJBQRcXw0MLVhZE9iQHmG68qAVIBg9CqmUYjmQIhgij9U5MFvrqkUL5FbtyyzZuOeOt0zdeRe4UY7ct+A==",
      "license": "MIT",
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.1",
        "es-errors": "^1.3.0",
        "gopd": "^1.2.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/electron-to-chromium": {
      "version": "1.5.277",
      "resolved": "https://registry.npmjs.org/electron-to-chromium/-/electron-to-chromium-1.5.277.tgz",
      "integrity": "sha512-wKXFZw4erWmmOz5N/grBoJ2XrNJGDFMu2+W5ACHza5rHtvsqrK4gb6rnLC7XxKB9WlJ+RmyQatuEXmtm86xbnw==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/es-define-property": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/es-define-property/-/es-define-property-1.0.1.tgz",
      "integrity": "sha512-e3nRfgfUZ4rNGL232gUgX06QNyyez04KdjFrF+LTRoOXmrOgFKDg4BCdsjW8EnT69eqdYGmRpJwiPVYNrCaW3g==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-errors": {
      "version": "1.3.0",
      "resolved": "https://registry.npmjs.org/es-errors/-/es-errors-1.3.0.tgz",
      "integrity": "sha512-Zf5H2Kxt2xjTvbJvP2ZWLEICxA6j+hAmMzIlypy4xcBg1vKVnx89Wy0GbS+kf5cwCVFFzdCFh2XSCFNULS6csw==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-object-atoms": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/es-object-atoms/-/es-object-atoms-1.1.1.tgz",
      "integrity": "sha512-FGgH2h8zKNim9ljj7dankFPcICIK9Cp5bm+c2gQSYePhpaG5+esrLODihIorn+Pe6FGJzWhXQotPv73jTaldXA==",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-set-tostringtag": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/es-set-tostringtag/-/es-set-tostringtag-2.1.0.tgz",
      "integrity": "sha512-j6vWzfrGVfyXxge+O0x5sh6cvxAog0a/4Rdd2K36zCMV5eJ+/+tOAngRO8cODMNWbVRdVlmGZQL2YS3yR8bIUA==",
      "license": "MIT",
      "dependencies": {
        "es-errors": "^1.3.0",
        "get-intrinsic": "^1.2.6",
        "has-tostringtag": "^1.0.2",
        "hasown": "^2.0.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/esbuild": {
      "version": "0.21.5",
      "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.21.5.tgz",
      "integrity": "sha512-mg3OPMV4hXywwpoDxu3Qda5xCKQi+vCTZq8S9J/EpkhB2HzKXq4SNFZE3+NK93JYxc8VMSep+lOUSC/RVKaBqw==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "bin": {
        "esbuild": "bin/esbuild"
      },
      "engines": {
        "node": ">=12"
      },
      "optionalDependencies": {
        "@esbuild/aix-ppc64": "0.21.5",
        "@esbuild/android-arm": "0.21.5",
        "@esbuild/android-arm64": "0.21.5",
        "@esbuild/android-x64": "0.21.5",
        "@esbuild/darwin-arm64": "0.21.5",
        "@esbuild/darwin-x64": "0.21.5",
        "@esbuild/freebsd-arm64": "0.21.5",
        "@esbuild/freebsd-x64": "0.21.5",
        "@esbuild/linux-arm": "0.21.5",
        "@esbuild/linux-arm64": "0.21.5",
        "@esbuild/linux-ia32": "0.21.5",
        "@esbuild/linux-loong64": "0.21.5",
        "@esbuild/linux-mips64el": "0.21.5",
        "@esbuild/linux-ppc64": "0.21.5",
        "@esbuild/linux-riscv64": "0.21.5",
        "@esbuild/linux-s390x": "0.21.5",
        "@esbuild/linux-x64": "0.21.5",
        "@esbuild/netbsd-x64": "0.21.5",
        "@esbuild/openbsd-x64": "0.21.5",
        "@esbuild/sunos-x64": "0.21.5",
        "@esbuild/win32-arm64": "0.21.5",
        "@esbuild/win32-ia32": "0.21.5",
        "@esbuild/win32-x64": "0.21.5"
      }
    },
    "node_modules/escalade": {
      "version": "3.2.0",
      "resolved": "https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz",
      "integrity": "sha512-WUj2qlxaQtO4g6Pq5c29GTcWGDyd8itL8zTlipgECz3JesAiiOKotd8JU6otB3PACgG6xkJUyVhboMS+bje/jA==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/follow-redirects": {
      "version": "1.15.11",
      "resolved": "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.15.11.tgz",
      "integrity": "sha512-deG2P0JfjrTxl50XGCDyfI97ZGVCxIpfKYmfyrQ54n5FO/0gfIES8C/Psl6kWVDolizcaaxZJnTS0QSMxvnsBQ==",
      "funding": [
        {
          "type": "individual",
          "url": "https://github.com/sponsors/RubenVerborgh"
        }
      ],
      "license": "MIT",
      "engines": {
        "node": ">=4.0"
      },
      "peerDependenciesMeta": {
        "debug": {
          "optional": true
        }
      }
    },
    "node_modules/form-data": {
      "version": "4.0.5",
      "resolved": "https://registry.npmjs.org/form-data/-/form-data-4.0.5.tgz",
      "integrity": "sha512-8RipRLol37bNs2bhoV67fiTEvdTrbMUYcFTiy3+wuuOnUog2QBHCZWXDRijWQfAkhBj2Uf5UnVaiWwA5vdd82w==",
      "license": "MIT",
      "dependencies": {
        "asynckit": "^0.4.0",
        "combined-stream": "^1.0.8",
        "es-set-tostringtag": "^2.1.0",
        "hasown": "^2.0.2",
        "mime-types": "^2.1.12"
      },
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/fsevents": {
      "version": "2.3.3",
      "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz",
      "integrity": "sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": "^8.16.0 || ^10.6.0 || >=11.0.0"
      }
    },
    "node_modules/function-bind": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz",
      "integrity": "sha512-7XHNxH7qX9xG5mIwxkhumTox/MIRNcOgDrxWsMt2pAr23WHp6MrRlN7FBSFpCpr+oVO0F744iUgR82nJMfG2SA==",
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/gensync": {
      "version": "1.0.0-beta.2",
      "resolved": "https://registry.npmjs.org/gensync/-/gensync-1.0.0-beta.2.tgz",
      "integrity": "sha512-3hN7NaskYvMDLQY55gnW3NQ+mesEAepTqlg+VEbj7zzqEMBVNhzcGYYeqFo/TlYz6eQiFcp1HcsCZO+nGgS8zg==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=6.9.0"
      }
    },
    "node_modules/get-intrinsic": {
      "version": "1.3.0",
      "resolved": "https://registry.npmjs.org/get-intrinsic/-/get-intrinsic-1.3.0.tgz",
      "integrity": "sha512-9fSjSaos/fRIVIp+xSJlE6lfwhES7LNtKaCBIamHsjr2na1BiABJPo0mOjjz8GJDURarmCPGqaiVg5mfjb98CQ==",
      "license": "MIT",
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.2",
        "es-define-property": "^1.0.1",
        "es-errors": "^1.3.0",
        "es-object-atoms": "^1.1.1",
        "function-bind": "^1.1.2",
        "get-proto": "^1.0.1",
        "gopd": "^1.2.0",
        "has-symbols": "^1.1.0",
        "hasown": "^2.0.2",
        "math-intrinsics": "^1.1.0"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-proto": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/get-proto/-/get-proto-1.0.1.tgz",
      "integrity": "sha512-sTSfBjoXBp89JvIKIefqw7U2CCebsc74kiY6awiGogKtoSGbgjYE/G/+l9sF3MWFPNc9IcoOC4ODfKHfxFmp0g==",
      "license": "MIT",
      "dependencies": {
        "dunder-proto": "^1.0.1",
        "es-object-atoms": "^1.0.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/gopd": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/gopd/-/gopd-1.2.0.tgz",
      "integrity": "sha512-ZUKRh6/kUFoAiTAtTYPZJ3hw9wNxx+BIBOijnlG9PnrJsCcSjs1wyyD6vJpaYtgnzDrKYRSqf3OO6Rfa93xsRg==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-symbols": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz",
      "integrity": "sha512-1cDNdwJ2Jaohmb3sg4OmKaMBwuC48sYni5HUw2DvsC8LjGTLK9h+eb1X6RyuOHe4hT0ULCW68iomhjUoKUqlPQ==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-tostringtag": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/has-tostringtag/-/has-tostringtag-1.0.2.tgz",
      "integrity": "sha512-NqADB8VjPFLM2V0VvHUewwwsw0ZWBaIdgo+ieHtK3hasLz4qeCRjYcqfB6AQrBggRKppKF8L52/VqdVsO47Dlw==",
      "license": "MIT",
      "dependencies": {
        "has-symbols": "^1.0.3"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/hasown": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz",
      "integrity": "sha512-0hJU9SCPvmMzIBdZFqNPXWa6dqh7WdH0cII9y+CyS8rG3nL48Bclra9HmKhVVUHyPWNH5Y7xDwAB7bfgSjkUMQ==",
      "license": "MIT",
      "dependencies": {
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/js-tokens": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz",
      "integrity": "sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==",
      "license": "MIT"
    },
    "node_modules/jsesc": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/jsesc/-/jsesc-3.1.0.tgz",
      "integrity": "sha512-/sM3dO2FOzXjKQhJuo0Q173wf2KOo8t4I8vHy6lF9poUp7bKT0/NHE8fPX23PwfhnykfqnC2xRxOnVw5XuGIaA==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "jsesc": "bin/jsesc"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/json5": {
      "version": "2.2.3",
      "resolved": "https://registry.npmjs.org/json5/-/json5-2.2.3.tgz",
      "integrity": "sha512-XmOWe7eyHYH14cLdVPoyg+GOH3rYX++KpzrylJwSW98t3Nk+U8XOl8FWKOgwtzdb8lXGf6zYwDUzeHMWfxasyg==",
      "dev": true,
      "license": "MIT",
      "bin": {
        "json5": "lib/cli.js"
      },
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/loose-envify": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/loose-envify/-/loose-envify-1.4.0.tgz",
      "integrity": "sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==",
      "license": "MIT",
      "dependencies": {
        "js-tokens": "^3.0.0 || ^4.0.0"
      },
      "bin": {
        "loose-envify": "cli.js"
      }
    },
    "node_modules/lru-cache": {
      "version": "5.1.1",
      "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-5.1.1.tgz",
      "integrity": "sha512-KpNARQA3Iwv+jTA0utUVVbrh+Jlrr1Fv0e56GGzAFOXN7dk/FviaDW8LHmK52DlcH4WP2n6gI8vN1aesBFgo9w==",
      "dev": true,
      "license": "ISC",
      "dependencies": {
        "yallist": "^3.0.2"
      }
    },
    "node_modules/math-intrinsics": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/math-intrinsics/-/math-intrinsics-1.1.0.tgz",
      "integrity": "sha512-/IXtbwEk5HTPyEwyKX6hGkYXxM9nbj64B+ilVJnC/R6B0pH5G4V3b0pVbL7DBj4tkhBAppbQUlf6F6Xl9LHu1g==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/mime-db": {
      "version": "1.52.0",
      "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.52.0.tgz",
      "integrity": "sha512-sPU4uV7dYlvtWJxwwxHD0PuihVNiE7TyAbQ5SWxDCB9mUYvOgroQOwYQQOKPJ8CIbE+1ETVlOoK1UC2nU3gYvg==",
      "license": "MIT",
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/mime-types": {
      "version": "2.1.35",
      "resolved": "https://registry.npmjs.org/mime-types/-/mime-types-2.1.35.tgz",
      "integrity": "sha512-ZDY+bPm5zTTF+YpCrAU9nK0UgICYPT0QtT1NZWFv4s++TNkcgVaT0g6+4R2uI4MjQjzysHB1zxuWL50hzaeXiw==",
      "license": "MIT",
      "dependencies": {
        "mime-db": "1.52.0"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/ms": {
      "version": "2.1.3",
      "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
      "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/nanoid": {
      "version": "3.3.11",
      "resolved": "https://registry.npmjs.org/nanoid/-/nanoid-3.3.11.tgz",
      "integrity": "sha512-N8SpfPUnUp1bK+PMYW8qSWdl9U+wwNWI4QKxOYDy9JAro3WMX7p2OeVRF9v+347pnakNevPmiHhNmZ2HbFA76w==",
      "dev": true,
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "bin": {
        "nanoid": "bin/nanoid.cjs"
      },
      "engines": {
        "node": "^10 || ^12 || ^13.7 || ^14 || >=15.0.1"
      }
    },
    "node_modules/node-releases": {
      "version": "2.0.27",
      "resolved": "https://registry.npmjs.org/node-releases/-/node-releases-2.0.27.tgz",
      "integrity": "sha512-nmh3lCkYZ3grZvqcCH+fjmQ7X+H0OeZgP40OierEaAptX4XofMh5kwNbWh7lBduUzCcV/8kZ+NDLCwm2iorIlA==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/picocolors": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/picocolors/-/picocolors-1.1.1.tgz",
      "integrity": "sha512-xceH2snhtb5M9liqDsmEw56le376mTZkEX/jEb/RxNFyegNul7eNslCXP9FDj/Lcu0X8KEyMceP2ntpaHrDEVA==",
      "dev": true,
      "license": "ISC"
    },
    "node_modules/postcss": {
      "version": "8.5.6",
      "resolved": "https://registry.npmjs.org/postcss/-/postcss-8.5.6.tgz",
      "integrity": "sha512-3Ybi1tAuwAP9s0r1UQ2J4n5Y0G05bJkpUIO0/bI9MhwmD70S5aTWbXGBwxHrelT+XM1k6dM0pk+SwNkpTRN7Pg==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/postcss/"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/postcss"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "nanoid": "^3.3.11",
        "picocolors": "^1.1.1",
        "source-map-js": "^1.2.1"
      },
      "engines": {
        "node": "^10 || ^12 || >=14"
      }
    },
    "node_modules/proxy-from-env": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/proxy-from-env/-/proxy-from-env-1.1.0.tgz",
      "integrity": "sha512-D+zkORCbA9f1tdWRK0RaCR3GPv50cMxcrz4X8k5LTSUD1Dkw47mKJEZQNunItRTkWwgtaUSo1RVFRIG9ZXiFYg==",
      "license": "MIT"
    },
    "node_modules/react": {
      "version": "18.3.1",
      "resolved": "https://registry.npmjs.org/react/-/react-18.3.1.tgz",
      "integrity": "sha512-wS+hAgJShR0KhEvPJArfuPVN1+Hz1t0Y6n5jLrGQbkb4urgPE/0Rve+1kMB1v/oWgHgm4WIcV+i7F2pTVj+2iQ==",
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "loose-envify": "^1.1.0"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/react-dom": {
      "version": "18.3.1",
      "resolved": "https://registry.npmjs.org/react-dom/-/react-dom-18.3.1.tgz",
      "integrity": "sha512-5m4nQKp+rZRb09LNH59GM4BxTh9251/ylbKIbpe7TpGxfJ+9kv6BLkLBXIjjspbgbnIBNqlI23tRnTWT0snUIw==",
      "license": "MIT",
      "dependencies": {
        "loose-envify": "^1.1.0",
        "scheduler": "^0.23.2"
      },
      "peerDependencies": {
        "react": "^18.3.1"
      }
    },
    "node_modules/react-refresh": {
      "version": "0.17.0",
      "resolved": "https://registry.npmjs.org/react-refresh/-/react-refresh-0.17.0.tgz",
      "integrity": "sha512-z6F7K9bV85EfseRCp2bzrpyQ0Gkw1uLoCel9XBVWPg/TjRj94SkJzUTGfOa4bs7iJvBWtQG0Wq7wnI0syw3EBQ==",
      "dev": true,
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/rollup": {
      "version": "4.56.0",
      "resolved": "https://registry.npmjs.org/rollup/-/rollup-4.56.0.tgz",
      "integrity": "sha512-9FwVqlgUHzbXtDg9RCMgodF3Ua4Na6Gau+Sdt9vyCN4RhHfVKX2DCHy3BjMLTDd47ITDhYAnTwGulWTblJSDLg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/estree": "1.0.8"
      },
      "bin": {
        "rollup": "dist/bin/rollup"
      },
      "engines": {
        "node": ">=18.0.0",
        "npm": ">=8.0.0"
      },
      "optionalDependencies": {
        "@rollup/rollup-android-arm-eabi": "4.56.0",
        "@rollup/rollup-android-arm64": "4.56.0",
        "@rollup/rollup-darwin-arm64": "4.56.0",
        "@rollup/rollup-darwin-x64": "4.56.0",
        "@rollup/rollup-freebsd-arm64": "4.56.0",
        "@rollup/rollup-freebsd-x64": "4.56.0",
        "@rollup/rollup-linux-arm-gnueabihf": "4.56.0",
        "@rollup/rollup-linux-arm-musleabihf": "4.56.0",
        "@rollup/rollup-linux-arm64-gnu": "4.56.0",
        "@rollup/rollup-linux-arm64-musl": "4.56.0",
        "@rollup/rollup-linux-loong64-gnu": "4.56.0",
        "@rollup/rollup-linux-loong64-musl": "4.56.0",
        "@rollup/rollup-linux-ppc64-gnu": "4.56.0",
        "@rollup/rollup-linux-ppc64-musl": "4.56.0",
        "@rollup/rollup-linux-riscv64-gnu": "4.56.0",
        "@rollup/rollup-linux-riscv64-musl": "4.56.0",
        "@rollup/rollup-linux-s390x-gnu": "4.56.0",
        "@rollup/rollup-linux-x64-gnu": "4.56.0",
        "@rollup/rollup-linux-x64-musl": "4.56.0",
        "@rollup/rollup-openbsd-x64": "4.56.0",
        "@rollup/rollup-openharmony-arm64": "4.56.0",
        "@rollup/rollup-win32-arm64-msvc": "4.56.0",
        "@rollup/rollup-win32-ia32-msvc": "4.56.0",
        "@rollup/rollup-win32-x64-gnu": "4.56.0",
        "@rollup/rollup-win32-x64-msvc": "4.56.0",
        "fsevents": "~2.3.2"
      }
    },
    "node_modules/scheduler": {
      "version": "0.23.2",
      "resolved": "https://registry.npmjs.org/scheduler/-/scheduler-0.23.2.tgz",
      "integrity": "sha512-UOShsPwz7NrMUqhR6t0hWjFduvOzbtv7toDH1/hIrfRNIDBnnBWd0CwJTGvTpngVlmwGCdP9/Zl/tVrDqcuYzQ==",
      "license": "MIT",
      "dependencies": {
        "loose-envify": "^1.1.0"
      }
    },
    "node_modules/semver": {
      "version": "6.3.1",
      "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.1.tgz",
      "integrity": "sha512-BR7VvDCVHO+q2xBEWskxS6DJE1qRnb7DxzUrogb71CWoSficBxYsiAGd+Kl0mmq/MprG9yArRkyrQxTO6XjMzA==",
      "dev": true,
      "license": "ISC",
      "bin": {
        "semver": "bin/semver.js"
      }
    },
    "node_modules/source-map-js": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/source-map-js/-/source-map-js-1.2.1.tgz",
      "integrity": "sha512-UXWMKhLOwVKb728IUtQPXxfYU+usdybtUrK/8uGE8CQMvrhOpwvzDBwj0QhSL7MQc7vIsISBG8VQ8+IDQxpfQA==",
      "dev": true,
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/update-browserslist-db": {
      "version": "1.2.3",
      "resolved": "https://registry.npmjs.org/update-browserslist-db/-/update-browserslist-db-1.2.3.tgz",
      "integrity": "sha512-Js0m9cx+qOgDxo0eMiFGEueWztz+d4+M3rGlmKPT+T4IS/jP4ylw3Nwpu6cpTTP8R1MAC1kF4VbdLt3ARf209w==",
      "dev": true,
      "funding": [
        {
          "type": "opencollective",
          "url": "https://opencollective.com/browserslist"
        },
        {
          "type": "tidelift",
          "url": "https://tidelift.com/funding/github/npm/browserslist"
        },
        {
          "type": "github",
          "url": "https://github.com/sponsors/ai"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "escalade": "^3.2.0",
        "picocolors": "^1.1.1"
      },
      "bin": {
        "update-browserslist-db": "cli.js"
      },
      "peerDependencies": {
        "browserslist": ">= 4.21.0"
      }
    },
    "node_modules/vite": {
      "version": "5.4.21",
      "resolved": "https://registry.npmjs.org/vite/-/vite-5.4.21.tgz",
      "integrity": "sha512-o5a9xKjbtuhY6Bi5S3+HvbRERmouabWbyUcpXXUA1u+GNUKoROi9byOJ8M0nHbHYHkYICiMlqxkg1KkYmm25Sw==",
      "dev": true,
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "esbuild": "^0.21.3",
        "postcss": "^8.4.43",
        "rollup": "^4.20.0"
      },
      "bin": {
        "vite": "bin/vite.js"
      },
      "engines": {
        "node": "^18.0.0 || >=20.0.0"
      },
      "funding": {
        "url": "https://github.com/vitejs/vite?sponsor=1"
      },
      "optionalDependencies": {
        "fsevents": "~2.3.3"
      },
      "peerDependencies": {
        "@types/node": "^18.0.0 || >=20.0.0",
        "less": "*",
        "lightningcss": "^1.21.0",
        "sass": "*",
        "sass-embedded": "*",
        "stylus": "*",
        "sugarss": "*",
        "terser": "^5.4.0"
      },
      "peerDependenciesMeta": {
        "@types/node": {
          "optional": true
        },
        "less": {
          "optional": true
        },
        "lightningcss": {
          "optional": true
        },
        "sass": {
          "optional": true
        },
        "sass-embedded": {
          "optional": true
        },
        "stylus": {
          "optional": true
        },
        "sugarss": {
          "optional": true
        },
        "terser": {
          "optional": true
        }
      }
    },
    "node_modules/yallist": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/yallist/-/yallist-3.1.1.tgz",
      "integrity": "sha512-a4UGQaWPH59mOXUYnAG2ewncQS4i4F43Tv3JoAM+s2VDAmS9NsK8GpDMLrCHPksFT7h3K6TOoUNn2pb7RoXx4g==",
      "dev": true,
      "license": "ISC"
    }
  }
}

==============================
FILE: .\frontend\package.json
==============================
{
  "name": "ai-trogiang-frontend",
  "private": true,
  "version": "0.1.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build"
  },
  "dependencies": {
    "axios": "^1.6.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0"
  },
  "devDependencies": {
    "@vitejs/plugin-react": "^4.0.0",
    "vite": "^5.0.0"
  }
}

==============================
FILE: .\frontend\vite.config.js
==============================
import { defineConfig } from "vite";
import react from "@vitejs/plugin-react";

export default defineConfig({
  plugins: [react()],
  server: {
    port: 5173
  }
});

==============================
FILE: .\frontend\src\App.jsx
==============================
import React from "react";
import FileUpload from "./components/FileUpload";
import ChatBox from "./components/ChatBox";

export default function App() {
  return (
    <div style={{ padding: "30px", fontFamily: "Segoe UI, Arial, sans-serif", maxWidth: "900px", margin: "0 auto" }}>
      <header style={{ textAlign: "center", marginBottom: "40px" }}>
        <h1 style={{ color: "#2c3e50" }}>AI TR·ª¢ GI·∫¢NG (DEMO MVP)</h1>
        <p style={{ color: "#7f8c8d" }}>H·ªá th·ªëng kh√¥ng y√™u c·∫ßu ƒëƒÉng nh·∫≠p - S·ª≠ d·ª•ng ngay</p>
      </header>

      <main>
        {/* Ph·∫ßn Upload t√†i li·ªáu - Lu√¥n hi·ªÉn th·ªã cho m·ªçi ng∆∞·ªùi d√πng */}
        <section style={{ marginBottom: "50px" }}>
          <FileUpload />
        </section>

        <hr style={{ border: "0", borderTop: "1px solid #eee", margin: "40px 0" }} />

        {/* Ph·∫ßn Chat AI */}
        <section>
          <ChatBox />
        </section>
      </main>
    </div>
  );
}
==============================
FILE: .\frontend\src\main.jsx
==============================
import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App";

ReactDOM.createRoot(document.getElementById("root")).render(<App />);

==============================
FILE: .\frontend\src\api\client.js
==============================
import axios from "axios";

// C·∫•u h√¨nh Base URL cho Backend
export const api = axios.create({
  baseURL: "http://127.0.0.1:8000"
});

[cite_start]// ƒê√£ g·ª° b·ªè logic setToken/Authorization Header [cite: 152, 153]
==============================
FILE: .\frontend\src\components\ChatBox.jsx
==============================
import React, { useState } from "react";

export default function ChatBox() {
  const [question, setQuestion] = useState("");
  const [answer, setAnswer] = useState("");
  const [sources, setSources] = useState([]);
  const [loading, setLoading] = useState(false);

  const ask = async () => {
    if (!question.trim()) return;
    
    setLoading(true);
    try {
      [cite_start]// G·ªçi tr·ª±c ti·∫øp endpoint chat (ƒê√£ b·ªè fetch auth) [cite: 161]
      const res = await fetch("http://localhost:8000/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ question }),
      });
      
      const data = await res.json();
      setAnswer(data.answer);
      setSources(data.sources || []);
    } catch (error) {
      setAnswer("L·ªói k·∫øt n·ªëi ƒë·∫øn m√°y ch·ªß AI.");
    } finally {
      setLoading(false);
    }
  };

  return (
    <div style={{ marginTop: "20px" }}>
      <h3>üí¨ H·ªèi ƒë√°p v·ªõi AI</h3>
      <textarea 
        value={question} 
        onChange={(e) => setQuestion(e.target.value)} 
        placeholder="Nh·∫≠p c√¢u h·ªèi t·ª´ t√†i li·ªáu ƒë√£ upload..."
        rows={4} 
        style={{ width: "100%", padding: "10px", borderRadius: "4px", border: "1px solid #ccc" }} 
      />
      <button 
        onClick={ask} 
        disabled={loading}
        style={{ marginTop: "10px", padding: "10px 25px", backgroundColor: "#2ecc71", color: "white", border: "none", borderRadius: "4px", cursor: "pointer" }}
      >
        {loading ? "ƒêang suy nghƒ©..." : "G·ª≠i c√¢u h·ªèi"}
      </button>

      {answer && (
        <div style={{ marginTop: "30px", padding: "20px", backgroundColor: "#fff", borderLeft: "5px solid #2ecc71", boxShadow: "0 2px 5px rgba(0,0,0,0.1)" }}>
          <h4 style={{ marginTop: 0 }}>Tr·∫£ l·ªùi:</h4>
          <p style={{ whiteSpace: "pre-wrap", lineHeight: "1.6" }}>{answer}</p>
          
          {sources.length > 0 && (
            <div style={{ marginTop: "20px", fontSize: "0.85rem" }}>
              <p><strong>Ngu·ªìn tham kh·∫£o:</strong></p>
              <ul style={{ paddingLeft: "20px" }}>
                {sources.map((s, idx) => (
                  <li key={idx}>
                    {s.source_file} {s.page !== null ? `- Trang ${s.page}` : ""}
                  </li>
                ))}
              </ul>
            </div>
          )}
        </div>
      )}
    </div>
  );
}
==============================
FILE: .\frontend\src\components\FileUpload.jsx
==============================
import React, { useState } from "react";
// FIX: Import tr·ª±c ti·∫øp t·ª´ file client.js thay v√¨ folder api
import api from "../api/client"; 

const FileUpload = () => {
    const [selectedFile, setSelectedFile] = useState(null);
    const [courseId, setCourseId] = useState("");
    const [loading, setLoading] = useState(false);

    const handleFileChange = (e) => {
        setSelectedFile(e.target.files[0]);
    };

    const handleUpload = async () => {
        if (!selectedFile || !courseId) {
            alert("Vui l√≤ng ch·ªçn file v√† nh·∫≠p Course ID");
            return;
        }

        setLoading(true);
        const formData = new FormData();
        formData.append("file", selectedFile);

        try {
            // Upload endpoint d·ª±a tr√™n router backend
            await api.post(`/upload/?course_id=${courseId}`, formData, {
                headers: { "Content-Type": "multipart/form-data" },
            });
            alert("Upload v√† Ingest th√†nh c√¥ng!");
        } catch (error) {
            console.error("L·ªói upload:", error);
            alert("L·ªói: " + (error.response?.data?.detail || "Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c server"));
        } finally {
            setLoading(false);
        }
    };

    return (
        <div style={{ padding: "20px", border: "1px solid #ccc", marginBottom: "20px", borderRadius: "8px" }}>
            <h3>üì§ Upload T√†i Li·ªáu</h3>
            <div style={{ marginBottom: "10px" }}>
                <input
                    type="text"
                    placeholder="Nh·∫≠p Course ID (vd: ML101)"
                    value={courseId}
                    onChange={(e) => setCourseId(e.target.value)}
                    style={{ padding: "8px", width: "200px" }}
                />
            </div>
            <input type="file" onChange={handleFileChange} accept=".txt,.pdf,.docx" />
            <button 
                onClick={handleUpload} 
                disabled={loading}
                style={{ marginLeft: "10px", padding: "8px 16px", cursor: "pointer" }}
            >
                {loading ? "ƒêang x·ª≠ l√Ω..." : "T·∫£i l√™n & Ingest"}
            </button>
        </div>
    );
};

export default FileUpload;
==============================
FILE: .\tests\conftest.py
==============================
import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

==============================
FILE: .\tests\test_eval.py
==============================
from backend.eval.runner import run_evaluation

def test_eval_pipeline_runs():
    results = run_evaluation()
    assert isinstance(results, list)
    assert len(results) > 0
    assert "score" in results[0]

==============================
FILE: .\tests\test_eval_pipeline.py
==============================
from backend.eval.runner import run_evaluation

def test_eval_runs():
    results = run_evaluation()
    assert isinstance(results, list)
    assert "grounded" in results[0]

==============================
FILE: .\tests\test_guardrails.py
==============================
from backend.rag.hybrid_retriever import hybrid_search
from backend.agent.qa import answer_question

def test_empty_query():
    assert hybrid_search("") == []

def test_short_query():
    assert hybrid_search("hi") == []

def test_out_of_scope_question():
    result = answer_question("Th·ªß ƒë√¥ c·ªßa Ph√°p l√† g√¨?")
    assert "kh√¥ng t√¨m th·∫•y" in result["answer"].lower() \
        or "v∆∞·ª£t ngo√†i ph·∫°m vi" in result["answer"].lower()

==============================
FILE: .\tests\test_rag_guardrail.py
==============================
from backend.agent.qa import answer_question

def test_out_of_scope():
    r = answer_question("Ai l√† t·ªïng th·ªëng M·ªπ?")
    assert "kh√¥ng t√¨m th·∫•y" in r["answer"].lower() \
        or "v∆∞·ª£t ngo√†i ph·∫°m vi" in r["answer"].lower()

==============================
FILE: .\uploads\test_knowledge.txt
==============================
Th·ªß ƒë√¥ c·ªßa Vi·ªát Nam l√† H√† N·ªôi. Kh√≥a h·ªçc ML101 d·∫°y v·ªÅ Machine Learning c∆° b·∫£n.