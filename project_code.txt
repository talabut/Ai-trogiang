
====================
FILE: .\bundle.py
====================
import os

def bundle_code(output_file="project_code.txt"):
    extensions = ['.py', '.jsx', '.js', '.css', '.html']
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for root, dirs, files in os.walk("."):
            # Bỏ qua các thư mục rác
            if 'venv' in root or 'node_modules' in root or '.git' in root:
                continue
            for file in files:
                if any(file.endswith(ext) for ext in extensions):
                    full_path = os.path.join(root, file)
                    outfile.write(f"\n{'='*20}\nFILE: {full_path}\n{'='*20}\n")
                    with open(full_path, 'r', encoding='utf-8', errors='ignore') as infile:
                        outfile.write(infile.read())
    print(f"Đã gom code xong vào file: {output_file}")

if __name__ == "__main__":
    bundle_code()
====================
FILE: .\check_imports.py
====================
import pkgutil
import importlib
import traceback

BASE_PACKAGE = "backend"

errors = []

for module in pkgutil.walk_packages([BASE_PACKAGE], BASE_PACKAGE + "."):
    name = module.name
    try:
        importlib.import_module(name)
    except Exception as e:
        errors.append((name, e))

print("\n====== IMPORT ERRORS ======")
for name, err in errors:
    print(f"\n❌ {name}")
    traceback.print_exception(type(err), err, err.__traceback__)

print(f"\nTotal errors: {len(errors)}")

====================
FILE: .\backend\main.py
====================
from fastapi import FastAPI

from backend.api.chat import router as chat_router
from backend.api.upload import router as upload_router
from backend.api.auth import router as auth_router
from backend.api.pedagogy import router as pedagogy_router
from backend.api.eval import router as eval_router
from backend.api.courses import router as courses_router

app = FastAPI(title="AI Trợ Giảng")

app.include_router(auth_router)
app.include_router(upload_router)
app.include_router(chat_router)
app.include_router(pedagogy_router)
app.include_router(eval_router)
app.include_router(courses_router)

====================
FILE: .\backend\__init__.py
====================

====================
FILE: .\backend\agent\prompt.py
====================
SYSTEM_PROMPT = """
Bạn là AI trợ giảng học thuật.

Quy tắc bắt buộc:
- Chỉ trả lời dựa trên nội dung tài liệu được cung cấp.
- Nếu không tìm thấy thông tin trong tài liệu, hãy trả lời: 
  "Tôi không tìm thấy thông tin trong tài liệu."
- Không được suy đoán hoặc bịa.
- Trả lời ngắn gọn, đúng trọng tâm.
"""

====================
FILE: .\backend\agent\qa.py
====================
from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import get_llm
from backend.agent.prompt import SYSTEM_PROMPT

def answer_question(question: str, course_id: str = "ML101"):
    # 1. Tìm tài liệu liên quan
    docs = hybrid_search(question, course_id)
    
    # 2. Chuẩn bị ngữ cảnh
    if not docs:
        context = "Không có tài liệu nào được tìm thấy."
    else:
        context = "\n".join([doc.page_content for doc in docs])

    # 3. Tạo prompt
    full_prompt = f"{SYSTEM_PROMPT}\n\nCONTEXT:\n{context}\n\nQUESTION:\n{question}"
    
    # 4. Gọi LLM
    llm = get_llm()
    answer = llm.invoke(full_prompt)
    
    # Đảm bảo kết quả là string
    if hasattr(answer, 'content'): # Nếu là object của LangChain
        answer = answer.content

    return {
        "answer": answer,
        "sources": [{"content": d.page_content, "metadata": d.metadata} for d in docs]
    }
====================
FILE: .\backend\agent\__init__.py
====================

====================
FILE: .\backend\api\auth.py
====================
from fastapi import APIRouter

router = APIRouter(prefix="/auth", tags=["Auth"])

@router.post(
    "/login",
    operation_id="auth_login"
)
def login(username: str, password: str):
    return {"status": "ok"}

====================
FILE: .\backend\api\chat.py
====================
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from backend.agent.qa import answer_question

router = APIRouter()

class ChatRequest(BaseModel):
    question: str
    course_id: str = "ML101"

@router.post("/")
async def chat(request: ChatRequest):
    try:
        # Quan trọng: Phải truyền request.course_id
        result = answer_question(request.question, request.course_id)
        return result
    except Exception as e:
        print(f"Lỗi Chat API: {e}")
        raise HTTPException(status_code=500, detail=str(e))
====================
FILE: .\backend\api\courses.py
====================
from fastapi import APIRouter

router = APIRouter(prefix="/courses", tags=["Courses"])

@router.get(
    "/",
    operation_id="courses_list"
)
def list_courses():
    return []

====================
FILE: .\backend\api\eval.py
====================
from fastapi import APIRouter
from backend.eval.runner import run_evaluation

router = APIRouter(prefix="/eval", tags=["Eval"])

@router.post(
    "/run",
    operation_id="eval_run"
)
def run_eval():
    return run_evaluation()

====================
FILE: .\backend\api\pedagogy.py
====================
from fastapi import APIRouter

router = APIRouter(prefix="/pedagogy", tags=["Pedagogy"])

@router.post(
    "/generate",
    operation_id="pedagogy_generate"
)
def generate():
    return {"status": "ok"}

====================
FILE: .\backend\api\upload.py
====================
import os
from fastapi import APIRouter, UploadFile, File, HTTPException
from backend.rag.ingest import ingest_document

router = APIRouter()

UPLOAD_DIR = "data/raw_docs"

@router.post("/")
async def upload_document(course_id: str, file: UploadFile = File(...)):
    # Tạo thư mục nếu chưa có
    if not os.path.exists(UPLOAD_DIR):
        os.makedirs(UPLOAD_DIR)

    file_path = os.path.join(UPLOAD_DIR, file.filename)
    
    try:
        # Ghi file vào ổ đĩa
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        # Ingest dữ liệu (chuyển văn bản thành vector)
        ingest_document(file_path, course_id)

        return {"message": f"Upload thành công file {file.filename}"}
    except Exception as e:
        # Trả về lỗi chi tiết để debug
        raise HTTPException(status_code=500, detail=str(e))
====================
FILE: .\backend\api\__init__.py
====================

====================
FILE: .\backend\auth\deps.py
====================
from backend.auth.roles import UserRole

def get_current_user():
    # MVP giả lập, sau này nối JWT
    return {
        "id": "demo_user",
        "role": UserRole.TEACHER
    }

====================
FILE: .\backend\auth\roles.py
====================
from enum import Enum

class UserRole(str, Enum):
    STUDENT = "student"
    TEACHER = "teacher"

====================
FILE: .\backend\auth\security.py
====================
from datetime import datetime, timedelta
from jose import jwt, JWTError

SECRET_KEY = "CHANGE_ME_SECRET"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60


def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)


def verify_token(token: str):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    except JWTError:
        return None

====================
FILE: .\backend\auth\users.py
====================
# backend/auth/users.py

from passlib.context import CryptContext

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def hash_password(password: str) -> str:
    return pwd_context.hash(password)

def verify_password(plain, hashed):
    return pwd_context.verify(plain, hashed)

# Hash sẵn (tạo 1 lần rồi hardcode)
USERS_DB = {
    "teacher1": {
        "username": "teacher1",
        "password": "$2b$12$6b8KJ7u9yM3mY8qY1bQ3KOUxM8Y1zA0k9Ff5M9XqZxQxJw5s5Oe9a",  # 123456
        "role": "teacher"
    },
    "student1": {
        "username": "student1",
        "password": "$2b$12$6b8KJ7u9yM3mY8qY1bQ3KOUxM8Y1zA0k9Ff5M9XqZxQxJw5s5Oe9a",  # 123456
        "role": "student"
    }
}

def authenticate(username: str, password: str):
    user = USERS_DB.get(username)
    if not user:
        return None
    if not verify_password(password, user["password"]):
        return None
    return user

====================
FILE: .\backend\courses\access.py
====================
def check_course_access(user, course):
    if user["role"] == "teacher":
        return course.owner_id == user["id"]
    if user["role"] == "student":
        # MVP: cho phép tất cả SV (sau này gắn class)
        return True
    return False

====================
FILE: .\backend\courses\models.py
====================
from dataclasses import dataclass

@dataclass
class Course:
    course_id: str
    name: str
    owner_id: str  # giảng viên

====================
FILE: .\backend\courses\service.py
====================
from backend.courses.models import Course

# MVP: in-memory, sau này thay DB
COURSES = {}

def create_course(course_id: str, name: str, owner_id: str):
    course = Course(course_id, name, owner_id)
    COURSES[course_id] = course
    return course

def get_course(course_id: str):
    return COURSES.get(course_id)

def list_courses_by_owner(owner_id: str):
    return [c for c in COURSES.values() if c.owner_id == owner_id]

====================
FILE: .\backend\courses\__init__.py
====================

====================
FILE: .\backend\eval\datasets.py
====================
from dataclasses import dataclass
from typing import List

@dataclass
class EvalSample:
    id: int
    question: str
    expected_answer: str | None = None

EVAL_DATASET: List[EvalSample] = [
    EvalSample(
        id=1,
        question="FAISS là gì trong hệ thống RAG?"
    ),
    EvalSample(
        id=2,
        question="BM25 dùng để làm gì?"
    ),
]

====================
FILE: .\backend\eval\evaluator.py
====================
from backend.rag.hybrid_retriever import hybrid_search

MIN_RETRIEVAL_SCORE = 0.15

def evaluate_sample(sample: dict) -> dict:
    """
    sample = {
        id: int,
        question: str
    }
    """

    question = sample.get("question")
    if not question:
        raise ValueError("Missing question in eval sample")

    results = hybrid_search(question)

    grounded = False
    used_chunks = []

    for doc, score in results:
        if score >= MIN_RETRIEVAL_SCORE:
            grounded = True
            used_chunks.append({
                "chunk_id": doc.metadata.get("chunk_id"),
                "source_file": doc.metadata.get("source_file"),
                "score": round(score, 4)
            })

    return {
        "id": sample.get("id"),
        "question": question,
        "grounded": grounded,
        "num_chunks": len(used_chunks),
        "chunks": used_chunks,
        "score": 1.0 if grounded else 0.0
    }

====================
FILE: .\backend\eval\faithfulness.py
====================
import re
from typing import Dict, Any, List


STOPWORDS = {
    "là", "và", "của", "trong", "cho", "với", "một", "các",
    "được", "khi", "này", "đó", "như", "để"
}


def normalize(text: str) -> List[str]:
    """
    Normalize text into keyword tokens
    """
    text = text.lower()
    text = re.sub(r"[^a-z0-9à-ỹ\s]", " ", text)
    tokens = text.split()
    return [t for t in tokens if t not in STOPWORDS and len(t) > 2]


def extract_claim_sentences(answer: str) -> List[str]:
    """
    Split answer into claim sentences (remove chunk tags)
    """
    clean = re.sub(r"\[CHUNK_\d+\]", "", answer)
    sentences = re.split(r"[.\n]", clean)
    return [s.strip() for s in sentences if len(s.strip()) > 20]


def check_faithfulness(
    answer: str,
    contexts: List[str],
    min_overlap_ratio: float = 0.3
) -> Dict[str, Any]:
    """
    Faithfulness check:
    - Each claim sentence must overlap sufficiently with context keywords
    """

    context_text = " ".join(contexts)
    context_tokens = set(normalize(context_text))

    claims = extract_claim_sentences(answer)

    if not claims:
        return {
            "faithful": False,
            "reason": "NO_CLAIM_SENTENCE",
            "details": "No valid claim sentence found in answer."
        }

    unfaithful_claims = []

    for claim in claims:
        claim_tokens = normalize(claim)

        if not claim_tokens:
            continue

        overlap = set(claim_tokens) & context_tokens
        overlap_ratio = len(overlap) / len(set(claim_tokens))

        if overlap_ratio < min_overlap_ratio:
            unfaithful_claims.append({
                "claim": claim,
                "overlap_ratio": round(overlap_ratio, 2)
            })

    if unfaithful_claims:
        return {
            "faithful": False,
            "reason": "LOW_CONTEXT_OVERLAP",
            "details": unfaithful_claims
        }

    return {
        "faithful": True,
        "reason": "OK",
        "details": "All claims sufficiently supported by context."
    }

====================
FILE: .\backend\eval\groundedness.py
====================
import re
from typing import Dict, Any, List


CHUNK_PATTERN = re.compile(r"\[CHUNK_(\d+)\]")


def extract_chunk_ids(answer: str) -> List[int]:
    """
    Extract chunk indices from answer text.
    Example: [CHUNK_0], [CHUNK_2] → [0, 2]
    """
    return [int(x) for x in CHUNK_PATTERN.findall(answer)]


def check_groundedness(
    answer: str,
    sources: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Groundedness check:
    - Answer must contain chunk tags
    - Chunk tags must exist in retrieved sources
    """

    chunk_ids_in_answer = extract_chunk_ids(answer)

    if not chunk_ids_in_answer:
        return {
            "grounded": False,
            "reason": "NO_CHUNK_TAG",
            "details": "Answer does not contain any [CHUNK_x] tags."
        }

    valid_chunk_ids = {
        src.get("chunk_id") for src in sources
    }

    invalid_refs = [
        cid for cid in chunk_ids_in_answer
        if cid not in valid_chunk_ids
    ]

    if invalid_refs:
        return {
            "grounded": False,
            "reason": "INVALID_CHUNK_REFERENCE",
            "details": f"Referenced chunk(s) not in sources: {invalid_refs}"
        }

    return {
        "grounded": True,
        "reason": "OK",
        "details": f"All referenced chunks are valid: {chunk_ids_in_answer}"
    }

====================
FILE: .\backend\eval\metrics.py
====================
def groundedness_score(sources, expected_sources):
    if not sources:
        return 0.0

    matched = 0
    for src in sources:
        source_file = src.get("source_file", "")
        if any(exp in source_file for exp in expected_sources):
            matched += 1

    return matched / len(expected_sources)


def citation_coverage(sources):
    if not sources:
        return 0.0

    cited = [s for s in sources if s.get("page") is not None]
    return len(cited) / len(sources)


def hallucination_flag(sources):
    return len(sources) == 0

====================
FILE: .\backend\eval\runner.py
====================
from backend.eval.datasets import EVAL_DATASET
from backend.eval.evaluator import evaluate_sample
from backend.logging.audit import audit_log

def run_evaluation():
    results = []

    for sample in EVAL_DATASET:
        result = evaluate_sample({
            "id": sample.id,
            "question": sample.question
        })

        results.append(result)

        audit_log(
            user="system_eval",
            action="evaluation_run",
            payload=result
        )

    return results

====================
FILE: .\backend\eval\__init__.py
====================

====================
FILE: .\backend\guardrails\citation.py
====================
def format_citations(source_documents):
    citations = []
    for doc in source_documents:
        meta = doc.metadata
        citations.append({
            "source": meta.get("source", "unknown"),
            "page": meta.get("page", None)
        })
    return citations

====================
FILE: .\backend\guardrails\grounding.py
====================
from typing import List

MIN_DOCS_REQUIRED = 1

def check_grounding(source_documents: List):
    if not source_documents or len(source_documents) < MIN_DOCS_REQUIRED:
        raise ValueError(
            "Không tìm thấy tài liệu liên quan. "
            "Hệ thống không thể trả lời để tránh sai lệch học thuật."
        )

====================
FILE: .\backend\guardrails\rate_limit.py
====================
import time

REQUEST_LIMIT = 20
WINDOW_SECONDS = 60

_user_requests = {}

def check_rate_limit(user_id: str):
    now = time.time()
    timestamps = _user_requests.get(user_id, [])

    timestamps = [t for t in timestamps if now - t < WINDOW_SECONDS]

    if len(timestamps) >= REQUEST_LIMIT:
        raise ValueError("Bạn gửi quá nhiều yêu cầu. Vui lòng thử lại sau.")

    timestamps.append(now)
    _user_requests[user_id] = timestamps

====================
FILE: .\backend\llm\llm.py
====================
import os
try:
    from google import genai
    HAS_GENAI = True
except ImportError:
    HAS_GENAI = False

class GeminiLLM:
    def __init__(self, api_key: str):
        self.client = genai.Client(api_key=api_key) if HAS_GENAI and api_key else None

    def invoke(self, prompt: str) -> str:
        if not self.client:
            return "AI đang Offline. Vui lòng kiểm tra API Key."
        try:
            # Lưu ý cú pháp model.generate_content của bản SDK mới
            response = self.client.models.generate_content(
                model="gemini-1.5-flash", 
                contents=prompt
            )
            return response.text
        except Exception as e:
            return f"Lỗi Gemini: {str(e)}"

class OfflineLLM:
    def invoke(self, prompt: str) -> str:
        return "AI (Offline): Tôi đã nhận được câu hỏi nhưng cần API Key để xử lý."

# --- CẤU HÌNH ---
MY_API_KEY = "" # Dán API Key vào đây

if MY_API_KEY:
    llm_instance = GeminiLLM(api_key=MY_API_KEY)
else:
    llm_instance = OfflineLLM()

def get_llm():
    return llm_instance
====================
FILE: .\backend\llm\__init__.py
====================

====================
FILE: .\backend\logging\audit.py
====================
import json
import uuid
from datetime import datetime
from pathlib import Path
from typing import List

LOG_PATH = Path("data/audit.log")

def audit_log(
    user: str,
    action: str,
    payload: dict,
    course_id: str | None = None,
    chunk_ids: List[str] | None = None,
    model_name: str | None = None,
    request_id: str | None = None
):
    LOG_PATH.parent.mkdir(parents=True, exist_ok=True)

    record = {
        "timestamp": datetime.utcnow().isoformat(),
        "request_id": request_id or str(uuid.uuid4()),
        "user": user,
        "action": action,
        "course_id": course_id,
        "chunk_ids": chunk_ids or [],
        "model": model_name or "offline_stub",
        "payload": payload,
        "schema": "academic_audit_v3"
    }

    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

====================
FILE: .\backend\pedagogy\bloom.py
====================
from enum import Enum
from typing import List


class BloomLevel(str, Enum):
    """
    Bloom's Taxonomy – Cognitive Domain
    """

    REMEMBER = "remember"
    UNDERSTAND = "understand"
    APPLY = "apply"
    ANALYZE = "analyze"
    EVALUATE = "evaluate"
    CREATE = "create"

    @property
    def description(self) -> str:
        return {
            self.REMEMBER: "Nhớ lại thông tin, định nghĩa, thuật ngữ",
            self.UNDERSTAND: "Giải thích, mô tả, diễn giải ý nghĩa",
            self.APPLY: "Áp dụng kiến thức vào bài toán cụ thể",
            self.ANALYZE: "Phân tích, so sánh, chỉ ra mối quan hệ",
            self.EVALUATE: "Đánh giá, nhận xét, phản biện",
            self.CREATE: "Tổng hợp, thiết kế, xây dựng mới"
        }[self]

    @property
    def action_verbs(self) -> List[str]:
        """
        Gợi ý động từ dùng trong câu hỏi / bài tập
        """
        return {
            self.REMEMBER: ["liệt kê", "nêu", "định nghĩa", "kể tên"],
            self.UNDERSTAND: ["giải thích", "trình bày", "mô tả", "tóm tắt"],
            self.APPLY: ["áp dụng", "tính toán", "giải", "thực hiện"],
            self.ANALYZE: ["phân tích", "so sánh", "phân loại", "làm rõ"],
            self.EVALUATE: ["đánh giá", "nhận xét", "phản biện", "so sánh ưu nhược điểm"],
            self.CREATE: ["thiết kế", "xây dựng", "đề xuất", "phát triển"]
        }[self]

    @classmethod
    def ordered_levels(cls) -> List["BloomLevel"]:
        """
        Trả về Bloom levels theo thứ tự tăng dần độ khó
        """
        return [
            cls.REMEMBER,
            cls.UNDERSTAND,
            cls.APPLY,
            cls.ANALYZE,
            cls.EVALUATE,
            cls.CREATE
        ]

====================
FILE: .\backend\pedagogy\generator.py
====================
import json
from typing import Dict, Any, List

from backend.pedagogy.bloom import BloomLevel
from backend.pedagogy.prompts import (
    build_question_prompt,
    build_assignment_prompt
)
from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import get_llm


CONTEXT_TOP_K = 5


def retrieve_context(query: str) -> str:
    """
    Retrieve learning context using Hybrid Search
    """
    results = hybrid_search(query)
    contexts = []

    for doc, score in results[:CONTEXT_TOP_K]:
        contexts.append(doc.page_content)

    return "\n\n".join(contexts)


def _invoke_llm(prompt: str) -> List[Dict[str, Any]]:
    """
    Invoke LLM and parse JSON output safely
    """
    llm = get_llm()
    raw = llm.invoke(prompt)

    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        raise ValueError(
            "LLM output is not valid JSON. "
            "Please refine prompt or model configuration."
        )


def generate_questions(
    topic: str,
    bloom: BloomLevel,
    num_items: int = 3
) -> List[Dict[str, Any]]:
    """
    Generate pedagogical questions based on Bloom level
    """

    context = retrieve_context(topic)

    if not context.strip():
        raise ValueError("No relevant learning material found.")

    prompt = build_question_prompt(
        context=context,
        bloom=bloom,
        num_items=num_items
    )

    return _invoke_llm(prompt)


def generate_assignments(
    topic: str,
    bloom: BloomLevel,
    num_items: int = 2
) -> List[Dict[str, Any]]:
    """
    Generate pedagogical assignments based on Bloom level
    """

    context = retrieve_context(topic)

    if not context.strip():
        raise ValueError("No relevant learning material found.")

    prompt = build_assignment_prompt(
        context=context,
        bloom=bloom,
        num_items=num_items
    )

    return _invoke_llm(prompt)

====================
FILE: .\backend\pedagogy\prompts.py
====================
from backend.pedagogy.bloom import BloomLevel


QUESTION_PROMPT_TEMPLATE = """
Bạn là AI Trợ Giảng.

NHIỆM VỤ:
Sinh {num_items} CÂU HỎI học tập theo mức độ Bloom: {bloom_level}

MÔ TẢ MỨC ĐỘ:
{bloom_description}

YÊU CẦU BẮT BUỘC:
- CHỈ sử dụng thông tin trong TÀI LIỆU bên dưới
- Không suy đoán, không thêm kiến thức bên ngoài
- Mỗi câu hỏi phải bám sát nội dung tài liệu
- Mức độ nhận thức phải đúng Bloom level
- Không trùng lặp câu hỏi

TÀI LIỆU:
{context}

ĐỊNH DẠNG TRẢ VỀ (JSON):
[
  {{
    "question": "...",
    "bloom_level": "{bloom_level}",
    "learning_objective": "...",
    "difficulty": "easy | medium | hard"
  }}
]
"""


ASSIGNMENT_PROMPT_TEMPLATE = """
Bạn là AI Trợ Giảng.

NHIỆM VỤ:
Sinh {num_items} BÀI TẬP học tập theo mức độ Bloom: {bloom_level}

MÔ TẢ MỨC ĐỘ:
{bloom_description}

YÊU CẦU BẮT BUỘC:
- CHỈ sử dụng thông tin trong TÀI LIỆU bên dưới
- Không suy đoán, không thêm kiến thức bên ngoài
- Bài tập phải yêu cầu người học vận dụng đúng mức Bloom
- Có mô tả rõ yêu cầu đầu ra

TÀI LIỆU:
{context}

ĐỊNH DẠNG TRẢ VỀ (JSON):
[
  {{
    "title": "...",
    "task": "...",
    "expected_output": "...",
    "bloom_level": "{bloom_level}",
    "difficulty": "easy | medium | hard"
  }}
]
"""


def build_question_prompt(
    context: str,
    bloom: BloomLevel,
    num_items: int = 3
) -> str:
    return QUESTION_PROMPT_TEMPLATE.format(
        num_items=num_items,
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )


def build_assignment_prompt(
    context: str,
    bloom: BloomLevel,
    num_items: int = 2
) -> str:
    return ASSIGNMENT_PROMPT_TEMPLATE.format(
        num_items=num_items,
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )

====================
FILE: .\backend\pedagogy\rubric.py
====================
import json
from typing import List, Dict, Any

from backend.pedagogy.bloom import BloomLevel
from backend.rag.hybrid_retriever import hybrid_search
from backend.llm.llm import get_llm


CONTEXT_TOP_K = 5


def retrieve_context(topic: str) -> str:
    results = hybrid_search(topic)
    contexts = []

    for doc, _ in results[:CONTEXT_TOP_K]:
        contexts.append(doc.page_content)

    return "\n\n".join(contexts)


RUBRIC_PROMPT_TEMPLATE = """
Bạn là GIẢNG VIÊN.

NHIỆM VỤ:
Xây dựng RUBRIC CHẤM ĐIỂM cho bài học với mức độ Bloom: {bloom_level}

MÔ TẢ MỨC ĐỘ:
{bloom_description}

YÊU CẦU:
- Chỉ sử dụng thông tin trong TÀI LIỆU
- Rubric phải đánh giá đúng mức Bloom
- Có nhiều tiêu chí chấm điểm
- Có mô tả rõ cho từng mức điểm

TÀI LIỆU:
{context}

ĐỊNH DẠNG TRẢ VỀ (JSON):
[
  {{
    "criterion": "...",
    "levels": {{
      "excellent": "...",
      "good": "...",
      "average": "...",
      "poor": "..."
    }}
  }}
]
"""


def generate_rubric(
    topic: str,
    bloom: BloomLevel
) -> List[Dict[str, Any]]:
    """
    Generate grading rubric based on Bloom level
    """

    context = retrieve_context(topic)

    if not context.strip():
        raise ValueError("No relevant learning material found.")

    prompt = RUBRIC_PROMPT_TEMPLATE.format(
        bloom_level=bloom.value,
        bloom_description=bloom.description,
        context=context
    )

    llm = get_llm()
    raw = llm.invoke(prompt)

    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        raise ValueError("LLM output is not valid JSON.")

====================
FILE: .\backend\pedagogy\schemas.py
====================
from pydantic import BaseModel
from typing import List, Optional

class TaskRequest(BaseModel):
    task_type: str   # lesson_plan | assignment | exam | rubric | quiz
    subject: str
    clo: Optional[List[str]] = None
    bloom_level: Optional[str] = None
    difficulty: Optional[str] = None
    num_items: Optional[int] = None
    duration_minutes: Optional[int] = None

====================
FILE: .\backend\pedagogy\templates.py
====================
LESSON_PLAN_TEMPLATE = """
Bạn là trợ giảng đại học.
Dựa CHỈ trên tài liệu sau:
{context}

Hãy soạn giáo án cho môn: {subject}
CLO: {clo}
Thời lượng: {duration} phút

Yêu cầu:
- Không thêm kiến thức ngoài tài liệu
- Cấu trúc rõ ràng
- Có hoạt động GV/SV
"""

====================
FILE: .\backend\pedagogy\validators.py
====================
def validate_groundedness(answer, sources):
    if not sources:
        raise ValueError(
            "Nội dung sinh ra không có căn cứ từ tài liệu."
        )

====================
FILE: .\backend\pedagogy\__init__.py
====================

====================
FILE: .\backend\rag\hybrid_retriever.py
====================
import os
from backend.vectorstore.faiss_store import EMBEDDING_MODEL
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

def hybrid_search(query: str, course_id: str = "ML101"):
    index_path = os.path.join("data", "faiss_index", course_id)
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    
    # Kiểm tra file index tồn tại
    faiss_file = os.path.join(index_path, "index.faiss")
    if not os.path.exists(faiss_file):
        print(f"--- Warning: Không tìm thấy index tại {faiss_file} ---")
        return []

    try:
        vector_store = FAISS.load_local(
            index_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        return vector_store.similarity_search(query, k=3)
    except Exception as e:
        print(f"--- Lỗi FAISS Search: {e} ---")
        return []
====================
FILE: .\backend\rag\ingest.py
====================
import os
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from backend.vectorstore.faiss_store import EMBEDDING_MODEL
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

def ingest_document(file_path: str, course_id: str):
    # 1. Load file
    loader = TextLoader(file_path, encoding='utf-8')
    documents = loader.load()

    # 2. Chia nhỏ văn bản
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = text_splitter.split_documents(documents)

    # 3. Chuyển thành Vector và lưu
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    vector_store = FAISS.from_documents(docs, embeddings)
    
    index_path = f"data/faiss_index/{course_id}"
    if not os.path.exists("data/faiss_index"):
        os.makedirs("data/faiss_index")
        
    vector_store.save_local(index_path)
====================
FILE: .\backend\rag\__init__.py
====================

====================
FILE: .\backend\utils\chunking.py
====================
# backend/utils/chunking.py

import uuid
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document



def chunk_text(
    text: str,
    source_file: str,
    page: int = None,
    section: str = None,
    chunk_size: int = 400,
    chunk_overlap: int = 80
) -> List[Document]:
    """
    Chunk text into Documents with full metadata
    """

    doc_id = str(uuid.uuid4())

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    chunks = splitter.split_text(text)

    documents = []

    for idx, chunk in enumerate(chunks):
        metadata = {
            "doc_id": doc_id,
            "source_file": source_file,
            "page": page,
            "section": section,
            "chunk_id": idx
        }

        documents.append(
            Document(
                page_content=chunk,
                metadata=metadata
            )
        )

    return documents

====================
FILE: .\backend\utils\citation.py
====================
def format_apa(source: dict) -> str:
    """
    APA 7th basic format
    """
    author = "Unknown"
    year = "n.d."
    title = source.get("source_file", "Unknown document")
    page = source.get("page")

    page_part = f" (p. {page})" if page is not None else ""

    return f"{author} ({year}). {title}{page_part}."


def format_ieee(source: dict, index: int) -> str:
    """
    IEEE basic format
    """
    title = source.get("source_file", "Unknown document")
    page = source.get("page")

    page_part = f", p. {page}" if page is not None else ""

    return f"[{index}] {title}{page_part}."

====================
FILE: .\backend\utils\text_extraction.py
====================
from pathlib import Path
from pypdf import PdfReader
from docx import Document as DocxDocument


def extract_text(file_path: str) -> str:
    ext = Path(file_path).suffix.lower()

    if ext == ".pdf":
        reader = PdfReader(file_path)
        return "\n".join(
            page.extract_text() or ""
            for page in reader.pages
        )

    if ext in [".docx"]:
        doc = DocxDocument(file_path)
        return "\n".join(p.text for p in doc.paragraphs)

    if ext in [".txt"]:
        with open(file_path, encoding="utf-8", errors="ignore") as f:
            return f.read()

    raise ValueError(f"Unsupported file type: {ext}")

====================
FILE: .\backend\vectorstore\bm25_store.py
====================
import pickle
from pathlib import Path
from typing import List, Tuple

from rank_bm25 import BM25Okapi
from langchain_core.documents import Document



BM25_PATH = Path("data/bm25.pkl")


class BM25Store:
    def __init__(self):
        self.documents: List[Document] = []
        self.corpus = []
        self.bm25 = None

    def add_documents(self, docs: List[Document]):
        for doc in docs:
            tokens = doc.page_content.lower().split()
            self.documents.append(doc)
            self.corpus.append(tokens)

        self.bm25 = BM25Okapi(self.corpus)

    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        if not self.bm25:
            return []

        query_tokens = query.lower().split()
        scores = self.bm25.get_scores(query_tokens)

        ranked = sorted(
            enumerate(scores),
            key=lambda x: x[1],
            reverse=True
        )[:k]

        return [
            (self.documents[idx], score)
            for idx, score in ranked
            if score > 0
        ]

    def save(self):
        BM25_PATH.parent.mkdir(parents=True, exist_ok=True)
        with open(BM25_PATH, "wb") as f:
            pickle.dump(self, f)

    @staticmethod
    def load():
        if not BM25_PATH.exists():
            return BM25Store()

        with open(BM25_PATH, "rb") as f:
            return pickle.load(f)

====================
FILE: .\backend\vectorstore\faiss_store.py
====================
import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Sử dụng model nhỏ để tiết kiệm RAM và chạy nhanh trên CPU
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

def get_faiss_store():
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    # Khởi tạo một store trống để tránh lỗi 500 khi chưa có dữ liệu
    return FAISS.from_texts(["Khởi tạo"], embeddings)
====================
FILE: .\frontend\index.html
====================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>AI Trợ Giảng</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>

====================
FILE: .\frontend\vite.config.js
====================
import { defineConfig } from "vite";
import react from "@vitejs/plugin-react";

export default defineConfig({
  plugins: [react()],
  server: {
    port: 5173
  }
});

====================
FILE: .\frontend\src\api.js
====================
import axios from "axios";

export const api = axios.create({
  baseURL: "http://127.0.0.1:8000"
});

export function setToken(token) {
  api.defaults.headers.common["Authorization"] = `Bearer ${token}`;
}

====================
FILE: .\frontend\src\App.jsx
====================
import { useState } from "react";
import { api, setToken } from "./api";
import ChatBox from "./components/ChatBox";

export default function App() {
  const [username, setUsername] = useState("");
  const [password, setPassword] = useState("");
  const [role, setRole] = useState(null);
  const [file, setFile] = useState(null);

  async function login() {
    const res = await api.post("/auth/login", {
      username,
      password,
    });

    setToken(res.data.access_token);
    setRole(res.data.role);

    alert("Login thành công với role: " + res.data.role);
  }

  async function upload() {
    const form = new FormData();
    form.append("file", file);

    await api.post("/upload/", form);
    alert("Upload & ingest xong");
  }

  return (
    <div style={{ padding: 30, fontFamily: "Arial" }}>
      <h2>AI Trợ Giảng</h2>

      {/* LOGIN */}
      <h3>Login</h3>
      <input
        placeholder="username"
        onChange={(e) => setUsername(e.target.value)}
      />
      <br />
      <input
        type="password"
        placeholder="password"
        onChange={(e) => setPassword(e.target.value)}
      />
      <br />
      <button onClick={login}>Login</button>

      {/* UPLOAD – TEACHER ONLY */}
      {role === "teacher" && (
        <>
          <h3>Upload tài liệu (GV)</h3>
          <input
            type="file"
            onChange={(e) => setFile(e.target.files[0])}
          />
          <br />
          <button onClick={upload}>Upload</button>
        </>
      )}

      <hr />

      {/* CHAT – DÙNG CHATBOX PHASE 3 */}
      <ChatBox />
    </div>
  );
}

====================
FILE: .\frontend\src\main.jsx
====================
import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App";

ReactDOM.createRoot(document.getElementById("root")).render(<App />);

====================
FILE: .\frontend\src\components\ChatBox.jsx
====================
import { useState } from "react";

export default function ChatBox() {
  const [question, setQuestion] = useState("");
  const [answer, setAnswer] = useState("");
  const [sources, setSources] = useState([]);
  const [citations, setCitations] = useState(null);

  const ask = async () => {
    const res = await fetch("http://localhost:8000/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ question }),
    });

    const data = await res.json();
    setAnswer(data.answer);
    setSources(data.sources || []);
    setCitations(data.citations || null);
  };

  return (
    <div style={{ maxWidth: 800, margin: "40px auto" }}>
      <h2>AI Trợ Giảng</h2>

      <textarea
        value={question}
        onChange={(e) => setQuestion(e.target.value)}
        rows={3}
        style={{ width: "100%" }}
      />

      <button onClick={ask} style={{ marginTop: 10 }}>
        Hỏi
      </button>

      {answer && (
        <>
          <h3>Trả lời (có gắn nguồn)</h3>
          <pre style={{ whiteSpace: "pre-wrap" }}>{answer}</pre>
        </>
      )}

      {sources.length > 0 && (
        <>
          <h4>Nguồn tham khảo theo Chunk</h4>
          <ul>
            {sources.map((s, idx) => (
              <li key={idx}>
                <b>[CHUNK_{idx}]</b> — {s.source_file}
                {s.page !== null && ` – Trang ${s.page}`}
                {s.section && ` – ${s.section}`}
                <details>
                  <summary>Xem trích đoạn</summary>
                  <small>{s.preview}</small>
                </details>
              </li>
            ))}
          </ul>
        </>
      )}

      {citations && (
        <>
          <h4>References (APA)</h4>
          <ul>
            {citations.apa.map((c, i) => (
              <li key={i}>{c}</li>
            ))}
          </ul>

          <h4>References (IEEE)</h4>
          <ul>
            {citations.ieee.map((c, i) => (
              <li key={i}>{c}</li>
            ))}
          </ul>
        </>
      )}
    </div>
  );
}

====================
FILE: .\frontend\src\components\FileUpload.jsx
====================
import React, { useState } from "react";
import axios from "axios";

function FileUpload() {
  const [selectedFile, setSelectedFile] = useState(null);
  const [courseId, setCourseId] = useState("ML101");
  const [loading, setLoading] = useState(false);

  const handleFileChange = (e) => {
    setSelectedFile(e.target.files[0]);
  };

  const handleUpload = async () => {
    if (!selectedFile) {
      alert("Vui lòng chọn một file!");
      return;
    }

    setLoading(true);
    const formData = new FormData();
    // Quan trọng: Tên 'file' phải khớp với backend
    formData.append("file", selectedFile);

    try {
      const response = await axios.post(
        `http://localhost:8000/upload/?course_id=${courseId}`,
        formData,
        {
          headers: { "Content-Type": "multipart/form-data" },
        }
      );
      alert("Upload thành công: " + response.data.message);
    } catch (error) {
      console.error("Lỗi upload:", error);
      alert("Lỗi upload: " + (error.response?.data?.detail || "Không rõ lỗi"));
    } finally {
      setLoading(false);
    }
  };

  return (
    <div style={{ padding: "20px", border: "1px solid #ccc", marginBottom: "20px" }}>
      <h3>Upload Tài Liệu (GV)</h3>
      <input
        type="text"
        placeholder="Course ID (vd: ML101)"
        value={courseId}
        onChange={(e) => setCourseId(e.target.value)}
      />
      <br /><br />
      <input type="file" onChange={handleFileChange} accept=".txt" />
      <button onClick={handleUpload} disabled={loading}>
        {loading ? "Đang xử lý..." : "Tải lên & Ingest"}
      </button>
    </div>
  );
}

export default FileUpload;
====================
FILE: .\tests\conftest.py
====================
import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

====================
FILE: .\tests\test_eval.py
====================
from backend.eval.runner import run_evaluation

def test_eval_pipeline_runs():
    results = run_evaluation()
    assert isinstance(results, list)
    assert len(results) > 0
    assert "score" in results[0]

====================
FILE: .\tests\test_eval_pipeline.py
====================
from backend.eval.runner import run_evaluation

def test_eval_runs():
    results = run_evaluation()
    assert isinstance(results, list)
    assert "grounded" in results[0]

====================
FILE: .\tests\test_guardrails.py
====================
from backend.rag.hybrid_retriever import hybrid_search
from backend.agent.qa import answer_question

def test_empty_query():
    assert hybrid_search("") == []

def test_short_query():
    assert hybrid_search("hi") == []

def test_out_of_scope_question():
    result = answer_question("Thủ đô của Pháp là gì?")
    assert "không tìm thấy" in result["answer"].lower() \
        or "vượt ngoài phạm vi" in result["answer"].lower()

====================
FILE: .\tests\test_rag_guardrail.py
====================
from backend.agent.qa import answer_question

def test_out_of_scope():
    r = answer_question("Ai là tổng thống Mỹ?")
    assert "không tìm thấy" in r["answer"].lower() \
        or "vượt ngoài phạm vi" in r["answer"].lower()
