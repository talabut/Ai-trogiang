# FILE: tests/test_llama_ingest_strict.py
import sys
import os
import shutil
import json

# Add project root to path
sys.path.append(os.getcwd())

from backend.rag.llama_ingest import ingest_canonical_chunks, get_index_path
from backend.rag.retrieval import get_llama_retriever

# --- MOCK DATA ---
MOCK_CHUNKS = [
    {
        "doc_id": "TEST_DOC_1",
        "page": 1,
        "line_start": 1,
        "line_end": 5,
        "text": "Ki·∫øn tr√∫c RAG bao g·ªìm Retrieval v√† Generation. ƒê√¢y l√† test chunk s·ªë 1."
    },
    {
        "doc_id": "TEST_DOC_1",
        "page": 1,
        "line_start": 6,
        "line_end": 10,
        "text": "LlamaIndex gi√∫p qu·∫£n l√Ω d·ªØ li·ªáu cho LLM. ƒê√¢y l√† test chunk s·ªë 2."
    },
    {
        "doc_id": "TEST_DOC_1",
        "page": 2,
        "line_start": 1,
        "line_end": 4,
        "text": "Embedding model chuy·ªÉn text th√†nh vector. ƒê√¢y l√† test chunk s·ªë 3."
    }
]

COURSE_ID = "TEST_COURSE_V1"

def clean_test_env():
    path = get_index_path(COURSE_ID)
    if os.path.exists(path):
        shutil.rmtree(path)
    print(f"üßπ Cleaned test environment: {path}")

def test_ingestion():
    print("\n--- üß™ TEST 1: Initial Ingestion ---")
    
    # 1. First Ingest
    ingest_canonical_chunks(MOCK_CHUNKS, COURSE_ID, "test_file.txt", "TEST_DOC_1")
    
    # Check if files exist
    idx_path = get_index_path(COURSE_ID)
    if os.path.exists(os.path.join(idx_path, "docstore.json")) and \
       os.path.exists(os.path.join(idx_path, "default__vector_store.json")):
        print("‚úÖ Storage files created.")
    else:
        print("‚ùå Storage files MISSING.")
        return False
    return True

def test_deduplication():
    print("\n--- üß™ TEST 2: Deduplication Check ---")
    
    # 2. Re-ingest same chunks (Expect skipping all)
    print("running re-ingest (should skip all)...")
    ingest_canonical_chunks(MOCK_CHUNKS, COURSE_ID, "test_file.txt", "TEST_DOC_1")
    
    # 3. Ingest new chunk
    NEW_CHUNK = [{
        "doc_id": "TEST_DOC_2", 
        "page": 99, 
        "line_start": 1, 
        "line_end": 1, 
        "text": "ƒê√¢y l√† chunk ho√†n to√†n m·ªõi."
    }]
    print("running ingest new chunk...")
    ingest_canonical_chunks(NEW_CHUNK, COURSE_ID, "test_file_2.txt", "TEST_DOC_2")
    
    print("‚úÖ Deduplication logic executed (check logs for 'Skipped' messages).")

def test_retrieval_integrity():
    print("\n--- üß™ TEST 3: Retrieval & Metadata Integrity ---")
    
    retriever = get_llama_retriever(COURSE_ID, top_k=1)
    results = retriever.retrieve("Ki·∫øn tr√∫c RAG")
    
    if not results:
        print("‚ùå No results found!")
        return

    top_node = results[0]
    meta = top_node.metadata
    
    print("üîç Retrieved Node Metadata:")
    print(json.dumps(meta, indent=2))
    
    # Strict Validation Checks
    checks = [
        ("page" in meta, "Missing 'page'"),
        (meta.get("page") == 1, "Wrong 'page' value"),
        ("line_start" in meta, "Missing 'line_start'"),
        ("line_end" in meta, "Missing 'line_end'"),
        (meta.get("doc_id") == "TEST_DOC_1", "Wrong 'doc_id'"),
        (meta.get("index_version") == "v1.0", "Missing/Wrong version"),
        (meta.get("embedding_model") == "sentence-transformers/all-MiniLM-L6-v2", "Wrong Embedding Tag")
    ]
    
    passed = True
    for condition, msg in checks:
        if not condition:
            print(f"‚ùå FAIL: {msg}")
            passed = False
    
    if passed:
        print("‚úÖ All Metadata checks PASSED.")

def main():
    clean_test_env()
    if test_ingestion():
        test_deduplication()
        test_retrieval_integrity()
    
    print("\nüéâ ALL TESTS COMPLETED.")

if __name__ == "__main__":
    main()