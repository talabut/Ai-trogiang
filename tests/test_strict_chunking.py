import json
import sys
import os

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import ƒë∆∞·ª£c module backend
sys.path.append(os.getcwd())

from backend.rag.chunking import chunk_canonical_data

# 1. GI·∫¢ L·∫¨P INPUT (Canonical Structured TXT)
# D·ªØ li·ªáu gi·∫£ l·∫≠p 2 trang, c√≥ d√≤ng, c√≥ n·ªôi dung li·ªÅn m·∫°ch
mock_canonical_input = [
    {
        "page": 1,
        "source": "manual_upload",
        "lines": [
            {"line_id": 1, "text": "CH∆Ø∆†NG 1: GI·ªöI THI·ªÜU V·ªÄ MACHINE LEARNING"},
            {"line_id": 2, "text": "H·ªçc m√°y (Machine Learning) l√† m·ªôt lƒ©nh v·ª±c c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o."},
            {"line_id": 3, "text": "N√≥ t·∫≠p trung v√†o vi·ªác x√¢y d·ª±ng c√°c h·ªá th·ªëng c√≥ kh·∫£ nƒÉng h·ªçc h·ªèi t·ª´ d·ªØ li·ªáu."},
            {"line_id": 4, "text": "Thay v√¨ l·∫≠p tr√¨nh c√°c quy t·∫Øc c·ª• th·ªÉ, ch√∫ng ta hu·∫•n luy·ªán m√¥ h√¨nh."},
            {"line_id": 5, "text": "M√¥ h√¨nh s·∫Ω t·ª± t√¨m ra c√°c quy lu·∫≠t ·∫©n ch·ª©a b√™n trong d·ªØ li·ªáu ƒë·∫ßu v√†o."},
            {"line_id": 6, "text": "C√≥ ba lo·∫°i h·ªçc m√°y ch√≠nh: H·ªçc c√≥ gi√°m s√°t, kh√¥ng gi√°m s√°t v√† b√°n gi√°m s√°t."},
            {"line_id": 7, "text": "Trong h·ªçc c√≥ gi√°m s√°t, d·ªØ li·ªáu ƒë∆∞·ª£c d√°n nh√£n c·ª• th·ªÉ."},
            {"line_id": 8, "text": "V√≠ d·ª•: Ph√¢n lo·∫°i email l√† spam hay kh√¥ng spam d·ª±a tr√™n l·ªãch s·ª≠."},
            {"line_id": 9, "text": "H·ªçc kh√¥ng gi√°m s√°t x·ª≠ l√Ω d·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c d√°n nh√£n."},
            {"line_id": 10, "text": "M·ª•c ti√™u l√† t√¨m ra c·∫•u tr√∫c ·∫©n, v√≠ d·ª• nh∆∞ gom nh√≥m kh√°ch h√†ng."}
        ]
    },
    {
        "page": 2,
        "source": "manual_upload",
        "lines": [
            {"line_id": 1, "text": "CH∆Ø∆†NG 2: M·∫†NG N∆†-RON NH√ÇN T·∫†O"},
            {"line_id": 2, "text": "M·∫°ng n∆°-ron ƒë∆∞·ª£c l·∫•y c·∫£m h·ª©ng t·ª´ b·ªô n√£o con ng∆∞·ªùi."},
            {"line_id": 3, "text": "N√≥ bao g·ªìm c√°c l·ªõp: L·ªõp ƒë·∫ßu v√†o, l·ªõp ·∫©n v√† l·ªõp ƒë·∫ßu ra."},
            {"line_id": 4, "text": "M·ªói n∆°-ron nh·∫≠n t√≠n hi·ªáu, nh√¢n tr·ªçng s·ªë v√† ƒëi qua h√†m k√≠ch ho·∫°t."},
            {"line_id": 5, "text": "Deep Learning l√† thu·∫≠t ng·ªØ ch·ªâ c√°c m·∫°ng n∆°-ron c√≥ nhi·ªÅu l·ªõp ·∫©n."},
            {"line_id": 6, "text": "·ª®ng d·ª•ng c·ªßa Deep Learning bao g·ªìm nh·∫≠n di·ªán ·∫£nh v√† x·ª≠ l√Ω ng√¥n ng·ªØ."}
        ]
    }
]

def run_test():
    doc_id = "DOC_TEST_001"
    
    # 2. CH·∫†Y LOGIC CHUNKING
    print("‚è≥ ƒêang th·ª±c hi·ªán Chunking...")
    chunks = chunk_canonical_data(mock_canonical_input, doc_id)
    
    # 3. XU·∫§T OUTPUT RA JSONL
    output_file = "chunks_output.jsonl"
    with open(output_file, "w", encoding="utf-8") as f:
        for chunk in chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + "\n")
            
    print(f"‚úÖ ƒê√£ t·∫°o file output: {output_file}")
    print(f"üìä T·ªïng s·ªë chunks: {len(chunks)}")
    
    # In m·∫´u ƒë·ªÉ ki·ªÉm tra
    print("\n--- M·∫™U 2 CHUNKS ƒê·∫¶U TI√äN ---")
    for c in chunks[:2]:
        print(json.dumps(c, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    run_test()